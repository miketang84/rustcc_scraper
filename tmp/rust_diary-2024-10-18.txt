https://github.com/StefanTerdell/userp
-->>-->>
Repository files navigation README Userp Work in progress Warning: This crate is heavily WIP! I'm holding off on doc-comments until I've worked out the module hierarchy and basic API to my satisfaction. Summary This crate provides a high-level user, authentication and session handling system for Axum, and likely Actix later on. The idea is to use it as a base for something like Next Auth but for Leptos, being easy to set up while heavy on features, with including batteris a higher approach than full customizability. If you need something truly custom you might want to look at the awesome axum-login or oauth2 crates, but if you just want... Users to be able to Log In Reset their Passwords with their verified Email Link their social accounts Manage their multiple Sessions ... Then this might be something for you! Screenshots Before you ask: design PRs are most welcome 😅 Features Login types Username / Password Email magic link Social logins (OAuth) Emails Validation Password reset Oauth Easily extendable with custom providers Ergonomicly implement user info fetching procedure Optional split callback paths Batteries included Askama based templates provide basic login/signup/account pages Growing list of built-in social providers Multiple sessions Todo Granular feature-controlled templates Replacable templates (by typed Fns returning impl IntoResponse) Webauthn MFA Doc-comments Tests ??? Publish!

======>
https://rerun.io/blog/dataframe
-->>-->>
Encoded video, dataframe view and query API Written by Nikolaus West 2 days ago Rerun is building the multimodal data stack for robotics and spatial intelligence.
Up until now that has meant logging and visualization, but with the 0.19 release , we're expanding that in a big way. Over the last year and a half,
Rerun has been used by some of the best teams in the world that build intelligent products for the physical world.
They have used Rerun to unify and simplify their visualization stacks to get a consistent view of their data; from notebook prototyping, monitoring a live robot, to debugging data pipelines for training. Throughout that time we've been getting the same, very reasonable, question again and again: "how can I read back data from Rerun to use for analysis/training/random-use-case?" .
With Rerun 0.19 we can finally answer that question: by using the new dataframe query API from the SDK.
To match, we've built a new dataframe view that lets you look at any data in Rerun as a table. This new dataframe query API lets you load Rerun recordings and run queries against them that return Apache Arrow data.
This makes it easy to pass results directly into your favorite dataframe tool like Pandas , Polars , or DuckDB . Another goal for this API is to enable you to easily create training datasets from your Rerun recordings, for example to use with 🤗 LeRobot . That brings us to the next major feature in the 0.19 release , part 1 of native video support in Rerun.
Modern imitation learning for robotics operate on sequences of images and storing those as videos has massive storage and performance benefits. This translates directly to iteration speed, which in turn means faster progress. Part 1 of video means supporting video in mp4 files encoded with either AV1 (full support) or H.264 (only web viewer for now). Part 2 will be video streaming and bringing H.264 support to the native viewer as well. You can read about using the new AssetVideo archetype here , and more details on what's currently supported here . SDK access to Rerun's query engine sdk-access-to-reruns-query-engine At the heart of what makes Rerun fast, easy to use, and flexible is our time-aware Entity Component System data model and the query-engine that operates on it. When you scroll the time-slider back and forth in the viewer, it's querying an in-memory database at 60fps for the latest versions of all components that are used for a visualization. In Rerun we call this a latest-at query, but in some systems something similar can also be called an AsOf Join . This is needed because data that's recorded from robotics-like systems are seldomly aligned.
For example, perception data like depth and color images tend to be generated at a much slower rate (15-60Hz) than
motion IMU data (500-1000Hz). 
If you think of a recording as a table where each row is a timestamp, you tend to end up with very sparse tables. Most systems for subsequent data analysis or training aren't built to handle these kinds of sparse tables, which means an important part of most data pipelines for robotics-like applications includes a time-alignment step. Time alignment can get pretty tricky which makes bugs common and code changes risky. With the new dataframe query API, extracting a time aligned dataframe from a recording can be as simple as this: import rerun as rr import pyarrow . parquet as pq

recording = rr . dataframe . load_recording ( "/path/to/file.rrd" ) batch_iterator = recording . view ( index = "frame_nr" , # Use the frame_nr timeline as row-index contents = "/world/robot/**" ) # Include all data under /world/robot . filter_is_not_null ( "/world/robot:Position3D" ) # Only rows containing this component . fill_latest_at ( ) . select ( ) # Select all columns # Read into Pandas dataframe df = batch_iterator . read_pandas ( ) . . . # Or write to Parquet with pq . ParquetWriter ( "train.parquet" , batch_iterator . schema ( ) ) as writer : for chunk in batch_iterator : writer . write_batch ( chunk ) Time-alignment can get a lot more complex that latest-at queries though, e.g. if you need to do motion interpolation, and we'll continue to expand these APIs to make that easier and easier to do with Rerun over time. We're building robotics-style primitives into the database layer to simplify data pipelines were-building-roboticsstyle-primitives-into-the-database-layer-to-simplify-data-pipelines Our goal at Rerun is to make data pipelines much simpler to build and operate for teams working on spatial and embodied AI.
We believe data should be visualizable and debuggable by default, wherever it is in the data pipeline.
For that reason, data that uses the Rerun data model is directly visualizable in the Rerun Viewer.
Using the Rerun SDK basically amounts to adding that semantic information to your data. To make data pipelines easier to build, we think the database layer needs to actually understand robotics data.
It's not enough to attach some semantics to specific robotics messages, all your data systems should maintain and use those semantics. As part of making that a reality we'll continue to push primitives for time-alignment, spatial transform resolution, label look-ups, and video-frame decoding into the database layer.
The Rerun Viewer already does this and we'll expose more and more of these capabilities to the SDK over time. This will all be part of Rerun's commercial offering, which is managed infrastructure to ingest, store, analyze, and stream data at scale with built-in visual debugging. 
It's currently in development with a few select design partners. Get in touch if you'd like to be one of them. Try it out and let us know what you think try-it-out-and-let-us-know-what-you-think We're really looking forward to hear how the first pieces of both the dataframe query SDK and video support works for you. Start with the get data out of Rerun guide or the docs on AssetVideo and then join us on Github or Discord and let us know what you think and would hope to see in the future.

======>
https://github.com/ptrglbvc/omd
-->>-->>
Repository files navigation README MIT license omd omd is a simple, fast, and lightweight Markdown renderer and previewer written in Rust. It allows you to convert Markdown files to HTML and preview them in your browser, either statically or with live-reload support. Features Static Mode : Convert Markdown files to HTML and open them directly in your default browser without running a server. Server Mode : Run a local server to preview your Markdown files with live-reload functionality as you edit them. CommonMark Extensions : Supports strikethrough, tables, footnotes, task lists, and smart punctuation. Customizable Styling : Includes default CSS styling, which can be customized by editing style.css . Embedded Fonts and Favicon : Uses embedded fonts and favicon for a consistent look and self-contained HTML output. Installation Prerequisites Rust and Cargo (for building from source) Build from Source Clone the Repository git clone https://github.com/ptrglbvc/omd.git cd omd Build the Project cargo build --release Install Optionally, you can install omd to your local Cargo bin directory: cargo install --path . This allows you to run omd from anywhere on your system. Get it from crates.io Run cargo install omd That is it. Usage omd [OPTIONS] [FILE] Options -s , --static-mode : Run in static mode. Converts the Markdown file to HTML and opens it in your default browser without starting a server. Examples Static Mode Convert a Markdown file to HTML and open it in your browser: omd --static-mode README.md If no file is specified, omd will read from stdin : cat README.md | omd --static-mode Server Mode (Live Preview) Start a local server to preview your Markdown file with live-reload functionality: omd README.md Open http://localhost:3030 in your browser. Whenever you save changes to README.md , the browser will automatically reload to reflect the updates. Configuration Custom CSS You can customize the CSS styling by editing the style.css file located in the src directory or by replacing it with your own CSS file. Fonts and Favicon The application uses embedded fonts and a favicon located in the fonts directory and favicon.ico respectively. To use your own fonts or icon, replace these files and update the code if necessary. Supported Markdown Extensions omd supports several CommonMark extensions: Strikethrough Tables Footnotes Task Lists Smart Punctuation These extensions are enabled by default to enhance the formatting capabilities of your Markdown files. How It Works Static Mode : Renders the Markdown to HTML, writes it to a temporary file, and opens it in your default browser. Server Mode : Starts a local web server using Warp and watches the Markdown file for changes using Notify . The browser automatically reloads when changes are detected. Dependencies Pulldown-Cmark for parsing and rendering Markdown. Warp for running the web server in server mode. Notify for watching file changes. License This project is licensed under the MIT License . Contributing Contributions are welcome! Please follow these steps: Fork the repository . Create a new branch for your feature or bugfix. Commit your changes with clear messages. Push to your fork and submit a Pull Request . Please make sure to update tests as appropriate. Acknowledgments Thanks to the Rust community for their amazing crates that make projects like this possible. Inspired by the need for a simple Markdown previewer without unnecessary overhead. Contact For questions or suggestions, feel free to open an issue or reach out via email at petar0golubovic@gmail.com .

=====>
https://github.com/fintelia
-->>-->>
fintelia Follow Jonathan Behrens fintelia Follow Opinions are my own.

I maintain image-rs and develop Terra in a personal capacity. 120 followers · 12 following Microsoft Seattle, WA 17:05 - same time Achievements x2 x2 x3 x2 Achievements x2 x2 x3 x2 Organizations Block or Report Block or report fintelia Block user Prevent this user from interacting with your repositories and sending you notifications.
          Learn more about blocking users . You must be logged in to block users. Add an optional note: Please don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you. Block user Report abuse Contact GitHub support about this user’s behavior.
        Learn more about reporting abuse . Report abuse Overview Repositories 141 Projects 0 Packages 0 Stars 87 More Overview Repositories Projects Packages Stars Pinned Loading terra terra Public A large scale terrain rendering library written in Rust Rust 504 14 mit-pdos/ ward mit-pdos/ward Public C++ 30 5 mit-pdos/ RVirt mit-pdos/RVirt Public RISC-V hypervisor written in Rust Rust 345 34 mit-pdos/ noria mit-pdos/noria Public Fast web applications through dynamic, partially-stateful dataflow Rust 5k 244 skyrender skyrender Public Rust 35 4 590
      contributions
        in the last year 1 contribution on October 15th. 4 contributions on October 22nd. 7 contributions on October 29th. No contributions on November 5th. 3 contributions on November 12th. 2 contributions on November 19th. 3 contributions on November 26th. 1 contribution on December 3rd. No contributions on December 10th. 6 contributions on December 17th. 3 contributions on December 24th. 4 contributions on December 31st. No contributions on January 7th. 2 contributions on January 14th. No contributions on January 21st. 3 contributions on January 28th. 5 contributions on February 4th. 28 contributions on February 11th. 8 contributions on February 18th. 2 contributions on February 25th. No contributions on March 3rd. 2 contributions on March 10th. No contributions on March 17th. 1 contribution on March 24th. 3 contributions on March 31st. 14 contributions on April 7th. 6 contributions on April 14th. 12 contributions on April 21st. 1 contribution on April 28th. No contributions on May 5th. 2 contributions on May 12th. 5 contributions on May 19th. 2 contributions on May 26th. 5 contributions on June 2nd. No contributions on June 9th. No contributions on June 16th. No contributions on June 23rd. No contributions on June 30th. No contributions on July 7th. 1 contribution on July 14th. 1 contribution on July 21st. 2 contributions on July 28th. 1 contribution on August 4th. No contributions on August 11th. 9 contributions on August 18th. 1 contribution on August 25th. 1 contribution on September 1st. No contributions on September 8th. No contributions on September 15th. 2 contributions on September 22nd. 3 contributions on September 29th. 7 contributions on October 6th. 7 contributions on October 13th. No contributions on October 16th. 2 contributions on October 23rd. 3 contributions on October 30th. No contributions on November 6th. 2 contributions on November 13th. 2 contributions on November 20th. No contributions on November 27th. 3 contributions on December 4th. 6 contributions on December 11th. No contributions on December 18th. 11 contributions on December 25th. 14 contributions on January 1st. No contributions on January 8th. No contributions on January 15th. 1 contribution on January 22nd. 1 contribution on January 29th. No contributions on February 5th. 2 contributions on February 12th. 9 contributions on February 19th. 1 contribution on February 26th. No contributions on March 4th. 5 contributions on March 11th. 1 contribution on March 18th. No contributions on March 25th. 7 contributions on April 1st. 2 contributions on April 8th. No contributions on April 15th. 1 contribution on April 22nd. No contributions on April 29th. No contributions on May 6th. 2 contributions on May 13th. 1 contribution on May 20th. No contributions on May 27th. No contributions on June 3rd. No contributions on June 10th. No contributions on June 17th. No contributions on June 24th. No contributions on July 1st. No contributions on July 8th. 3 contributions on July 15th. 1 contribution on July 22nd. No contributions on July 29th. 6 contributions on August 5th. No contributions on August 12th. 8 contributions on August 19th. 1 contribution on August 26th. 8 contributions on September 2nd. No contributions on September 9th. 2 contributions on September 16th. 1 contribution on September 23rd. 1 contribution on September 30th. 1 contribution on October 7th. No contributions on October 14th. No contributions on October 17th. 2 contributions on October 24th. 1 contribution on October 31st. No contributions on November 7th. 5 contributions on November 14th. No contributions on November 21st. 3 contributions on November 28th. 1 contribution on December 5th. No contributions on December 12th. 5 contributions on December 19th. 5 contributions on December 26th. 1 contribution on January 2nd. 3 contributions on January 9th. 7 contributions on January 16th. 1 contribution on January 23rd. No contributions on January 30th. No contributions on February 6th. 1 contribution on February 13th. 1 contribution on February 20th. No contributions on February 27th. 1 contribution on March 5th. No contributions on March 12th. No contributions on March 19th. 1 contribution on March 26th. 1 contribution on April 2nd. No contributions on April 9th. No contributions on April 16th. No contributions on April 23rd. 1 contribution on April 30th. No contributions on May 7th. 1 contribution on May 14th. No contributions on May 21st. 1 contribution on May 28th. No contributions on June 4th. 1 contribution on June 11th. No contributions on June 18th. No contributions on June 25th. 1 contribution on July 2nd. No contributions on July 9th. 4 contributions on July 16th. No contributions on July 23rd. No contributions on July 30th. No contributions on August 6th. No contributions on August 13th. No contributions on August 20th. 3 contributions on August 27th. 1 contribution on September 3rd. No contributions on September 10th. 4 contributions on September 17th. 1 contribution on September 24th. 3 contributions on October 1st. No contributions on October 8th. 2 contributions on October 15th. 1 contribution on October 18th. 1 contribution on October 25th. No contributions on November 1st. 1 contribution on November 8th. No contributions on November 15th. No contributions on November 22nd. No contributions on November 29th. No contributions on December 6th. No contributions on December 13th. 1 contribution on December 20th. 8 contributions on December 27th. 1 contribution on January 3rd. No contributions on January 10th. No contributions on January 17th. No contributions on January 24th. 1 contribution on January 31st. 1 contribution on February 7th. No contributions on February 14th. 2 contributions on February 21st. No contributions on February 28th. No contributions on March 6th. 1 contribution on March 13th. No contributions on March 20th. No contributions on March 27th. No contributions on April 3rd. No contributions on April 10th. 1 contribution on April 17th. No contributions on April 24th. No contributions on May 1st. 1 contribution on May 8th. 1 contribution on May 15th. No contributions on May 22nd. No contributions on May 29th. No contributions on June 5th. No contributions on June 12th. No contributions on June 19th. No contributions on June 26th. No contributions on July 3rd. 1 contribution on July 10th. 2 contributions on July 17th. No contributions on July 24th. 2 contributions on July 31st. No contributions on August 7th. 13 contributions on August 14th. No contributions on August 21st. No contributions on August 28th. No contributions on September 4th. No contributions on September 11th. 1 contribution on September 18th. 1 contribution on September 25th. 1 contribution on October 2nd. No contributions on October 9th. 2 contributions on October 16th. 1 contribution on October 19th. 1 contribution on October 26th. 1 contribution on November 2nd. 1 contribution on November 9th. 1 contribution on November 16th. No contributions on November 23rd. No contributions on November 30th. 2 contributions on December 7th. 1 contribution on December 14th. No contributions on December 21st. No contributions on December 28th. 2 contributions on January 4th. 6 contributions on January 11th. No contributions on January 18th. No contributions on January 25th. 1 contribution on February 1st. No contributions on February 8th. 2 contributions on February 15th. 2 contributions on February 22nd. 1 contribution on February 29th. No contributions on March 7th. 2 contributions on March 14th. No contributions on March 21st. No contributions on March 28th. No contributions on April 4th. 1 contribution on April 11th. 1 contribution on April 18th. 1 contribution on April 25th. No contributions on May 2nd. No contributions on May 9th. 1 contribution on May 16th. No contributions on May 23rd. No contributions on May 30th. No contributions on June 6th. No contributions on June 13th. No contributions on June 20th. No contributions on June 27th. 3 contributions on July 4th. No contributions on July 11th. No contributions on July 18th. No contributions on July 25th. 2 contributions on August 1st. No contributions on August 8th. 5 contributions on August 15th. No contributions on August 22nd. No contributions on August 29th. No contributions on September 5th. No contributions on September 12th. 1 contribution on September 19th. 1 contribution on September 26th. 2 contributions on October 3rd. No contributions on October 10th. 1 contribution on October 17th. No contributions on October 20th. 3 contributions on October 27th. 6 contributions on November 3rd. No contributions on November 10th. No contributions on November 17th. No contributions on November 24th. 1 contribution on December 1st. No contributions on December 8th. No contributions on December 15th. No contributions on December 22nd. No contributions on December 29th. 2 contributions on January 5th. No contributions on January 12th. 1 contribution on January 19th. No contributions on January 26th. 3 contributions on February 2nd. No contributions on February 9th. No contributions on February 16th. 1 contribution on February 23rd. 1 contribution on March 1st. No contributions on March 8th. No contributions on March 15th. 4 contributions on March 22nd. No contributions on March 29th. No contributions on April 5th. No contributions on April 12th. No contributions on April 19th. No contributions on April 26th. 1 contribution on May 3rd. 1 contribution on May 10th. No contributions on May 17th. No contributions on May 24th. No contributions on May 31st. No contributions on June 7th. No contributions on June 14th. No contributions on June 21st. No contributions on June 28th. No contributions on July 5th. No contributions on July 12th. 1 contribution on July 19th. No contributions on July 26th. 2 contributions on August 2nd. No contributions on August 9th. No contributions on August 16th. No contributions on August 23rd. No contributions on August 30th. No contributions on September 6th. 1 contribution on September 13th. 2 contributions on September 20th. 4 contributions on September 27th. No contributions on October 4th. No contributions on October 11th. 1 contribution on October 18th. 5 contributions on October 21st. No contributions on October 28th. 1 contribution on November 4th. No contributions on November 11th. 1 contribution on November 18th. 5 contributions on November 25th. 3 contributions on December 2nd. No contributions on December 9th. 4 contributions on December 16th. 4 contributions on December 23rd. No contributions on December 30th. 4 contributions on January 6th. 19 contributions on January 13th. 10 contributions on January 20th. 5 contributions on January 27th. No contributions on February 3rd. 11 contributions on February 10th. 3 contributions on February 17th. 14 contributions on February 24th. No contributions on March 2nd. 10 contributions on March 9th. No contributions on March 16th. 5 contributions on March 23rd. 3 contributions on March 30th. 1 contribution on April 6th. 1 contribution on April 13th. 4 contributions on April 20th. 1 contribution on April 27th. No contributions on May 4th. 4 contributions on May 11th. No contributions on May 18th. No contributions on May 25th. No contributions on June 1st. No contributions on June 8th. 1 contribution on June 15th. No contributions on June 22nd. No contributions on June 29th. No contributions on July 6th. No contributions on July 13th. No contributions on July 20th. No contributions on July 27th. No contributions on August 3rd. No contributions on August 10th. 1 contribution on August 17th. No contributions on August 24th. No contributions on August 31st. No contributions on September 7th. 2 contributions on September 14th. 2 contributions on September 21st. No contributions on September 28th. 2 contributions on October 5th. 2 contributions on October 12th. No contributions on October 19th. Contribution Graph Day of Week October Oct November Nov December Dec January Jan February Feb March Mar April Apr May May June Jun July Jul August Aug September Sep October Oct Sunday Sun Monday Mon Tuesday Tue Wednesday Wed Thursday Thu Friday Fri Saturday Sat Learn how we count contributions Less No contributions. Low contributions. Medium-low contributions. Medium-high contributions. High contributions. More 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 Contribution activity October 2024 Created 13
            commits in
            4
            repositories image-rs/fdeflate 4 commits image-rs/image 4 commits fintelia/corpus-bench 3 commits image-rs/image-webp 2 commits Created a pull request in image-rs/image that received 2
        comments Oct 12 Remove GenericImage<Pixel = Rgba<u8>> impl for DynamicImage Revive #2136 Currently the DynamicImage type pretends to be an rgba8 image even when it isn't. This makes it easier to use in places where the pixe… +32 −152 lines changed • 2
          comments Opened 8
            other
            pull requests in
            3
            repositories image-rs/fdeflate 4 merged Split decoding loop This contribution was made on Oct 14 Oct 14 Fix MSRV and CI This contribution was made on Oct 14 Oct 14 Don't hardcode deflate tables This contribution was made on Oct 14 Oct 14 Optimize deflate table building This contribution was made on Oct 4 Oct 4 image-rs/image 3 merged Upgrade image-webp This contribution was made on Oct 6 Oct 6 Clippy fixes This contribution was made on Oct 5 Oct 5 Update WebPDecoder doc comments This contribution was made on Oct 2 Oct 2 image-rs/image-webp 1 merged Fix missing color cache update in lossless decoding fastpath This contribution was made on Oct 2 Oct 2 Reviewed 9
            pull requests in
            3
            repositories image-rs/image 7 pull requests Propose wording for republishing as 0.25.4 This contribution was made on Oct 16 Oct 16 Fill in changelog and bump version for 0.25.3 This contribution was made on Oct 16 Oct 16 Move Orientation to metadata module This contribution was made on Oct 15 Oct 15 Update apply_orientation doc comment to use .orientation() This contribution was made on Oct 15 Oct 15 test(fuzz): cover other image types This contribution was made on Oct 13 Oct 13 fix: make set_icc_profile function available This contribution was made on Oct 7 Oct 7 Added compression and f32 support for TIFF encoding/decoding. This contribution was made on Oct 6 Oct 6 image-rs/image-png 1 pull request Simplify and encapsulate ReadDecoder implementation This contribution was made on Oct 18 Oct 18 johannesvollmer/exrs 1 pull request add rayon feature This contribution was made on Oct 12 Oct 12 Loading Show more activity Seeing something unexpected? Take a look at the GitHub profile guide .
======>
https://github.com/torfmaster
-->>-->>
43
      contributions
        in the last year No contributions on October 15th. No contributions on October 22nd. No contributions on October 29th. No contributions on November 5th. No contributions on November 12th. No contributions on November 19th. No contributions on November 26th. No contributions on December 3rd. No contributions on December 10th. No contributions on December 17th. No contributions on December 24th. No contributions on December 31st. No contributions on January 7th. No contributions on January 14th. No contributions on January 21st. No contributions on January 28th. No contributions on February 4th. No contributions on February 11th. No contributions on February 18th. No contributions on February 25th. No contributions on March 3rd. No contributions on March 10th. No contributions on March 17th. No contributions on March 24th. No contributions on March 31st. No contributions on April 7th. No contributions on April 14th. No contributions on April 21st. No contributions on April 28th. No contributions on May 5th. No contributions on May 12th. No contributions on May 19th. No contributions on May 26th. No contributions on June 2nd. No contributions on June 9th. No contributions on June 16th. No contributions on June 23rd. No contributions on June 30th. No contributions on July 7th. No contributions on July 14th. No contributions on July 21st. No contributions on July 28th. No contributions on August 4th. No contributions on August 11th. No contributions on August 18th. No contributions on August 25th. 1 contribution on September 1st. No contributions on September 8th. No contributions on September 15th. No contributions on September 22nd. No contributions on September 29th. No contributions on October 6th. No contributions on October 13th. No contributions on October 16th. No contributions on October 23rd. No contributions on October 30th. No contributions on November 6th. No contributions on November 13th. No contributions on November 20th. No contributions on November 27th. No contributions on December 4th. No contributions on December 11th. No contributions on December 18th. No contributions on December 25th. No contributions on January 1st. No contributions on January 8th. No contributions on January 15th. No contributions on January 22nd. No contributions on January 29th. No contributions on February 5th. No contributions on February 12th. No contributions on February 19th. No contributions on February 26th. No contributions on March 4th. No contributions on March 11th. No contributions on March 18th. No contributions on March 25th. No contributions on April 1st. No contributions on April 8th. No contributions on April 15th. No contributions on April 22nd. No contributions on April 29th. No contributions on May 6th. No contributions on May 13th. No contributions on May 20th. No contributions on May 27th. No contributions on June 3rd. No contributions on June 10th. No contributions on June 17th. No contributions on June 24th. No contributions on July 1st. No contributions on July 8th. No contributions on July 15th. 1 contribution on July 22nd. No contributions on July 29th. No contributions on August 5th. 2 contributions on August 12th. No contributions on August 19th. No contributions on August 26th. No contributions on September 2nd. No contributions on September 9th. No contributions on September 16th. No contributions on September 23rd. No contributions on September 30th. No contributions on October 7th. No contributions on October 14th. No contributions on October 17th. No contributions on October 24th. No contributions on October 31st. No contributions on November 7th. No contributions on November 14th. No contributions on November 21st. No contributions on November 28th. No contributions on December 5th. No contributions on December 12th. No contributions on December 19th. No contributions on December 26th. No contributions on January 2nd. No contributions on January 9th. No contributions on January 16th. No contributions on January 23rd. No contributions on January 30th. No contributions on February 6th. No contributions on February 13th. No contributions on February 20th. No contributions on February 27th. No contributions on March 5th. No contributions on March 12th. No contributions on March 19th. No contributions on March 26th. No contributions on April 2nd. No contributions on April 9th. No contributions on April 16th. No contributions on April 23rd. No contributions on April 30th. No contributions on May 7th. No contributions on May 14th. No contributions on May 21st. No contributions on May 28th. No contributions on June 4th. No contributions on June 11th. No contributions on June 18th. No contributions on June 25th. No contributions on July 2nd. No contributions on July 9th. No contributions on July 16th. No contributions on July 23rd. No contributions on July 30th. No contributions on August 6th. 2 contributions on August 13th. No contributions on August 20th. 1 contribution on August 27th. No contributions on September 3rd. No contributions on September 10th. No contributions on September 17th. No contributions on September 24th. No contributions on October 1st. No contributions on October 8th. No contributions on October 15th. No contributions on October 18th. No contributions on October 25th. No contributions on November 1st. No contributions on November 8th. No contributions on November 15th. No contributions on November 22nd. No contributions on November 29th. No contributions on December 6th. No contributions on December 13th. No contributions on December 20th. No contributions on December 27th. No contributions on January 3rd. No contributions on January 10th. No contributions on January 17th. No contributions on January 24th. No contributions on January 31st. 1 contribution on February 7th. 2 contributions on February 14th. No contributions on February 21st. No contributions on February 28th. No contributions on March 6th. No contributions on March 13th. No contributions on March 20th. No contributions on March 27th. No contributions on April 3rd. No contributions on April 10th. No contributions on April 17th. No contributions on April 24th. No contributions on May 1st. No contributions on May 8th. No contributions on May 15th. No contributions on May 22nd. No contributions on May 29th. No contributions on June 5th. No contributions on June 12th. No contributions on June 19th. No contributions on June 26th. No contributions on July 3rd. No contributions on July 10th. No contributions on July 17th. No contributions on July 24th. No contributions on July 31st. No contributions on August 7th. No contributions on August 14th. 6 contributions on August 21st. No contributions on August 28th. 3 contributions on September 4th. No contributions on September 11th. No contributions on September 18th. No contributions on September 25th. No contributions on October 2nd. No contributions on October 9th. No contributions on October 16th. No contributions on October 19th. No contributions on October 26th. No contributions on November 2nd. No contributions on November 9th. No contributions on November 16th. No contributions on November 23rd. No contributions on November 30th. No contributions on December 7th. No contributions on December 14th. No contributions on December 21st. No contributions on December 28th. 3 contributions on January 4th. No contributions on January 11th. No contributions on January 18th. No contributions on January 25th. No contributions on February 1st. No contributions on February 8th. No contributions on February 15th. No contributions on February 22nd. No contributions on February 29th. No contributions on March 7th. No contributions on March 14th. No contributions on March 21st. No contributions on March 28th. No contributions on April 4th. No contributions on April 11th. No contributions on April 18th. No contributions on April 25th. No contributions on May 2nd. No contributions on May 9th. No contributions on May 16th. No contributions on May 23rd. No contributions on May 30th. No contributions on June 6th. No contributions on June 13th. No contributions on June 20th. No contributions on June 27th. No contributions on July 4th. No contributions on July 11th. No contributions on July 18th. No contributions on July 25th. No contributions on August 1st. No contributions on August 8th. No contributions on August 15th. No contributions on August 22nd. 5 contributions on August 29th. No contributions on September 5th. 1 contribution on September 12th. No contributions on September 19th. No contributions on September 26th. No contributions on October 3rd. 1 contribution on October 10th. No contributions on October 17th. No contributions on October 20th. No contributions on October 27th. No contributions on November 3rd. No contributions on November 10th. No contributions on November 17th. No contributions on November 24th. No contributions on December 1st. No contributions on December 8th. No contributions on December 15th. No contributions on December 22nd. No contributions on December 29th. No contributions on January 5th. No contributions on January 12th. No contributions on January 19th. No contributions on January 26th. No contributions on February 2nd. No contributions on February 9th. No contributions on February 16th. No contributions on February 23rd. No contributions on March 1st. No contributions on March 8th. No contributions on March 15th. No contributions on March 22nd. No contributions on March 29th. No contributions on April 5th. No contributions on April 12th. No contributions on April 19th. No contributions on April 26th. No contributions on May 3rd. No contributions on May 10th. No contributions on May 17th. No contributions on May 24th. No contributions on May 31st. No contributions on June 7th. No contributions on June 14th. No contributions on June 21st. No contributions on June 28th. No contributions on July 5th. No contributions on July 12th. No contributions on July 19th. No contributions on July 26th. 3 contributions on August 2nd. No contributions on August 9th. No contributions on August 16th. No contributions on August 23rd. No contributions on August 30th. No contributions on September 6th. No contributions on September 13th. No contributions on September 20th. No contributions on September 27th. No contributions on October 4th. No contributions on October 11th. No contributions on October 18th. No contributions on October 21st. No contributions on October 28th. No contributions on November 4th. No contributions on November 11th. No contributions on November 18th. No contributions on November 25th. No contributions on December 2nd. No contributions on December 9th. No contributions on December 16th. No contributions on December 23rd. 9 contributions on December 30th. No contributions on January 6th. No contributions on January 13th. No contributions on January 20th. No contributions on January 27th. No contributions on February 3rd. No contributions on February 10th. 1 contribution on February 17th. 1 contribution on February 24th. No contributions on March 2nd. No contributions on March 9th. No contributions on March 16th. No contributions on March 23rd. No contributions on March 30th. No contributions on April 6th. No contributions on April 13th. No contributions on April 20th. No contributions on April 27th. No contributions on May 4th. No contributions on May 11th. No contributions on May 18th. No contributions on May 25th. No contributions on June 1st. No contributions on June 8th. No contributions on June 15th. No contributions on June 22nd. No contributions on June 29th. No contributions on July 6th. No contributions on July 13th. No contributions on July 20th. No contributions on July 27th. No contributions on August 3rd. No contributions on August 10th. No contributions on August 17th. No contributions on August 24th. No contributions on August 31st. No contributions on September 7th. No contributions on September 14th. No contributions on September 21st. No contributions on September 28th. No contributions on October 5th. No contributions on October 12th. No contributions on October 19th. Contribution Graph Day of Week October Oct November Nov December Dec January Jan February Feb March Mar April Apr May May June Jun July Jul August Aug September Sep October Oct Sunday Sun Monday Mon Tuesday Tue Wednesday Wed Thursday Thu Friday Fri Saturday Sat Learn how we count contributions Less No contributions. Low contributions. Medium-low contributions. Medium-high contributions. High contributions. More 2024 2023 2022 2021 2020 2019 2018 2017 2016 Contribution activity October 2024 Reviewed 1
            pull request in
            1
            repository rustwasm/wasm-bindgen 1 pull request PoC for inline target This contribution was made on Oct 10 Oct 10 Loading Show more activity Seeing something unexpected? Take a look at the GitHub profile guide .

======>
https://github.com/fintelia
-->>-->>
fintelia Follow Jonathan Behrens fintelia Follow Opinions are my own.

I maintain image-rs and develop Terra in a personal capacity. 120 followers · 12 following Microsoft Seattle, WA 17:04 - same time Achievements x2 x2 x3 x2 Achievements x2 x2 x3 x2 Organizations Block or Report Block or report fintelia Block user Prevent this user from interacting with your repositories and sending you notifications.
          Learn more about blocking users . You must be logged in to block users. Add an optional note: Please don't include any personal information such as legal names or email addresses. Maximum 100 characters, markdown supported. This note will be visible to only you. Block user Report abuse Contact GitHub support about this user’s behavior.
        Learn more about reporting abuse . Report abuse Overview Repositories 141 Projects 0 Packages 0 Stars 87 More Overview Repositories Projects Packages Stars Pinned Loading terra terra Public A large scale terrain rendering library written in Rust Rust 504 14 mit-pdos/ ward mit-pdos/ward Public C++ 30 5 mit-pdos/ RVirt mit-pdos/RVirt Public RISC-V hypervisor written in Rust Rust 345 34 mit-pdos/ noria mit-pdos/noria Public Fast web applications through dynamic, partially-stateful dataflow Rust 5k 244 skyrender skyrender Public Rust 35 4 590
      contributions
        in the last year 1 contribution on October 15th. 4 contributions on October 22nd. 7 contributions on October 29th. No contributions on November 5th. 3 contributions on November 12th. 2 contributions on November 19th. 3 contributions on November 26th. 1 contribution on December 3rd. No contributions on December 10th. 6 contributions on December 17th. 3 contributions on December 24th. 4 contributions on December 31st. No contributions on January 7th. 2 contributions on January 14th. No contributions on January 21st. 3 contributions on January 28th. 5 contributions on February 4th. 28 contributions on February 11th. 8 contributions on February 18th. 2 contributions on February 25th. No contributions on March 3rd. 2 contributions on March 10th. No contributions on March 17th. 1 contribution on March 24th. 3 contributions on March 31st. 14 contributions on April 7th. 6 contributions on April 14th. 12 contributions on April 21st. 1 contribution on April 28th. No contributions on May 5th. 2 contributions on May 12th. 5 contributions on May 19th. 2 contributions on May 26th. 5 contributions on June 2nd. No contributions on June 9th. No contributions on June 16th. No contributions on June 23rd. No contributions on June 30th. No contributions on July 7th. 1 contribution on July 14th. 1 contribution on July 21st. 2 contributions on July 28th. 1 contribution on August 4th. No contributions on August 11th. 9 contributions on August 18th. 1 contribution on August 25th. 1 contribution on September 1st. No contributions on September 8th. No contributions on September 15th. 2 contributions on September 22nd. 3 contributions on September 29th. 7 contributions on October 6th. 7 contributions on October 13th. No contributions on October 16th. 2 contributions on October 23rd. 3 contributions on October 30th. No contributions on November 6th. 2 contributions on November 13th. 2 contributions on November 20th. No contributions on November 27th. 3 contributions on December 4th. 6 contributions on December 11th. No contributions on December 18th. 11 contributions on December 25th. 14 contributions on January 1st. No contributions on January 8th. No contributions on January 15th. 1 contribution on January 22nd. 1 contribution on January 29th. No contributions on February 5th. 2 contributions on February 12th. 9 contributions on February 19th. 1 contribution on February 26th. No contributions on March 4th. 5 contributions on March 11th. 1 contribution on March 18th. No contributions on March 25th. 7 contributions on April 1st. 2 contributions on April 8th. No contributions on April 15th. 1 contribution on April 22nd. No contributions on April 29th. No contributions on May 6th. 2 contributions on May 13th. 1 contribution on May 20th. No contributions on May 27th. No contributions on June 3rd. No contributions on June 10th. No contributions on June 17th. No contributions on June 24th. No contributions on July 1st. No contributions on July 8th. 3 contributions on July 15th. 1 contribution on July 22nd. No contributions on July 29th. 6 contributions on August 5th. No contributions on August 12th. 8 contributions on August 19th. 1 contribution on August 26th. 8 contributions on September 2nd. No contributions on September 9th. 2 contributions on September 16th. 1 contribution on September 23rd. 1 contribution on September 30th. 1 contribution on October 7th. No contributions on October 14th. No contributions on October 17th. 2 contributions on October 24th. 1 contribution on October 31st. No contributions on November 7th. 5 contributions on November 14th. No contributions on November 21st. 3 contributions on November 28th. 1 contribution on December 5th. No contributions on December 12th. 5 contributions on December 19th. 5 contributions on December 26th. 1 contribution on January 2nd. 3 contributions on January 9th. 7 contributions on January 16th. 1 contribution on January 23rd. No contributions on January 30th. No contributions on February 6th. 1 contribution on February 13th. 1 contribution on February 20th. No contributions on February 27th. 1 contribution on March 5th. No contributions on March 12th. No contributions on March 19th. 1 contribution on March 26th. 1 contribution on April 2nd. No contributions on April 9th. No contributions on April 16th. No contributions on April 23rd. 1 contribution on April 30th. No contributions on May 7th. 1 contribution on May 14th. No contributions on May 21st. 1 contribution on May 28th. No contributions on June 4th. 1 contribution on June 11th. No contributions on June 18th. No contributions on June 25th. 1 contribution on July 2nd. No contributions on July 9th. 4 contributions on July 16th. No contributions on July 23rd. No contributions on July 30th. No contributions on August 6th. No contributions on August 13th. No contributions on August 20th. 3 contributions on August 27th. 1 contribution on September 3rd. No contributions on September 10th. 4 contributions on September 17th. 1 contribution on September 24th. 3 contributions on October 1st. No contributions on October 8th. 2 contributions on October 15th. 1 contribution on October 18th. 1 contribution on October 25th. No contributions on November 1st. 1 contribution on November 8th. No contributions on November 15th. No contributions on November 22nd. No contributions on November 29th. No contributions on December 6th. No contributions on December 13th. 1 contribution on December 20th. 8 contributions on December 27th. 1 contribution on January 3rd. No contributions on January 10th. No contributions on January 17th. No contributions on January 24th. 1 contribution on January 31st. 1 contribution on February 7th. No contributions on February 14th. 2 contributions on February 21st. No contributions on February 28th. No contributions on March 6th. 1 contribution on March 13th. No contributions on March 20th. No contributions on March 27th. No contributions on April 3rd. No contributions on April 10th. 1 contribution on April 17th. No contributions on April 24th. No contributions on May 1st. 1 contribution on May 8th. 1 contribution on May 15th. No contributions on May 22nd. No contributions on May 29th. No contributions on June 5th. No contributions on June 12th. No contributions on June 19th. No contributions on June 26th. No contributions on July 3rd. 1 contribution on July 10th. 2 contributions on July 17th. No contributions on July 24th. 2 contributions on July 31st. No contributions on August 7th. 13 contributions on August 14th. No contributions on August 21st. No contributions on August 28th. No contributions on September 4th. No contributions on September 11th. 1 contribution on September 18th. 1 contribution on September 25th. 1 contribution on October 2nd. No contributions on October 9th. 2 contributions on October 16th. 1 contribution on October 19th. 1 contribution on October 26th. 1 contribution on November 2nd. 1 contribution on November 9th. 1 contribution on November 16th. No contributions on November 23rd. No contributions on November 30th. 2 contributions on December 7th. 1 contribution on December 14th. No contributions on December 21st. No contributions on December 28th. 2 contributions on January 4th. 6 contributions on January 11th. No contributions on January 18th. No contributions on January 25th. 1 contribution on February 1st. No contributions on February 8th. 2 contributions on February 15th. 2 contributions on February 22nd. 1 contribution on February 29th. No contributions on March 7th. 2 contributions on March 14th. No contributions on March 21st. No contributions on March 28th. No contributions on April 4th. 1 contribution on April 11th. 1 contribution on April 18th. 1 contribution on April 25th. No contributions on May 2nd. No contributions on May 9th. 1 contribution on May 16th. No contributions on May 23rd. No contributions on May 30th. No contributions on June 6th. No contributions on June 13th. No contributions on June 20th. No contributions on June 27th. 3 contributions on July 4th. No contributions on July 11th. No contributions on July 18th. No contributions on July 25th. 2 contributions on August 1st. No contributions on August 8th. 5 contributions on August 15th. No contributions on August 22nd. No contributions on August 29th. No contributions on September 5th. No contributions on September 12th. 1 contribution on September 19th. 1 contribution on September 26th. 2 contributions on October 3rd. No contributions on October 10th. 1 contribution on October 17th. No contributions on October 20th. 3 contributions on October 27th. 6 contributions on November 3rd. No contributions on November 10th. No contributions on November 17th. No contributions on November 24th. 1 contribution on December 1st. No contributions on December 8th. No contributions on December 15th. No contributions on December 22nd. No contributions on December 29th. 2 contributions on January 5th. No contributions on January 12th. 1 contribution on January 19th. No contributions on January 26th. 3 contributions on February 2nd. No contributions on February 9th. No contributions on February 16th. 1 contribution on February 23rd. 1 contribution on March 1st. No contributions on March 8th. No contributions on March 15th. 4 contributions on March 22nd. No contributions on March 29th. No contributions on April 5th. No contributions on April 12th. No contributions on April 19th. No contributions on April 26th. 1 contribution on May 3rd. 1 contribution on May 10th. No contributions on May 17th. No contributions on May 24th. No contributions on May 31st. No contributions on June 7th. No contributions on June 14th. No contributions on June 21st. No contributions on June 28th. No contributions on July 5th. No contributions on July 12th. 1 contribution on July 19th. No contributions on July 26th. 2 contributions on August 2nd. No contributions on August 9th. No contributions on August 16th. No contributions on August 23rd. No contributions on August 30th. No contributions on September 6th. 1 contribution on September 13th. 2 contributions on September 20th. 4 contributions on September 27th. No contributions on October 4th. No contributions on October 11th. 1 contribution on October 18th. 5 contributions on October 21st. No contributions on October 28th. 1 contribution on November 4th. No contributions on November 11th. 1 contribution on November 18th. 5 contributions on November 25th. 3 contributions on December 2nd. No contributions on December 9th. 4 contributions on December 16th. 4 contributions on December 23rd. No contributions on December 30th. 4 contributions on January 6th. 19 contributions on January 13th. 10 contributions on January 20th. 5 contributions on January 27th. No contributions on February 3rd. 11 contributions on February 10th. 3 contributions on February 17th. 14 contributions on February 24th. No contributions on March 2nd. 10 contributions on March 9th. No contributions on March 16th. 5 contributions on March 23rd. 3 contributions on March 30th. 1 contribution on April 6th. 1 contribution on April 13th. 4 contributions on April 20th. 1 contribution on April 27th. No contributions on May 4th. 4 contributions on May 11th. No contributions on May 18th. No contributions on May 25th. No contributions on June 1st. No contributions on June 8th. 1 contribution on June 15th. No contributions on June 22nd. No contributions on June 29th. No contributions on July 6th. No contributions on July 13th. No contributions on July 20th. No contributions on July 27th. No contributions on August 3rd. No contributions on August 10th. 1 contribution on August 17th. No contributions on August 24th. No contributions on August 31st. No contributions on September 7th. 2 contributions on September 14th. 2 contributions on September 21st. No contributions on September 28th. 2 contributions on October 5th. 2 contributions on October 12th. No contributions on October 19th. Contribution Graph Day of Week October Oct November Nov December Dec January Jan February Feb March Mar April Apr May May June Jun July Jul August Aug September Sep October Oct Sunday Sun Monday Mon Tuesday Tue Wednesday Wed Thursday Thu Friday Fri Saturday Sat Learn how we count contributions Less No contributions. Low contributions. Medium-low contributions. Medium-high contributions. High contributions. More 2024 2023 2022 2021 2020 2019 2018 2017 2016 2015 2014 2013 2012 2011 2010 2009 Contribution activity October 2024 Created 13
            commits in
            4
            repositories image-rs/fdeflate 4 commits image-rs/image 4 commits fintelia/corpus-bench 3 commits image-rs/image-webp 2 commits Created a pull request in image-rs/image that received 2
        comments Oct 12 Remove GenericImage<Pixel = Rgba<u8>> impl for DynamicImage Revive #2136 Currently the DynamicImage type pretends to be an rgba8 image even when it isn't. This makes it easier to use in places where the pixe… +32 −152 lines changed • 2
          comments Opened 8
            other
            pull requests in
            3
            repositories image-rs/fdeflate 4 merged Split decoding loop This contribution was made on Oct 14 Oct 14 Fix MSRV and CI This contribution was made on Oct 14 Oct 14 Don't hardcode deflate tables This contribution was made on Oct 14 Oct 14 Optimize deflate table building This contribution was made on Oct 4 Oct 4 image-rs/image 3 merged Upgrade image-webp This contribution was made on Oct 6 Oct 6 Clippy fixes This contribution was made on Oct 5 Oct 5 Update WebPDecoder doc comments This contribution was made on Oct 2 Oct 2 image-rs/image-webp 1 merged Fix missing color cache update in lossless decoding fastpath This contribution was made on Oct 2 Oct 2 Reviewed 9
            pull requests in
            3
            repositories image-rs/image 7 pull requests Propose wording for republishing as 0.25.4 This contribution was made on Oct 16 Oct 16 Fill in changelog and bump version for 0.25.3 This contribution was made on Oct 16 Oct 16 Move Orientation to metadata module This contribution was made on Oct 15 Oct 15 Update apply_orientation doc comment to use .orientation() This contribution was made on Oct 15 Oct 15 test(fuzz): cover other image types This contribution was made on Oct 13 Oct 13 fix: make set_icc_profile function available This contribution was made on Oct 7 Oct 7 Added compression and f32 support for TIFF encoding/decoding. This contribution was made on Oct 6 Oct 6 image-rs/image-png 1 pull request Simplify and encapsulate ReadDecoder implementation This contribution was made on Oct 18 Oct 18 johannesvollmer/exrs 1 pull request add rayon feature This contribution was made on Oct 12 Oct 12 Loading Show more activity Seeing something unexpected? Take a look at the GitHub profile guide .
======>
https://blog.rust-lang.org/2024/07/25/Rust-1.80.0.html
-->>-->>
Rust Blog Rust Install Learn Tools Governance Community 🖌 Light Dark Announcing Rust 1.80.0 July 25, 2024 · The Rust Release Team The Rust team is happy to announce a new version of Rust, 1.80.0. Rust is a programming language empowering everyone to build reliable and efficient software. If you have a previous version of Rust installed via rustup , you can get 1.80.0 with: $ rustup update stable If you don't have it already, you can get rustup from the appropriate page on our website, and check out the detailed release notes for 1.80.0 . If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel ( rustup default beta ) or the nightly channel ( rustup default nightly ). Please report any bugs you might come across! What's in 1.80.0 stable LazyCell and LazyLock These "lazy" types delay the initialization of their data until first access. They are similar to the OnceCell and OnceLock types stabilized in 1.70 , but with the initialization function included in the cell. This completes the stabilization of functionality adopted into the standard library from the popular lazy_static and once_cell crates. LazyLock is the thread-safe option, making it suitable for places like static values. For example, both the spawn thread and the main scope will see the exact same duration below, since LAZY_TIME will be initialized once, by whichever ends up accessing the static first. Neither use has to know how to initialize it, unlike they would with OnceLock::get_or_init() . use std::sync::LazyLock; use std::time::Instant; static LAZY_TIME: LazyLock<Instant> = LazyLock::new(Instant::now); fn main () { let start = Instant::now();
    std::thread::scope(|s| {
        s.spawn(|| { println! ( "Thread lazy time is {:?}" , LAZY_TIME.duration_since(start));
        }); println! ( "Main lazy time is {:?}" , LAZY_TIME.duration_since(start));
    });
} LazyCell does the same thing without thread synchronization, so it doesn't implement Sync , which is needed for static , but it can still be used in thread_local! statics (with distinct initialization per thread). Either type can also be used in other data structures as well, depending on thread-safety needs, so lazy initialization is available everywhere! Checked cfg names and values In 1.79, rustc stabilized a --check-cfg flag , and now Cargo 1.80 is enabling those checks for all cfg names and values that it knows (in addition to the well known names and values from rustc ). This includes feature names from Cargo.toml as well as new cargo::rustc-check-cfg output from build scripts. Unexpected cfgs are reported by the warn-by-default unexpected_cfgs lint, which is meant to catch typos or other misconfiguration. For example, in a project with an optional rayon dependency, this code is configured for the wrong feature value: fn main () { println! ( "Hello, world!" ); #[cfg(feature = "crayon" )] rayon::join(
        || println! ( "Hello, Thing One!" ),
        || println! ( "Hello, Thing Two!" ),
    );
} warning: unexpected `cfg` condition value: `crayon`
 --> src/main.rs:4:11
  |
4 |     #[cfg(feature = "crayon")]
  |           ^^^^^^^^^^--------
  |                     |
  |                     help: there is a expected value with a similar name: `"rayon"`
  |
  = note: expected values for `feature` are: `rayon`
  = help: consider adding `crayon` as a feature in `Cargo.toml`
  = note: see <https://doc.rust-lang.org/nightly/rustc/check-cfg/cargo-specifics.html> for more information about checking conditional configuration
  = note: `#[warn(unexpected_cfgs)]` on by default The same warning is reported regardless of whether the actual rayon feature is enabled or not. The [lints] table in the Cargo.toml manifest can also be used to extend the list of known names and values for custom cfg . rustc automatically provides the syntax to use in the warning. [lints.rust] unexpected_cfgs = { level = "warn" , check-cfg = [ 'cfg(foo, values("bar"))' ] } You can read more about this feature in a previous blog post announcing the availability of the feature on nightly. Exclusive ranges in patterns Rust ranged patterns can now use exclusive endpoints, written a..b or ..b similar to the Range and RangeTo expression types. For example, the following patterns can now use the same constants for the end of one pattern and the start of the next: pub fn size_prefix (n: u32 ) -> & 'static str { const K: u32 = 10u32 .pow( 3 ); const M: u32 = 10u32 .pow( 6 ); const G: u32 = 10u32 .pow( 9 ); match n {
        ..K => "" ,
        K..M => "k" ,
        M..G => "M" ,
        G.. => "G" ,
    }
} Previously, only inclusive ( a..=b or ..=b ) or open ( a.. ) ranges were allowed in patterns, so code like this would require separate constants for inclusive endpoints like K - 1 . Exclusive ranges have been implemented as an unstable feature for a long time, but the blocking concern was that they might add confusion and increase the chance of off-by-one errors in patterns. To that end, exhaustiveness checking has been enhanced to better detect gaps in pattern matching, and new lints non_contiguous_range_endpoints and overlapping_range_endpoints will help detect cases where you might want to switch exclusive patterns to inclusive, or vice versa. Stabilized APIs impl Default for Rc<CStr> impl Default for Rc<str> impl Default for Rc<[T]> impl Default for Arc<str> impl Default for Arc<CStr> impl Default for Arc<[T]> impl IntoIterator for Box<[T]> impl FromIterator<String> for Box<str> impl FromIterator<char> for Box<str> LazyCell LazyLock Duration::div_duration_f32 Duration::div_duration_f64 Option::take_if Seek::seek_relative BinaryHeap::as_slice NonNull::offset NonNull::byte_offset NonNull::add NonNull::byte_add NonNull::sub NonNull::byte_sub NonNull::offset_from NonNull::byte_offset_from NonNull::read NonNull::read_volatile NonNull::read_unaligned NonNull::write NonNull::write_volatile NonNull::write_unaligned NonNull::write_bytes NonNull::copy_to NonNull::copy_to_nonoverlapping NonNull::copy_from NonNull::copy_from_nonoverlapping NonNull::replace NonNull::swap NonNull::drop_in_place NonNull::align_offset <[T]>::split_at_checked <[T]>::split_at_mut_checked str::split_at_checked str::split_at_mut_checked str::trim_ascii str::trim_ascii_start str::trim_ascii_end <[u8]>::trim_ascii <[u8]>::trim_ascii_start <[u8]>::trim_ascii_end Ipv4Addr::BITS Ipv4Addr::to_bits Ipv4Addr::from_bits Ipv6Addr::BITS Ipv6Addr::to_bits Ipv6Addr::from_bits Vec::<[T; N]>::into_flattened <[[T; N]]>::as_flattened <[[T; N]]>::as_flattened_mut These APIs are now stable in const contexts: <[T]>::last_chunk BinaryHeap::new Other changes Check out everything that changed in Rust , Cargo , and Clippy . Contributors to 1.80.0 Many people came together to create Rust 1.80.0. We couldn't have done it without all of you. Thanks! Get help! Documentation Contact the Rust Team Terms and policies Code of Conduct Licenses Logo Policy and Media Guide Security Disclosures All Policies Social RSS Main Blog "Inside Rust" Blog Maintained by the Rust Team. See a typo? Send a fix here !
======>
https://old.reddit.com/r/rust/comments/1g6ljge/any_resources_to_learn_how_exactly_lifetime/
-->>-->>
Hi,    

   I have managed to find some SO answers and reddit    posts    here that explain lifetime annotations, but what is bugging me that I can not find some more detailed descriptions of what exactly compiler is doing. Reading about subtyping and variance did not help.   
In particular:   

   
   here obviously x y and result can have different lifetimes, and all we want is to say that minimum (lifetime of x, lifetime y)  >= lifetime(result), I presume there is some rule that says that lifetime annotations behave differently (although they are all 'a) to give us desired logic, but I was unable to find exact rules that compiler uses. Again I know what this does and how to think about it in simple terms, but I wonder if there is more formal description, in particular what generic parameter lifetimes compiler tries to instantiate longest with at the call site(or is it just 1 deterministic lifetime he just tries and that is it)     fn longest<'a>(x: &'a str, y: &'a str) -> &'a str {fn longest<'a>(x: &'a str, y: &'a str) -> &'a str {   
    what exactly is a end of lifetime of a variable in rust? This may sound like a stupid question, but if you have 3 Vec variables defined in same scope and they all get dropped at the same } do their lifetime end at the same time as far as rust compiler is concerned? I ask because on the lower level obviously we will deallocate memory they hold in 3 different steps.   I have played around and it seems that all variables in same scope are considered to end at the same time from perspective of rust compiler since I do not think    this    would compile if there was ordering.    
   

   P.S. I know I do not need to learn this to use LA, but sometimes I have found that knowing underlying mechanism makes the "emergent" higher level behavior easier to remember even if I only ever operate with higher level, e.g. vector/deque iterator invalidation  in C++ is pain to remember unless you do know how vector/deque are implemented.      
   

======>
https://old.reddit.com/r/rust/comments/1g6us2s/which_companies_have_full_time_rust_project_like/
-->>-->>
I wonder which companies have full time rust project contributors and what are their roles or job descriptions?   

   Further has anyone managed to find a way to either convince their company to have rust project contributor position created?   
   

======>
https://blog.rust-lang.org/2024/10/17/Rust-1.82.0.html
-->>-->>
Rust Blog Rust Install Learn Tools Governance Community 🖌 Light Dark Announcing Rust 1.82.0 Oct. 17, 2024 · The Rust Release Team The Rust team is happy to announce a new version of Rust, 1.82.0. Rust is a programming language empowering everyone to build reliable and efficient software. If you have a previous version of Rust installed via rustup , you can get 1.82.0 with: $ rustup update stable If you don't have it already, you can get rustup from the appropriate page on our website, and check out the detailed release notes for 1.82.0 . If you'd like to help us out by testing future releases, you might consider updating locally to use the beta channel ( rustup default beta ) or the nightly channel ( rustup default nightly ). Please report any bugs you might come across! What's in 1.82.0 stable cargo info Cargo now has an info subcommand to display information about a package in the registry, fulfilling a long standing request just shy of its tenth anniversary! Several third-party extensions like this have been written over the years, and this implementation was developed as cargo-information before merging into Cargo itself. For example, here's what you could see for cargo info cc : cc #build-dependencies A build-time dependency for Cargo build scripts to assist in invoking the native
C compiler to compile native C code into a static archive to be linked into Rust
code. version: 1.1.23 (latest 1.1.30) license: MIT OR Apache-2.0 rust-version: 1.63 documentation: https://docs.rs/cc homepage: https://github.com/rust-lang/cc-rs repository: https://github.com/rust-lang/cc-rs crates.io: https://crates.io/crates/cc/1.1.23 features: jobserver = []
  parallel  = [dep:libc, dep:jobserver] note : to see how you depend on cc, run ` cargo tree --invert --package cc@1.1.23 ` By default, cargo info describes the package version in the local Cargo.lock , if any. As you can see, it will indicate when there's a newer version too, and cargo info cc@1.1.30 would report on that. Apple target promotions macOS on 64-bit ARM is now Tier 1 The Rust target aarch64-apple-darwin for macOS on 64-bit ARM (M1-family or later Apple Silicon CPUs) is now a tier 1 target, indicating our highest guarantee of working properly. As the platform support page describes, every change in the Rust repository must pass full tests on every tier 1 target before it can be merged. This target was introduced as tier 2 back in Rust 1.49, making it available in rustup . This new milestone puts the aarch64-apple-darwin target on par with the 64-bit ARM Linux and the X86 macOS, Linux, and Windows targets. Mac Catalyst targets are now Tier 2 Mac Catalyst is a technology by Apple that allows running iOS applications natively on the Mac. This is especially useful when testing iOS-specific code, as cargo test --target=aarch64-apple-ios-macabi --target=x86_64-apple-ios-macabi mostly just works (in contrast to the usual iOS targets, which need to be bundled using external tooling before they can be run on a native device or in the simulator). The targets are now tier 2, and can be downloaded with rustup target add aarch64-apple-ios-macabi x86_64-apple-ios-macabi , so now is an excellent time to update your CI pipeline to test that your code also runs in iOS-like environments. Precise capturing use<..> syntax Rust now supports use<..> syntax within certain impl Trait bounds to control which generic lifetime parameters are captured. Return-position impl Trait (RPIT) types in Rust capture certain generic parameters.  Capturing a generic parameter allows that parameter to be used in the hidden type.  That in turn affects borrow checking. In Rust 2021 and earlier editions, lifetime parameters are not captured in opaque types on bare functions and on functions and methods of inherent impls unless those lifetime parameters are mentioned syntactically in the opaque type.  E.g., this is an error: //@ edition: 2021 fn f (x: &()) -> impl Sized { x } error[E0700]: hidden type for `impl Sized` captures lifetime that does not appear in bounds
 --> src/main.rs:1:30
  |
1 | fn f(x: &()) -> impl Sized { x }
  |         ---     ----------   ^
  |         |       |
  |         |       opaque type defined here
  |         hidden type `&()` captures the anonymous lifetime defined here
  |
help: add a `use<...>` bound to explicitly capture `'_`
  |
1 | fn f(x: &()) -> impl Sized + use<'_> { x }
  |                            +++++++++ With the new use<..> syntax, we can fix this, as suggested in the error, by writing: fn f (x: &()) -> impl Sized + use < '_ > { x } Previously, correctly fixing this class of error required defining a dummy trait, conventionally called Captures , and using it as follows: trait Captures <T: ? Sized > {} impl <T: ? Sized , U: ? Sized > Captures<T> for U {} fn f (x: &()) -> impl Sized + Captures<& '_ ()> { x } That was called "the Captures trick" , and it was a bit baroque and subtle.  It's no longer needed. There was a less correct but more convenient way to fix this that was often used called "the outlives trick" .  The compiler even previously suggested doing this.  That trick looked like this: fn f (x: &()) -> impl Sized + '_ { x } In this simple case, the trick is exactly equivalent to + use<'_> for subtle reasons explained in RFC 3498 .  However, in real life cases, this overconstrains the bounds on the returned opaque type, leading to problems.  For example, consider this code, which is inspired by a real case in the Rust compiler: struct Ctx < 'cx >(& 'cx u8 ); fn f < 'cx , 'a >(
    cx: Ctx< 'cx >,
    x: & 'a u8 ,
) -> impl Iterator <Item = & 'a u8 > + 'cx {
    core::iter::once_with( move || {
        eprintln!( "LOG: {}" , cx. 0 );
        x
    }) //~^ ERROR lifetime may not live long enough } We can't remove the + 'cx , since the lifetime is used in the hidden type and so must be captured.  Neither can we add a bound of 'a: 'cx , since these lifetimes are not actually related and it won't in general be true that 'a outlives 'cx .  If we write + use<'cx, 'a> instead, however, this will work and have the correct bounds. There are some limitations to what we're stabilizing today.  The use<..> syntax cannot currently appear within traits or within trait impls (but note that there, in-scope lifetime parameters are already captured by default), and it must list all in-scope generic type and const parameters.  We hope to lift these restrictions over time. Note that in Rust 2024, the examples above will "just work" without needing use<..> syntax (or any tricks).  This is because in the new edition, opaque types will automatically capture all lifetime parameters in scope.  This is a better default, and we've seen a lot of evidence about how this cleans up code.  In Rust 2024, use<..> syntax will serve as an important way of opting-out of that default. For more details about use<..> syntax, capturing, and how this applies to Rust 2024, see the "RPIT lifetime capture rules" chapter of the edition guide.  For details about the overall direction, see our recent blog post, "Changes to impl Trait in Rust 2024" . Native syntax for creating a raw pointer Unsafe code sometimes has to deal with pointers that may dangle, may be misaligned, or may not point to valid data. A common case where this comes up are repr(packed) structs. In such a case, it is important to avoid creating a reference, as that would cause undefined behavior. This means the usual & and &mut operators cannot be used, as those create a reference -- even if the reference is immediately cast to a raw pointer, it's too late to avoid the undefined behavior. For several years, the macros std::ptr::addr_of! and std::ptr::addr_of_mut! have served this purpose. Now the time has come to provide a proper native syntax for this operation: addr_of!(expr) becomes &raw const expr , and addr_of_mut!(expr) becomes &raw mut expr . For example: #[repr(packed)] struct Packed {
    not_aligned_field: i32 ,
} fn main () { let p = Packed { not_aligned_field: 1_82 }; // This would be undefined behavior! // It is rejected by the compiler. //let ptr = &p.not_aligned_field as *const i32; // This is the old way of creating a pointer. let ptr = std::ptr::addr_of!(p.not_aligned_field); // This is the new way. let ptr = &raw const p.not_aligned_field; // Accessing the pointer has not changed. // Note that `val = *ptr` would be undefined behavior because // the pointer is not aligned! let val = unsafe { ptr.read_unaligned() };
} The native syntax makes it more clear that the operand expression of these operators is interpreted as a place expression . It also avoids the term "address-of" when referring to the action of creating a pointer. A pointer is more than just an address , so Rust is moving away from terms like "address-of" that reaffirm a false equivalence of pointers and addresses. Safe items with unsafe extern Rust code can use functions and statics from foreign code.  The type signatures of these foreign items are provided in extern blocks.  Historically, all items within extern blocks have been unsafe to use, but we didn't have to write unsafe anywhere on the extern block itself. However, if a signature within the extern block is incorrect, then using that item will result in undefined behavior.  Would that be the fault of the person who wrote the extern block, or the person who used that item? We've decided that it's the responsibility of the person writing the extern block to ensure that all signatures contained within it are correct, and so we now allow writing unsafe extern : unsafe extern { pub safe static TAU: f64 ; pub safe fn sqrt (x: f64 ) -> f64 ; pub unsafe fn strlen (p: * const u8 ) -> usize ;
} One benefit of this is that items within an unsafe extern block can be marked as safe to use.  In the above example, we can call sqrt or read TAU without using unsafe .  Items that aren't marked with either safe or unsafe are conservatively assumed to be unsafe . In future releases, we'll be encouraging the use of unsafe extern with lints.  Starting in Rust 2024, using unsafe extern will be required. For further details, see RFC 3484 and the "Unsafe extern blocks" chapter of the edition guide. Unsafe attributes Some Rust attributes, such as no_mangle , can be used to cause undefined behavior without any unsafe block . If this were regular code we would require them to be placed in an unsafe {} block, but so far attributes have not had comparable syntax. To reflect the fact that these attributes can undermine Rust's safety guarantees, they are now considered "unsafe" and should be written as follows: #[unsafe(no_mangle)] pub fn my_global_function () { } The old form of the attribute (without unsafe ) is currently still accepted, but might be linted against at some point in the future, and will be a hard error in Rust 2024. This affects the following attributes: no_mangle link_section export_name For further details, see the "Unsafe attributes" chapter of the edition guide. Omitting empty types in pattern matching Patterns which match empty (a.k.a. uninhabited) types by value can now be omitted: use std::convert::Infallible; pub fn unwrap_without_panic <T>(x: Result <T, Infallible>) -> T { let Ok (x) = x; // the `Err` case does not need to appear x
} This works with empty types such as a variant-less enum Void {} , or structs and enums with a visible empty field and no #[non_exhaustive] attribute. It will also be particularly useful in combination with the never type ! , although that type is still unstable at this time. There are some cases where empty patterns must still be written. For reasons related to uninitialized values and unsafe code, omitting patterns is not allowed if the empty type is accessed through a reference, pointer, or union field: pub fn unwrap_ref_without_panic <T>(x: & Result <T, Infallible>) -> &T { match x { Ok (x) => x, // this arm cannot be omitted because of the reference Err (infallible) => match *infallible {},
    }
} To avoid interfering with crates that wish to support several Rust versions, match arms with empty patterns are not yet reported as “unreachable code” warnings, despite the fact that they can be removed. Floating-point NaN semantics and const Operations on floating-point values (of type f32 and f64 ) are famously subtle. One of the reasons for this is the existence of NaN ("not a number") values which are used to represent e.g. the result of 0.0 / 0.0 . What makes NaN values subtle is that more than one possible NaN value exists. A NaN value has a sign (that can be checked with f.is_sign_positive() ) and a payload (that can be extracted with f.to_bits() ). However, both the sign and payload of NaN values are entirely ignored by == (which always returns false ). Despite very successful efforts to standardize the behavior of floating-point operations across hardware architectures, the details of when a NaN is positive or negative and what its exact payload is differ across architectures. To make matters even more complicated, Rust and its LLVM backend apply optimizations to floating-point operations when the exact numeric result is guaranteed not to change, but those optimizations can change which NaN value is produced. For instance, f * 1.0 may be optimized to just f . However, if f is a NaN, this can change the exact bit pattern of the result! With this release, Rust standardizes on a set of rules for how NaN values behave. This set of rules is not fully deterministic, which means that the result of operations like (0.0 / 0.0).is_sign_positive() can differ depending on the hardware architecture, optimization levels, and the surrounding code. Code that aims to be fully portable should avoid using to_bits and should use f.signum() == 1.0 instead of f.is_sign_positive() . However, the rules are carefully chosen to still allow advanced data representation techniques such as NaN boxing to be implemented in Rust code. For more details on what the exact rules are, check out our documentation . With the semantics for NaN values settled, this release also permits the use of floating-point operations in const fn . Due to the reasons described above, operations like (0.0 / 0.0).is_sign_positive() (which will be const-stable in Rust 1.83) can produce a different result when executed at compile-time vs at run-time. This is not a bug, and code must not rely on a const fn always producing the exact same result. Constants as assembly immediates The const assembly operand now provides a way to use integers as immediates
without first storing them in a register. As an example, we implement a syscall to write by hand: const WRITE_SYSCALL: c_int = 0x01 ; // syscall 1 is `write` const STDOUT_HANDLE: c_int = 0x01 ; // `stdout` has file handle 1 const MSG: & str = "Hello, world!\n" ; let written: usize ; // Signature: `ssize_t write(int fd, const void buf[], size_t count)` unsafe {
    core::arch::asm!( "mov rax, {SYSCALL} // rax holds the syscall number" , "mov rdi, {OUTPUT}  // rdi is `fd` (first argument)" , "mov rdx, {LEN}     // rdx is `count` (third argument)" , "syscall            // invoke the syscall" , "mov {written}, rax // save the return value" ,
        SYSCALL = const WRITE_SYSCALL,
        OUTPUT = const STDOUT_HANDLE,
        LEN = const MSG.len(), in ( "rsi" ) MSG.as_ptr(), // rsi is `buf *` (second argument) written = out(reg) written,
    );
} assert_eq! (written, MSG.len()); Output: Hello, world! Playground link . In the above, a statement such as LEN = const MSG.len() populates the format
specifier LEN with an immediate that takes the value of MSG.len() . This can be seen
in the generated assembly (the value is 14 ): lea     rsi, [rip + .L__unnamed_3]
mov     rax, 1    # rax holds the syscall number
mov     rdi, 1    # rdi is `fd` (first argument)
mov     rdx, 14   # rdx is `count` (third argument)
syscall # invoke the syscall
mov     rax, rax  # save the return value See the reference for more details. Safely addressing unsafe static s This code is now allowed: static mut STATIC_MUT: Type = Type::new(); extern "C" { static EXTERN_STATIC: Type;
} fn main () { let static_mut_ptr = &raw mut STATIC_MUT; let extern_static_ptr = &raw const EXTERN_STATIC;
} In an expression context, STATIC_MUT and EXTERN_STATIC are place expressions . Previously, the compiler's safety checks were not aware that the raw ref operator did not actually affect the operand's place, treating it as a possible read or write to a pointer. No unsafety is actually present, however, as it just creates a pointer. Relaxing this may cause problems where some unsafe blocks are now reported as unused if you deny the unused_unsafe lint, but they are now only useful on older versions. Annotate these unsafe blocks with #[allow(unused_unsafe)] if you wish to support multiple versions of Rust, as in this example diff: static mut STATIC_MUT: Type = Type::new();
 fn main() {
+    #[allow(unused_unsafe)]
     let static_mut_ptr = unsafe { std::ptr::addr_of_mut!(STATIC_MUT) };
 } A future version of Rust is expected to generalize this to other expressions which would be safe in this position, not just statics. Stabilized APIs std::thread::Builder::spawn_unchecked std::str::CharIndices::offset std::option::Option::is_none_or [T]::is_sorted [T]::is_sorted_by [T]::is_sorted_by_key Iterator::is_sorted Iterator::is_sorted_by Iterator::is_sorted_by_key std::future::Ready::into_inner std::iter::repeat_n impl<T: Clone> DoubleEndedIterator for Take<Repeat<T>> impl<T: Clone> ExactSizeIterator for Take<Repeat<T>> impl<T: Clone> ExactSizeIterator for Take<RepeatWith<T>> impl Default for std::collections::binary_heap::Iter impl Default for std::collections::btree_map::RangeMut impl Default for std::collections::btree_map::ValuesMut impl Default for std::collections::vec_deque::Iter impl Default for std::collections::vec_deque::IterMut Rc<T>::new_uninit Rc<T>::assume_init Rc<[T]>::new_uninit_slice Rc<[MaybeUninit<T>]>::assume_init Arc<T>::new_uninit Arc<T>::assume_init Arc<[T]>::new_uninit_slice Arc<[MaybeUninit<T>]>::assume_init Box<T>::new_uninit Box<T>::assume_init Box<[T]>::new_uninit_slice Box<[MaybeUninit<T>]>::assume_init core::arch::x86_64::_bextri_u64 core::arch::x86_64::_bextri_u32 core::arch::x86::_mm_broadcastsi128_si256 core::arch::x86::_mm256_stream_load_si256 core::arch::x86::_tzcnt_u16 core::arch::x86::_mm_extracti_si64 core::arch::x86::_mm_inserti_si64 core::arch::x86::_mm_storeu_si16 core::arch::x86::_mm_storeu_si32 core::arch::x86::_mm_storeu_si64 core::arch::x86::_mm_loadu_si16 core::arch::x86::_mm_loadu_si32 core::arch::wasm32::u8x16_relaxed_swizzle core::arch::wasm32::i8x16_relaxed_swizzle core::arch::wasm32::i32x4_relaxed_trunc_f32x4 core::arch::wasm32::u32x4_relaxed_trunc_f32x4 core::arch::wasm32::i32x4_relaxed_trunc_f64x2_zero core::arch::wasm32::u32x4_relaxed_trunc_f64x2_zero core::arch::wasm32::f32x4_relaxed_madd core::arch::wasm32::f32x4_relaxed_nmadd core::arch::wasm32::f64x2_relaxed_madd core::arch::wasm32::f64x2_relaxed_nmadd core::arch::wasm32::i8x16_relaxed_laneselect core::arch::wasm32::u8x16_relaxed_laneselect core::arch::wasm32::i16x8_relaxed_laneselect core::arch::wasm32::u16x8_relaxed_laneselect core::arch::wasm32::i32x4_relaxed_laneselect core::arch::wasm32::u32x4_relaxed_laneselect core::arch::wasm32::i64x2_relaxed_laneselect core::arch::wasm32::u64x2_relaxed_laneselect core::arch::wasm32::f32x4_relaxed_min core::arch::wasm32::f32x4_relaxed_max core::arch::wasm32::f64x2_relaxed_min core::arch::wasm32::f64x2_relaxed_max core::arch::wasm32::i16x8_relaxed_q15mulr core::arch::wasm32::u16x8_relaxed_q15mulr core::arch::wasm32::i16x8_relaxed_dot_i8x16_i7x16 core::arch::wasm32::u16x8_relaxed_dot_i8x16_i7x16 core::arch::wasm32::i32x4_relaxed_dot_i8x16_i7x16_add core::arch::wasm32::u32x4_relaxed_dot_i8x16_i7x16_add These APIs are now stable in const contexts: std::task::Waker::from_raw std::task::Context::from_waker std::task::Context::waker $integer::from_str_radix std::num::ParseIntError::kind Other changes Check out everything that changed in Rust , Cargo , and Clippy . Contributors to 1.82.0 Many people came together to create Rust 1.82.0. We couldn't have done it without all of you. Thanks! Get help! Documentation Contact the Rust Team Terms and policies Code of Conduct Licenses Logo Policy and Media Guide Security Disclosures All Policies Social RSS Main Blog "Inside Rust" Blog Maintained by the Rust Team. See a typo? Send a fix here !
======>
https://dirkjan.ochtman.nl/writing/2024/10/18/september-2024-on-github.html
-->>-->>
Dirkjan Ochtman: writing September on GitHub (2024) Published on 2024-10-18 by Dirkjan Ochtman in tech , code , rust Since leaving my job at the end of August, I figured I would try to write up
a report of all the open source stuff I worked on in September.
Herewith a not quite complete account of things I was involved in. rustls rustls is a pure Rust implementation of the TLS protocol. Substantial effort last month went into migrating support for PEM decoding from the rustls-pemfile crate (backed by the base64 crate) to the rustls-pki-types crate. While
the new decoder is slightly slower, it avoids timing side channel attacks (only used for PEM items containing
private keys). We iterated a number of times on making the new API easy to use. Adopting the new APIs should
shrink dependency graphs by getting rid of both rustls-pemfile (which got a semver-compatible bump
to use the new implementation ) and the base64 crate. We went through a number of iterations reviewing a PR from Bart Dubbeldam and Adolfo Ochagavía adding support for RFC 7250 raw public keys . ComplexSpaces added deployment considerations to the
rustls-platform-verifier README. We now recommend (almost) everyone use the platform verifier for their server
certificate verification needs. Joe has been doing interesting work to improve performance: We can now send flights of handshake messages in a single message. He eliminated an allocation in signature verification. And eliminated large copies in resumption code paths. I contributed some smaller improvements last month: I finally converted one of the rustls examples from the
long-dead docopt to clap. Joe kindly offered to take care of the others. I finished a contributed PR to enable signing without a
private key in rcgen. Clarified error handling in rustls-native-certs. Hickory DNS Hickory DNS is a project to
build a comprehensive suite of Rust libraries to build DNS services on top of.
Because the project is nearing a (fairly large) feature release, I've been trying
to make a number of improvements to the project, cleaning up the API and moving
code around to reduce complexity. Here are some of the things I worked on: I moved the RuntimeProvider trait from hickory-resolver down into
hickory-proto. This abstraction was created to allow the resolver to abstract
over different async runtimes (and their I/O traits and types), but it did
not actually depend on the resolver infrastructure itself. Meanwhile, at the
lower -proto level we had a somewhat parallel set of abstractions, so I set
out to move the RuntimeProvider into -proto and try to make it the focal
point of abstracting over runtimes and I/O. This work was split over two
PRs, one for UDP and one for TCP . I should probably review whether there is
potential for further simplification in the other protocols (H2, QUIC, H3). Hickory DNS depends on the resolve-conf crate in order to find the system DNS configuration (on Unix systems). Unfortunately
the resolv-conf crate has been unmaintained for a while. While attempts to contact
the maintainer didn't lead to any results, Ferrous team member Andrei
Listochkin mentioned that he knew the original
maintainer and could talk to them via Telegram. Soon, the repository and
crate ownership had been transferred to us, so that we can start fixing some
of the outstanding issues soon. After reviewing a PR to allow customization of the DNS over HTTP URL path I noticed that the configuration
for the main Hickory DNS binary crate was distributed over the -server library crate
and the hickory-dns binary crate. Some of this didn't really make sense, so I
submitted a collection of config tweaks to improve on the situation. While reviewing another PR ,
I noticed that the hickory-server crates had a bunch of instances of Arc<Box<dyn AuthorityObject>> , which seemed unnecessarily complex. I cleaned these up so that we use Arc<dyn AuthorityObject> instead. After we noticed that some tests against the Quad9 name servers kept causing issues in CI, I decided
to kill the quad9 tests .
We still run tests against Google and Cloudflare to help validate our code. A user reported that the Cargo features for the server were pulling in OpenSSL even when that
should be necessary. I submitted a PR to clean up server features which improved the situation. While working on KumoMTA (see below), I noticed that RecordSet::new() took a &Name argument only to clone it. I changed it to take a Name directly so the caller can hand over ownership where possible. Made some minor recursor tweaks . I reviewed 25 PRs Hickory DNS last month, here are the more notable ones: Chained authority implementation CNAME resolution support for the recursor NSEC3 validation rustup rustup is the tool most Rust developers to use manage their compiler toolchains; I joined
the team last year after some of the previous members had left, reducing the available
capacity. We're currently gearing up for the beta of rustup 1.28 which contains some larger changes, including: No implicit toolchain installs Default to rustls with the rustls-platform-verifier for downloads Improved logging based on tracing-rs In September I recruited Chris to the team. He had already been helping out a bunch with the Windows parts of rustup in particular, and
we decided it made sense to ask him to join the team, which he happily agreed to. Notable changes from last month: We finally merged a contributed PR to replace winreg with windows registry ,
after we coordinated with the windows-rs team and Chris on the best way forward. This included scrapping some unsafe code from the rustup code base because we can now build on better upstream abstractions. Improve the logging setup to better integrate tracing. pyrtls I launched a new project called pyrtls last month.
pyrtls provides Python bindings to rustls, seeking to expand access to rustls' great
performance and security track record as an alternative to the OpenSSL-based ssl module
that is a part of the standard library. I started working on this intermittently back in 2021 and finally decided to write up some
documentation and "ship" it by announcing the project on social media. At the end of the month,
it earned a mention in last month's issue of the Feisty Duck Cryptography and Security Newsletter.
I was happy to receive my first contributed PR adding a typing stub a few days later. Although the project has managed to earn 70 stars over the past
4 weeks, I've been a little disappointed with the lack of feedback so far. Given that I don't have
a use case for this project myself (since I don't really use Python anymore), I probably won't
spend much more time on it until there is some uptake. KumoMTA I've been doing some consulting work for KumoMTA , a startup building
a mail transfer agent (MTA) for enterprise senders. I had connected with their cofounder Wez (of wezterm fame) through
issues against rustls and Hickory (both of which KumoMTA uses to build their MTA). My first issue was to build a Redis rate limiting implementation that doesn't use the redis-cell Redis extension. This integrated a Lua
script-based implementation of the generic cell rate algorithm which can be used in deployments where custom Redis extensions are not viable. Then, I started working on an implementation of the Sender Policy Framework (SPF) standard. This will be used in KumoMTA to help
detect spam. Wez had already started on an implementation, so I picked up from
where he had left off and started implementing the core protocol. I noticed that the KumoMTA codebase was using both lazy-static and once_cell , so I decided
to spend a little time (after getting approval from Wez, of course) cleaning this up and migrating the entire project to the newly available OnceLock and LazyLock types stabilized in 1.70.0 and 1.80.0. Quinn Quinn is the most popular Rust implementation of the QUIC transport protocol. Some interesting PRs I
reviewed last month: Enabling building quinn-proto for WASM targets from the folks
at n0 computer, who are building Iroh on top of Quinn. Faster UDP I/O on Apple platforms , from the Mozilla folks
who are in the process of adopting quinn-udp for Firefox's UDP needs. I also submitted some PRs of my own: Fixed bookkeeping for datagrams in response to an issue. Cleaned up some new clippy lints. Avoided a potential panic in TLS server config setup. Published 0.11.7 and 0.11.8 releases for quinn-proto and quinn.
We also investigated a DoS vulnerability that was introduced in quinn-proto 0.11.0 and
published a security advisory after 0.11.7 was released. Published 0.5.5 for quinn-udp. Askama Askama is a compile-time template engine using a Jinja-like syntax. Reviewed some changes to: Fix include statements within block fragments. Improve CI to run all tests and packages to completion even if some fail. Accept a trailing comma in arrays. I also: Improved the error in case a Rust operator is used as a
delimiter in a custom syntax, in response to an issue report. Applied clippy suggestions from Rust 1.81 and
fixed dependencies after a new version of Axum was published that uses tower 0.5 . I should really reserve some time to finish the 0.13 release . tokio I prepared a tokio-stream release to support
some work I was doing in tonic to improve newly public API . Although I am only peripherally involved with the tracing-rs project, I spent some time cleaning up
all the warnings that had accumulated over time. Reviewed a tracing-opentelemetry upgrade to opentelemetry 0.25 and published the 0.26.0 release. instant-acme instant-acme is a RFC 8555 client for provisioning TLS certificates. Reviewed a contributed PR to avoid bad nonce errors which the Pebble test CA generates to exercise client code. As suggested by the spec, we now retry
requests that trigger a bad nonce error a few times before giving up. On sustainability Although I have been reaching out to organizations that I'd hoped would be interested in funding this
kind of work across the Rust ecosystem, so far I have not succeeded in finding any. Although I'm very
grateful to my current sponsors, I'd like to find some companies that can help make my work more
sustainable. Happy to get on a call to discuss how I could help out your team! Many thanks to these sponsors (5 USD/month or more): astral-sh codecov thomaseizinger stepfunc MJDSys repi sourcegraph eightseventhreethree paolobarbolini Blog Archives Feed Elsewhere Email GitHub Mastodon Twitter Resume Patreon LinkedIn Projects Askama Quinn rnc2rng AWMY abna nnpy Galatna Jasinja Open source Mozilla Gentoo Linux Powered by Pelican .
				Copyright 2012-2018 by Dirkjan Ochtman. // <![CDATA[
			
			var bits = window.location.hostname.split('.');
			if (bits.length > 3) bits = bits.slice(bits.length - 3);
			var href = 'mailto:' + bits[0] + '@' + bits[1] + '.' + bits[2];
			document.getElementById('mail-link').setAttribute('href', href);
			
			// ]]> var _paq = _paq || [];
  		_paq.push(['trackPageView']);
  		_paq.push(['enableLinkTracking']);
  		(function() {
    		var u=(("https:" == document.location.protocol) ? "https" : "http") + "://piwik.xavamedia.nl/";
    		_paq.push(['setTrackerUrl', u+'piwik.php']);
    		_paq.push(['setSiteId', 1]);
    		var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0]; g.type='text/javascript';
    		g.defer=true; g.async=true; g.src=u+'piwik.js'; s.parentNode.insertBefore(g,s);
  		})(); <p><img src="http://piwik.xavamedia.nl/piwik.php?idsite=1" style="border:0;" alt="" /></p>
======>
https://old.reddit.com/r/rust/comments/1g6gk7k/introducing_userp_a_batteries_included_user/
-->>-->>
Hey guys!   

   TL;DR: I'm making an    Auth thing called Userp    and you're welcome to join me!   

   I've been migrating a webapp from Next JS to Leptos for the past month or so. One of the things I ran in to was the lack of a batteries included user management system. Specifically, I needed something that would handle magic link logins and cross-linking OAuth accounts. In Next Auth I was able to get this working without too many hacks, but when I went to use the otherwise excellent axum-login crate I didn't find the abstractions particularly ergonomic for my use-case.   

   Like everyone else I've been warned against rolling my own auth, but having ignored that I got to work. After all, as the meme proclaims; we are developers! We don't do things because they are easy. We do them because we    thought    they were going to be easy.   

   What I have to show for a few weeks of work is    this   . It's still    very    early, and subject to a lot of API changes, but the essential parts are there. Like with axum-login you    implement a few traits   , including    a store   ,    hook it up to Axum   , use    an extractor   , and you're off. Like Next Auth there are ready-made routes with login and signup screens (Askama-based for now), and additionally there is a small account management page where the user can handle their OAuth tokens, verify their email addresses, manage their login sessions and so on. All of this is optional of course - if you just want an axum-extracted auth engine that's fine too. Speaking of which, I originally called it axum-user, but I'm very open to "porting it" to actix as well! Don't know the first thing about it though and would happily receive contributions.   

   This goes for any part of the project, btw! Even reading through the code and critiquing the API would be most helpful (and sligthly embarassing, but hey, it's early). My hope is that this will turn into a collaborative effort :)   
   

======>
https://old.reddit.com/r/rust/comments/1g6jfjh/announcing_rerun_019_video_and_dataframe_support/
-->>-->>
Rerun is an easy-to-use visualization toolbox and data platform for multimodal and temporal data.   

   Rerun is written in Rust, using egui.   

   Rerun 0.19 adds video support, as well as a new dataframe view and API.   

   
   Crate:    https://crates.io/crates/rerun   
   Release notes:    https://github.com/rerun-io/rerun/releases/tag/0.19.0   
   Blog post:    https://rerun.io/blog/dataframe   
   Web demo:    https://rerun.io/viewer   
   

   Feel free to ask any questions :)   
   

======>
https://old.reddit.com/r/rust/comments/1g6rpye/made_my_first_rust_project_finally_a_markdown/
-->>-->>
This is a small app that I wanted to build for a long time, and finally got around to it. It basically transpiles a md file into an html and serves it to the user via localhost (or any address you wish with the --host flag, including    0.0.0.0    for hosting on the local network), refreshing automatically if the file is changed.    

   https://preview.redd.it/pgwnttb7pkvd1.png?width=2880&format=png&auto=webp&s=6adf452d7306580a2f8fba5ade2f6a06bf63fcf2   

   Could imagine it being useful for some people for previewing READMEs while editing them or sharing notes with people on the same local network. Or just maybe nice notes are your thing and you want to touch yourself whilst looking at them. Whatever man, I don't judge. I do that too sometimes.   

   It's called omd. It's on    crates.io    and    github   . Hope you find it useful, Rustaceans!   
   

======>
https://lwn.net/Articles/992055/
-->>-->>
Smart pointers for the kernel We're bad at marketing We can admit it, marketing is not our strong suit. Our strength is
writing the kind of articles that developers, administrators, and
free-software supporters depend on to know what is going on in the
Linux world. Please subscribe today to help us keep doing that, and so
we don’t have to get good at marketing. By Daroc Alden October 4, 2024 Kangrejos 2024 Rust has a plethora of smart-pointer types, including reference-counted
pointers, which have special support in the compiler to make them
easier to use. The Rust-for-Linux project would like to reap those same benefits
for its smart pointers, which need to be written by hand to conform to
the Linux kernel
memory model . Xiangfei Ding
presented at Kangrejos about the work to enable custom
smart pointers to function the same as built-in smart pointers. Ding showed the specific "superpowers" that built-in smart pointers have in his slides :
unsizing and dynamic dispatch. Unsizing allows the programmer
to remove the length of an array behind a pointer from its type,
turning
a Ptr<[T; N]> (bounds-checked at compile time) into a Ptr<[T]> (bounds-checked at run time). This needs special support because slices
(values of type [T] ) do not have a known size at compile time; therefore the compiler
needs to store the size somewhere at run time. The compiler could store the size
in the pointed-to allocation, but that would require reallocating the array's
memory, which would be expensive. Instead, the compiler stores the size
alongside the pointer itself, as a fat pointer. On nightly Rust compilers, users
can enable an experimental feature and then have their
pointer type implement CoerceUnsized to indicate that it supports that. The second superpower is called DispatchFromDyn and allows converting a Ptr<T> into a Ptr<dyn
Trait> when T implements Trait . This has to do with
the way that Rust implements dynamic dispatch — a value of type Ptr<dyn
Trait> uses a dispatch table to find the implementation of the method
being invoked at run time.
That method expects to receive a self pointer. So converting a smart
pointer to use dynamic dispatch only works when the smart pointer can be used as
a self pointer. These features are both experimental, because the Rust project is still working
on their design. Ding explained that there is an RFC aimed at stabilizing just enough for the Linux kernel to use, without
impeding the development of the features. The RFC would add a new macro
that makes it trivial for
a smart pointer satisfying certain requirements to implement the
necessary traits, no matter what the final forms of the traits end up looking
like. That would let the kernel start using its custom smart pointers on stable
Rust sooner rather than later. There is one catch — implementing these features for a smart-pointer type with a
malicious or broken Deref (the trait that lets a programmer dereference a value)
implementation could break the guarantees
Rust relies on to determine when objects can be moved in memory.
This is of particular importance to Pin ,
which is a wrapper type used to mark an allocation that cannot be moved.
It's not hard to write smart-pointer types that don't cause problems,
but in keeping with Rust's
commitment to ensuring safe code cannot cause memory-safety problems, the RFC
also requires programmers to use unsafe (specifically, implementing an unsafe marker trait ) as a promise that
they've read the relevant documentation and are not going to break Pin .
With that addition, the code for a smart-pointer type would look like this: // Use Ding's macro ...
    #[derive(SmartPointer)]
    // On a struct that is just a wrapper around a pointer
    #[repr(transparent)]
    struct MySmartPointer<T: ?Sized>(Box<T>);

    // Implement Deref, with whatever custom logic is needed
    impl<T: ?Sized> Deref for MySmartPointer<T> {
        type Target = T;
        fn deref(&self) -> &T {
            ...
        }
    }

    // And then promise the compiler that the Deref implementation is okay to
    // use in conjunction with Pin:
    unsafe impl<T: ?Sized> PinCoerceUnsized for MySmartPointer<T> {} Andreas Hindborg asked for some clarification about why the marker trait is
needed. Deref is supposed to be simple, Ding explained. Usually, someone writing a
smart-pointer type would have a normal pointer stored in their type; when implementing Deref , they can just use the normal pointer. But it's technically
possible to implement something more complicated than that. In this case, you
could have a Deref implementation that actually moves data out of the
object pointed to and stores something else there. This would not normally be a problem,
except when the smart pointer is contained in a Pin , which is supposed
to prevent the value from being moved. If the Deref implementation
moves the value anyway, then that would be undefined behavior. The unsafe marker
trait is a promise to the compiler that the programmer has not done that. The new macro is available on nightly Rust, although Ding says that it needs a
bit more testing in order to stabilize, as well as some additional documentation
which he is working on. Miguel Ojeda asked how soon the macro might be
stabilized; Ding answered that it should be quite soon. He will make a
stabilization report shortly, and then it is just a matter of checking off the
requirements. Index entries for this article Kernel Development tools/Rust Conference Kangrejos/2024 to post comments Unsafe Posted Oct 5, 2024 16:11 UTC (Sat)
                               by tialaramex (subscriber, #21167)
                              [ Link ] (2 responses) I'm glad to see this requires the unsafe marker upholding Rust's principles here. I implement Deref and DerefMut for misfortunate::Double which (the whole point of the misfortunate crate) does something that's legal in safe Rust but is probably not what you wanted. in this case although dereferencing a Double<T> gets you access to a T, *mutably* dereferencing it gets you a different T! There are two inside it (hence the name) but they appear to be singular from outside. I don't know whether it would be safe for this to have the marker or not, but because it's unsafe I know I can just not implement it and never need to know. Unsafe Posted Oct 6, 2024 15:13 UTC (Sun)
                               by daroc (editor, #160859)
                              [ Link ] (1 responses) Although I'm not familiar with misfortunate::Double, that sounds like a central example of a type that is not safe to do these conversions on. Unsafe Posted Oct 8, 2024 0:25 UTC (Tue)
                               by NYKevin (subscriber, #129325)
                              [ Link ] It's not possible (at compile-time) to do these coercions on Double, because the unsizing coercion is only legal for types that are layout-identical to a single raw pointer. More prosaically, Double does not allow T: ?Sized, so it wouldn't be possible even if it were possible. But if neither of those issues were present, this would be entirely fine since Double::swap() takes a &mut self parameter, and you only violate this invariant if you re-seat the pointer without calling such a method. Pin does not allow borrowing either the pointer or the pointee mutably (unless the pointee is Unpin), so it would prevent you from calling swap(), and everything would be entirely sound. Rust vs. C Posted Oct 6, 2024 7:54 UTC (Sun)
                               by smurf (subscriber, #17840)
                              [ Link ] (17 responses) … and that's the difference between Rust and C, right here. With Rust you add funky semi-comprehensible macros to your classes to tell the compiler which invariants your class and/or pointers to its members requires (and obeys). You don't do that? people will have problems using your class. With C you write simple and legible code (or what looks like such) and document the invariants in comments or documentation (or not). Your users can walk right over them and nobody cares — until the kernel BUGs on you, that is. Or worse. Rust vs. C Posted Oct 6, 2024 13:38 UTC (Sun)
                               by tux3 (subscriber, #101245)
                              [ Link ] (15 responses) I would nitpick that the culture of annotating invariants in comment or documentation is certainly something I'd like to see more on the C side. Methinks we'd need a lot less tooling enforcement if people always diligently thought about (and documented!) all the important invariants their API expects. Rust vs. C Posted Oct 6, 2024 21:11 UTC (Sun)
                               by kleptog (subscriber, #1183)
                              [ Link ] (10 responses) The main problem with documenting API invariants via comments is that they'll be wrong after the first refactor. Unless there's a process to actually validate them, they're almost certainly going to be wrong fairly quickly. Rust vs. C Posted Oct 7, 2024 16:42 UTC (Mon)
                               by carlosrodfern (subscriber, #166486)
                              [ Link ] (9 responses) This is so true in the company environment with the time pressure. The company puts deadlines over excellency in many cases and it is satisfied with a "good enough" code quality. However, in the Open Source projects, if there is no time pressure (and usually there isn't), you can take your time to get it as perfect as you want, and demand that from your contributors. It depends a lot on the main maintainer's perspective on code quality discipline. In the case of Linux, they have Linus, and he has done a great job communicating the culture of excellency over deadlines. However, once the key individual of a project leaves or changes code quality perspective, there is a high risk of things "relaxing" and falling apart. Documented invariants versus invariants in code Posted Oct 7, 2024 17:59 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (3 responses) The bigger problem, IME, is not time, but actually recognising that you've changed something relevant. In that regard, documentation and comments (including Rust safety comments) are the worst possible way to deal with an API invariant, since there's not even a guarantee that the wording of the comment will be consistent enough to be usefully greppable. The ideal is always type-level checking of your invariants, because that check stops you making mistakes. The next best thing is what this proposal does with unsafe impl , where users get the invariant type-checked (so I can remove the unsafe impl if it's wrong, and find all the places that need fixing), and where it's not hard to write a tool that will definitely find all the places that need manual checks. The worst case is a situation where code at the point of use says something like "relies on the fact that foo does not move its contents, ever" in a comment, so that when someone refactors FooPtr, they've got to review all users of FooPtr to discover that the "foo" in the comment refers to something of type FooPtr. Documented invariants versus invariants in code Posted Oct 7, 2024 23:51 UTC (Mon)
                               by NYKevin (subscriber, #129325)
                              [ Link ] (2 responses) The "good" news is that, in this particular case, it would be quite perverse to define an object that violates this invariant. The vast majority of reasonable smart pointers are fully compliant and can just opt into it without further thought. It is only a problem if you (in the smart pointer implementation) change what object you're pointing to, the objects have different fat pointer metadata (vtable or slice size), and you make this change without any &mut self method being called. Nobody does that combination of things, because it's obviously crazy to re-seat a pointer (smart or dumb) without counting that as a mutation of the pointer. Even misfortunate does not do it (at time of writing): Double does not specify T: ?Sized so you can't put a fat pointer into it (although I suspect that's merely an oversight rather than an intentional decision), and Double::swap() is a &mut self method, so even if T could be unsized, swap() counts as a modification of the pointer anyway. In order to violate this invariant, you'd need to be doing something really ridiculous, like modifying the raw pointer value through a Cell<T> (not a Mutex<T>, since the whole smart pointer has to have the same layout as a raw pointer for this unsizing coercion to be possible in the first place), or using mem::transmute() to replace the raw pointer's vtable with a different one (not UB if the concrete types are layout-compatible). Documented invariants versus invariants in code Posted Oct 9, 2024 20:22 UTC (Wed)
                               by NYKevin (subscriber, #129325)
                              [ Link ] > and you make this change without any &mut self method being called. Self-nitpicking: This should read "any &mut self method other than DerefMut::deref_mut()." But that's still a ridiculous thing to do, because deref_mut() is the method that overrides the * (dereference) operator for write operations, so if you have it re-seat the pointer, you'd be causing code like *foo = bar to change what address ("place" in Rust's formal terminology) foo points at. Nobody who writes *foo = bar expects something like that to happen. At this point, I imagine that C++ programmers will object that, even though this operation should not change the address of the pointee, it could change the object's dynamic type. This is sort of true in C++ (objects may change dynamic type during construction, so when the object is copy-constructed, it could temporarily have a different dynamic type from bar, and you can interact with it through foo during this time), but not true at all in Rust (vtables are not even part of the object representation in the first place, they are statically allocated and tracked by fat pointers, so there is no plausible mechanism for modifying an object's dynamic type, nor is it possible to interact with partially-constructed objects through safe Rust). Documented invariants versus invariants in code Posted Oct 13, 2024 20:06 UTC (Sun)
                               by tialaramex (subscriber, #21167)
                              [ Link ] In the days since reading this comment I've been trying to figure out a suitably nefarious new misfortunate type which meets this requirement but I didn't succeed so far, maybe I'm missing a trick (although I did make one new "smart pointer" type that is fairly silly but it's still Sized because it requires Default) In the event you find yourself writing such a type please let me know, I'm named tialaramex most places - including Google's "Gmail" of course. Rust vs. C Posted Oct 8, 2024 18:04 UTC (Tue)
                               by jezuch (subscriber, #52988)
                              [ Link ] (4 responses) That's true even in teams that take things seriously and require at least two approvals before things get merged so that issues like this are exposed to more eyeballs  - and with no time pressure to boot. In my experience, a programmer's brain gets trained to "unsee" comments as irrelevant filler. They are just invisible. How can you keep them in sync if your brain thinks they don't exist? That said, the claim was that comments documenting invariants have value. They do, but only because you cannot do it in the programming language itself. How can you put faith in proofs in an unspecified language that the compiler doesn't even look at? Use a language in which you don't have to do it most of the time. Rust vs. C Posted Oct 9, 2024 8:37 UTC (Wed)
                               by taladar (subscriber, #68407)
                              [ Link ] > In my experience, a programmer's brain gets trained to "unsee" comments as irrelevant filler. They are just invisible. How can you keep them in sync if your brain thinks they don't exist? That is a good point, essentially you can never rely on them when trying to understand the code because they might be outdated but you would always have to read them when modifying the code to see if they need to be updated. That is a very unlikely combination of behaviors to develop naturally. Rust vs. C Posted Oct 13, 2024 13:41 UTC (Sun)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (2 responses) > In my experience, a programmer's brain gets trained to "unsee" comments as irrelevant filler. Sounds like a bad habit to me. I find comments to be very helpful in both development and review. A recent example: https://gitlab.kitware.com/utils/ghostflow-director/-/mer... Rust vs. C Posted Oct 13, 2024 13:57 UTC (Sun)
                               by pizza (subscriber, #46)
                              [ Link ] (1 responses) > Sounds like a bad habit to me. I find comments to be very helpful in both development and review. So I take it you've never had to review (or otherwise work with/consume) external/third party code? In my experience, comments are rarely helpful, and more often than not, actively harmful to your understanding of what the code _actually_ does.  (As opposed to what was intended at the time the comments were written). Rust vs. C Posted Oct 13, 2024 20:08 UTC (Sun)
                               by mathstuf (subscriber, #69389)
                              [ Link ] Sure, I've dug through third party code. The comments may be *bad*, but I still have to read them to know that. *Ignoring* them means also being unaware of the (or at least "an") *intent* of the code (regardless of its implementation quality). Rust vs. C Posted Oct 6, 2024 21:17 UTC (Sun)
                               by iabervon (subscriber, #722)
                              [ Link ] (3 responses) There are a lot of invarants that are almost always true and that people don't realize they're assuming, especially ones which require that two variables of the same type don't have the same value. For example, git's strbuf_addstr() assumes that the string you're adding isn't the buf of the strbuf you're adding it to. Any other string works, and there's a function that adds a strbuf to another strbuf that works for this case (and is more efficient in all cases), but there's nothing pointing out that this one operation that practically nobody would want to do is actually unsafe, and it's probably not better to consume programmers' attention making sure they know about it, since they're much less likely to make this mistake than all sorts of other mistakes, but it would be good if the compiler could point out the issue if they actually violate it. For that matter, the author of the strbuf_addstr() function probably didn't consider the possibility that the string to add might be the same memory as the one being reallocated to make space. To be certain to avoid unlikely issues, the compiler would have to tell the library author to include an annotation of an obscure invariant that the compiler should tell users of, if they ever violate it. C assumes that nobody will ever make that mistake (and it's probably right), whereas Rust wants to be sure that they can't make it. Rust vs. C Posted Oct 18, 2024 6:00 UTC (Fri)
                               by blackfire (guest, #92738)
                              [ Link ] (2 responses) > C assumes that nobody will ever make that mistake (and it's probably right), whereas Rust wants to be sure that they can't make it. And it's actually trivial enough in this case. See a sample StrBuf in the Rust playground: https://play.rust-lang.org/?version=stable&mode=debug... Uncommenting the one commented line (which would lead to the bug you mention) will cause a compile error. The `append` method takes a mutable reference to `self` (the object it's modifying) and an immutable reference to the bytes it's about to append. The catch is you cannot have both a mutable and immutable reference to the same object, so there's no way trying to append a buf to itself would work (except `unsafe`, but if you use it to violate the "aliased xor mutable" rule, it's insta-UB anyway). Rust vs. C Posted Oct 18, 2024 6:13 UTC (Fri)
                               by mb (subscriber, #50428)
                              [ Link ] (1 responses) >except `unsafe`, but if you use it to violate the "aliased xor mutable" rule, it's insta-UB anyway) Hm, are you sure that just having two aliasing nonconst raw pointers is insta-UB? That doesn't look right to me. Rust vs. C Posted Oct 18, 2024 7:56 UTC (Fri)
                               by blackfire (guest, #92738)
                              [ Link ] > Hm, are you sure that just having two aliasing nonconst raw pointers is insta-UB? > That doesn't look right to me. Having multiple raw pointers (const or not) is fine, having multiple mut references (or one mut and 1+ const) is UB, even without dereferencing any of them. (this is made mildly more complex by stacked borrows but that's the gist and honestly I don't claim to fully understand the intricacies of the model, but you do have to watch out for UB any time you make a reference from a raw pointer) Rust vs. C Posted Oct 8, 2024 0:26 UTC (Tue)
                               by gerdesj (subscriber, #5446)
                              [ Link ] "… and that's the difference between Rust and C, right here." My takeaway from this article is that both camps are (begrudgingly) listening to each other. As a civilian, I don't really give a shit about the rights and wrongs of a programming paradigm or whatever wankery is in the ascendant today.  I am persuaded that C and Rust are both serious ways of generating machine code to run on my CPUs 'n that. What I do like to see is gangs of serious engineers getting to grips with novel ideas and gradually thrashing out the best (for a given value of best) way forwards. In the end its all about the engineering.  Unless you are writing raw machine code, you need Assembly, C or Rust or whatevs to get stuff done ... err make stuff happen.  Do remember that in the end you are generating machine code that does something - that's the goal.  How you get there is a "journey" and that is up to you. My laptop does not "run" C or Rust.  It runs machine code and that's all.  Please ensure it runs the best machine code available, however you get it there!  I will be forever grateful for that. Smart pointers and memory models Posted Oct 11, 2024 18:41 UTC (Fri)
                               by geofft (subscriber, #59789)
                              [ Link ] (4 responses) I'm trying to understand the reference to the Linux kernel memory model here. Here's my very limited understanding so far and I'd appreciate corrections: The Rust standard library type Arc<T> (atomically reference-counted pointer to T uses the atomic support in the Rust standard library for the refcount, and there's some particular implementation of those. The Linux kernel might want to use a different implementation of atomics (concretely, maybe different assembly instructions, maybe adding barriers) than the Rust standard library uses. In particular, if Rust code and C code are modifying the same atomic, they need to use compatible instructions. Arc (and Rc and a few other standard library pointer-wrapping types) have the ability that, if they hold a two-word "fat pointer", you can use them in contexts where you need a normal pointer and you know you don't need the metadata (e.g., because you're calling a method from the vtable, and that method was compiled knowing what type it's being called on) by just casting (transmuting) the type of the smart pointer and ignoring the second word. This functionality is implemented by these types implementing some unstable marker traits, which they can do because they're in the standard library. Apart from the correctness things described in the article, the other important thing these traits do is promise that this cast is valid and that e.g. Rc<[u8; 10]> is the first few bytes of Rc<[u8]> . The kernel wants a refcounted type with this same ability. So the Rust team is adding a derive annotation that implements these traits (instead of exposing the traits themselves, whose API they're not ready to commit to) that requires that anything you derive the trait on is a #[repr(transparent)] wrapper around something in the standard library that is marked as implementing those traits (a raw pointer is one of these), which guarantees the layout trick. I think the part I don't understand is the relevance of interop between Rust and C. Is there any case where Rust and C are modifying the same atomic variable? Is the kernel's implementation of Arc (or ListArc ) intended to be layout-compatible with any existing C code in the kernel? It seems like "We want to leak on overflow instead of panicking" and "We need the intrusive linked list support because that is a useful design in kernelspace" by itself justify this, and the different memory model doesn't come into play. What goes wrong if Rust types use Rust atomics and C types use kernel atomics? Maybe put another way, if the standard library gained SaturatingArc<T> and an appropriate intrusive linked list type, which seem like not unreasonable things for everyone to have, would those suffice? (To be clear, I'm not actually suggesting this instead of the current approach, just asking it as a hypothetical for my understanding.) For that matter, why are Rust atomics and LKMM atomics different? I'm vaguely familiar with the fact that (e.g.) Alpha does things with memory ordering that people don't expect and so you need more barriers than your same code would need on other architectures, and so the kernel does emit those barriers on Alpha. But wouldn't this apply to all code on Alpha, and thus wouldn't Rust want to handle atomics in the same way? Why is there not one obvious correct way for anyone to implement an atomic reference count on a given architecture? Smart pointers and memory models Posted Oct 11, 2024 19:01 UTC (Fri)
                               by daroc (editor, #160859)
                              [ Link ] (1 responses) That is an excellent summary of the talk. As to why the LKMM is relevant — the LKMM doesn't just say things about how to write to atomic variables, it also has guarantees about how those writes interact with other constructs like threads. Boqun Feng actually had a neat talk that gave more detail about this later in the day that I'm still in the process of writing up. But one example is that if one thread writes to an atomic variable and then wakes another thread, the LKMM says that second thread is guaranteed to see the write. Rust atomics don't make that guarantee. Does that discrepancy cause problems? Maybe. At the very least, it's an extra complication to think about. Writing correct multithreaded code is hard enough; there's no reason to make it harder by having two conflicting models. Smart pointers and memory models Posted Oct 11, 2024 20:44 UTC (Fri)
                               by geofft (subscriber, #59789)
                              [ Link ] Ah, thanks, that specific discrepancy is the sort of thing I was curious about, thanks! I will look forward to the writeup. Smart pointers and memory models Posted Oct 11, 2024 19:12 UTC (Fri)
                               by farnz (subscriber, #17727)
                              [ Link ] (1 responses) Rust atomics exactly match the memory model defined for C11, but with memory_order_consume (which no compiler benefited from, last I checked - all implementations I've seen treat it as a weird spelling of memory_order_acquire ) removed completely. The LKMM predates C11 by quite some time, and has different rules about what happens-before and synchronizes-with relationships are established by the various atomic operations you can do. As a result, something needs to make sure that when Rust code accesses atomics that are shared with C, the LKMM rules are followed, and not the C11 rules; if everything just followed the C11 rules, then the kernel code that assumes LKMM would be broken (although I believe that if everything follows the LKMM rules, code that assumes C11 rules will still work). And Rust code will eventually need to modify atomics shared with C, because some of the existing kernel data structures depend on atomic modifications. Smart pointers and memory models Posted Oct 18, 2024 6:28 UTC (Fri)
                               by westurner (guest, #145208)
                              [ Link ] FWIW e.g. Apache Arrow is zero copy with C, CFFI, C++, Java, Python, and Rust APIs: [Memory and IO Interfaces — Apache Arrow v17.0.0]( https://arrow.apache.org/docs/python/memory.html#memory-p... ) > External memory, under the form of a raw pointer and size, can also be referenced using the foreign_buffer() function. pyarrow.foreign_buffer(address, size, base=None) https://arrow.apache.org/docs/python/generated/pyarrow.fo... : > base: Object that owns the referenced memory. > The buffer will be optionally backed by the Python base object, if given. The base object will be kept alive as long as this buffer is alive, including across language boundaries (for example if the buffer is referenced by C++ code) The Arrow Parquet docs mention that parquet must be reshaped when reading and writing from disk. Feather (Arrow IPC) format is the same shape on disk as in RAM, with ZSTD or LZ4 compression by default. serde.rs supports very many serialization formats for rust, including Python pickles and CSV and JSON and so on. lancedb also does zero-copy with Rust and Arrow: https://lancedb.github.io/lancedb/#why-use-lancedb : > Tight integration with the Arrow ecosystem, allowing true zero-copy access in shared memory with SIMD and GPU acceleration arrow-ipc-bench compares Flight, Plasma (*), sharedmemory with MacOS, IIRC: https://github.com/wjones127/arrow-ipc-bench/tree/main Rust arrow_ipc::reader > Struct StreamReader: https://docs.rs/arrow-ipc/53.1.0/arrow_ipc/reader/struct.... Apache Arrow > Serde.rs compatibility: https://docs.rs/arrow/latest/arrow/#serde-compatibility
======>
https://walnut356.github.io/posts/twovec-a-very-silly-container/
-->>-->>
Home » Posts » TwoVec: A Very Silly Container TwoVec: A Very Silly Container 2024-10-18 · 21 min · 4077 words · Walnut356 Table of Contents How it works Crimes against debugability Crimes against the type system Bit Fiddling Limitations Speed Traits Space But why? Lets say you want to store two different types of objects in 1 container. Simple right? Just slap those puppies in a tuple and you're good to go: let list : Vec < ( u8 , f32 ) > = vec! [ ( 255 , 20.0 ) , ( 10 , 37.0 ) ] ; copy But what if you wanted to include elements of two different types in arbitrary orders ? Thankfully, there's sum types for that: pub enum Val { A ( u8 ) , B ( f32 ) , } ... { let list = vec! [ Val :: A ( 255 ) , Val :: B ( 20.0 ) , Val :: B ( 37.0 ) , Val :: A ( 10 ) ] ; } copy One small problem. That's wasteful . We have 2 variants that cannot be niche optimized, and the variants are different sizes. Since Val itself must have 1 known size, smaller variants are required to add pad bytes to match the size of the largest variant. Additionally, since default Rust struct layout guarantees that the alignment of a struct is at least the maximum of all of its fields, the f32 contained in Val::B forces all Val s to be 4-byte aligned. That means even with #[repr(u8)] , a Val::A(u8) is 64 bits in size , despite it being possible to represent all Val s with only 5 bytes, and all Val::A(u8) 's with only 1 byte + 1 bit. Val::B(f32) is less wasteful, as only ~48% of its bits are complete dead weight, but it's still a sad number. Not to worry, I know RAM is the most precious of commodities in 2024 so I've come up with the answer to your woes. The TwoVec : let x = 8 u8 ; let y = 20.0 f32 ; // type inference from .push_a and .push_b let mut list = TwoVec :: new ( ) ; // dedicated functions list . push_a ( x ) ; list . push_b ( y ) ; // or through type inference list . push ( 18.0 ) ; list . push ( 17 ) ; list . push ( 12 ) ; list . push ( 35.7 ) ; list . push ( 1 ) ; dbg! ( & list ) ; // in bytes dbg! ( list . capacity ( ) ) ; // returns Some(val) if the value at that index is the correct type let mut v : Option < u8 > = list . get ( 0 ) ; dbg! ( v ) ; // returns None when the value is not the correct type let mut w : Option < f32 > = list . get ( 0 ) ; dbg! ( w ) ; w = list . get ( 1 ) ; dbg! ( w ) ; copy [Output] &list = TwoVec[8, 20.0, 18.0, 17, 12, 35.7, 1] list.capacity() = 18 v = Some(8,) w = None w = Some(20.0,) copy In the above example, I made sure to exactly fill the current allocation size of list . That means the listed capacity (18 bytes) minus the size of the data is the amount of overhead for bookkeeping. 4 u8 s and 3 f32 s takes up 16 bytes total, so we only have a storage overhead of 2 bytes! Compared to the Val enum above, that's a savings of 30 bytes. The source code is available here , and the crate is available on crates.io How it works # The basics are pretty simple, but getting it to work in Rust's type system was a bit of an adventure. To get the basis of the container out of the way as quickly as possible: array indexing is nothing but a base pointer + an offset. Arrays themselves are nothing more than an arbitrary region of bytes, regardless of what type they're "assigned" (see: C's void* ). Types are only "real" at compile time. Having arrays that contain only 1 type - or ensuring that all types contained in the array are the same size like enums do - allows for consistent and universal indexing math. That being said, if you can figure out the offsets on your own, there's nothing stopping you from implementing the indexing logic yourself. In a sense, hashmaps already work based on this principle. We only have two states, easily represented by a single bit. We know the size of both of the types, so we should be able to calculate the byte-offset ourselves by counting the number of each type and multiplying it by its respective size. We can start with a struct very similar to a standard Vec : pub struct TwoVec < A , B > { len : usize , capacity : usize , bitfield : NonNull < u8 > , data : NonNull < u8 > , a : PhantomData < A > , b : PhantomData < B > , } copy Crimes against debugability # You might notice that there's only 1 capacity and 1 len , despite there being 2 pointers do data. Since RAM is at a premium, we clearly can't store our bookkeeping bits in a different region of memory. Just think of the wasted allocator metadata bytes. As the old saying goes: "Only 1 allocation for this rustacean". This increases the bookkeeping complexity by a bit, but at least we can now say we're storing three different types in a single allocation instead of just 2. This has a substantially negative impact on the debugging experience, since now it's possible to overwrite bookkeeping data with element data and vice versa if we fuck up our indexing math. Which I definitely didn't do like 5 separate times. And when yo- ahem if you fuck up, have fun debugging bitfields and packed, probably unaligned values. Just awful. I definitely do not speak from experience though. Just for sake of example, imagine a few of the bits being set incorrectly due to an off-by-one error. Suddenly, your pretty-printer for the data block isn't just interpreting the bytes as the wrong value. The offset of the next value is partially based on the size of the one we have. If we interpret that size wrong, we're now jumping to the wrong location and even if we weren't, there's a good chance we're interpreting the bytes there as the wrong type too. The debugger display (at least, the one I was using) is largely unhelpful as well since an unsigned char* is interpreted as a C string. Hope you didn't have any \0 bytes in your data anywhere or you only get to see fraction of the total block =) Reading listings that looks like this {pointer:"\xba]B\xe3;?e7&z\\\U00000011\xbe\xcb\xd2>\xf9;5?3T"} copy and trying to extract useful information about why things went wrong is a special kind of pain. Again, not that I speak from experience or anything. Anyway, upon the first allocation of a data block, typical Vec implementations will reserve space for a handful of elements to reduce the number of times the Vec needs to be realloc 'd. In our case, it's not so simple. If we reserve 8 spots for f32 s + 1 additional byte for the bitfield, we run into a problem. If we filled the TwoVec with u8 s, we wouldn't have enough bits to tag them all - 8 f32 s takes the same space as 32 u8 s. We always need to make sure we have enough bits to cover the worst-possible case: // "base" size of the allocation, taken from rust's `RawVec` implementation let base_size = Self :: MAX_A_B * Self :: MIN_NON_ZER0_CAPACITY ; // number of bits needed to represent the "worst case" (i.e. 100% of capacity filled with the smaller element) let max_elements = base_size / Self :: MIN_A_B ; let bitfield_size = max_elements / 8 ; let new_capacity = base_size + bitfield_size ; let new_layout = Layout :: array :: < u8 > ( new_capacity ) . unwrap ( ) ; copy On each reallocation, we double the number of elements that can be stored, which exactly doubles the number of degenerate-case slots. Thus we don't have to worry about further fiddling with the bitfield size after the initial allocation, doubling the whole capacity suffices. Unfortunately, that brings up another issue. We cannot simply call realloc . realloc automatically copies all of the old bytes into the new allocation. In TwoVec , the distance between the start of the bitfield and the start of the data block is not static, it grows when the allocation grows. That means we need to manually copy the data over in two steps, leaving an extra gap for the new bitfield bytes unsafe { let offset = self . data . offset_from ( self . bitfield ) as usize ; self . bitfield . copy_to_nonoverlapping ( new_ptr , offset ) ; let new_data = new_ptr . add ( bitfield_size ) ; self . data . copy_to_nonoverlapping ( new_data , self . capacity - offset ) ; dealloc ( self . bitfield . as_ptr ( ) , Layout :: array :: < u8 > ( self . capacity ) . unwrap ( ) , ) ; } copy Crimes against the type system # The get and push functions require 2 different implementations because we need to be able to detect if the type we're asking for matches the value of the type-bit. i.e. if we want a B , we have to check if the bit is 1 , but if we want an A , we have to check if the bit is a 0 . It might be possible to pull this off dynamically in 1 function, but we still have another issue. Those with a keen eye may have heard some alarm bells when I showed off the type inference of the get and push methods. After all, if they were defined as impl < A , B > TwoVec < A , B > { pub fn get < T > ( & self , idx : usize ) -> Option < T > { ... } } copy We suddenly have a very big safety hole. Rust's type system can express exactly 2 relationships between types . TwoVec<A, A> is a TwoVec whose elements are always of type A . Conversely, TwoVec<A, B> means that elements may be of type A or type B , and that A might be the same as type B , but also might not. There is no way to indicate that two types must not be the same. There is also no way to guarantee that a third type ( T in this case) is A or B . Imagine we have a completely full TwoVec<A, B> and we access the last element with our get function. Except we forget what type A is, and call get with a type where size_of::<T>() > size_of::<A>() . Since this whole house of cards functions via pointer casting, there is nothing stopping this from reading out-of-bounds memory. Even if we added a check (e.g. if ptr_to_elmt + size_of::<T>() > ptr_to_end_of_alloc ) we're still giving people an "unsafe-free" mem::transmute() . We could try declaring the function get<A> or get<B> , but now it only works for one of our two types which isn't ideal. Instead, we can try using a Get<T> trait to accomplish this. There's a problem when we add the impl s though: impl < A , B > Get < A > for TwoVec < A , B > { fn get ( & self ) -> Option < A > { ... } } impl < A , B > Get < B > for TwoVec < A , B > { // Error, overlapping impls fn get ( & self ) -> Option < B > { ... } } copy Because A and B could be the same type, the compiler wouldn't know which impl to choose. We, as the programmer, know that it doesn't even make sense for a TwoVec to be used when A and B are the same type - TwoVec<A, A> is strictly worse than just using Vec<A> - but there's no way to express this via a trait on TwoVec . A bare marker trait on the types themselves also doesn't quite work. pub trait MarkerA { fn get ( tv : , idx : usize ) -> Option < Self > ; } pub trait MarkerB { fn get ( & self , idx : usize ) -> Option < Self > ; } impl MarkerA for u8 { ... } impl MarkerB for f32 { ... } copy Aside from the fact that the user must now define these marker traits, we also have the issue that there's no negative trait bounds (outside of the compiler) either! A type could easily implement MarkerA and MarkerB and it still wouldn't know which version to call. Even if we somehow bypassed that restriction and make the traits mutually exclusive, we cannot have TwoVec<u8, f32> and TwoVec<f32, u8> in the same program because u8 would be required to impl MarkerA for the former and MarkerB for the latter. It took me quite a bit of fiddling to work around this issue. I tried lots of AsRef and deref-abuse shenanigans, some unstable features, lots of different things on both the TwoVec itself and on its constituent types. Many things kindof work, but require the fully qualified syntax which is... not preferable for obvious reasons. I have half an RFC written out of pure frustration titled negative_type_bounds . Being able to specify pub struct TwoVec<A, B> where A: !B (or type subsets like fn get<T: A | B> ) would make this significantly easier. As far as I know, that sort of thing is trivial with C++'s templates. I found one way to make things work, and it's const generics (though there are lots of ways that don't work with const generics and unstable generic_const_expr , maybe a story for another day). By including a const bool type parameter on a trait implemented on each type A and B , we can differentiate between <A, B> where A is important and <A, B> where B is important. We can blanket-impl this trait to every Sized type: pub trait Gettable < A , B , const Z : bool > where Self : Sized , { fn get ( tv : & TwoVec < A , B > , idx : usize ) -> Option < Self > ; } impl < A , B > Gettable < A , B , false > for A { fn get ( tv : & TwoVec < A , B > , idx : usize ) -> Option < Self > { tv . get_a ( idx ) } } impl < A , B > Gettable < A , B , true > for B { fn get ( tv : & TwoVec < A , B > , idx : usize ) -> Option < Self > { tv . get_b ( idx ) } } copy From there, it's possible to add a function get<T, const Z: bool> in TwoVec with the bound where T: Gettable<A, B, Z> . Not only does this restrict to only the exact types A and B of TwoVec<A, B> , it also does not conflict with TwoVec<B, A> , since that is an entirely different type from TwoVec<A, B> . Technically, when turbofishing get you're now required to also include the const bool parameter, but in most cases this is unnecessary because type inference removes the need for a turbofish at all. To clarify further, since this is sorta spaghetti, here's an example. Say you have TwoVec<u8, f32> When asking for an Option<u8> , the compiler sees that u8 is Gettable for u8, f32, true , and the function signature requires a TwoVec<u8, f32> . There is no Gettable with u8, f32, false , as false is only cares about the B parameter, and u8 != f32 . There is no other Gettable implementation for u8 with valid criteria, thus the const parameter true can be assumed TwoVec<u8, f32> impls cannot overlap with, say, TwoVec<u8, i64> because the B parameter of i64 (and the TwoVec<u8, f32> in the function signature) would place it in an entirely different monomorphization of Gettable There is no self-overlap with TwoVec<u8, u8> because Gettable<A, B, false> and Gettable<A, B, true> are mutually exclusive (even though in this case they'd be completely interchangeable). Interestingly, type inference doesn't require us to turbofish on a TwoVec<A, A> even though both implementations would be valid. I'm not sure whether the compiler is choosing get::<u8, true>() or get::<u8, false>() or how it even makes the decision. Weird. Grimy? Yes. Cool? Arguably. We can even add a third impl that returns an either::Either , though this one feels even more gross since now the const parameter is truly meaningless (though it still must be a definite value for type inference to work) impl < A : Copy , B : Copy > Gettable < A , B , true > for Either < A , B > { fn get ( tv : & TwoVec < A , B > , idx : usize ) -> Option < Self > { if idx > tv . len ( ) { return None ; } if tv . is_a ( idx ) { tv . get_a ( idx ) . map ( | x | Either :: Left ( x ) ) } else { tv . get_b ( idx ) . map ( | x | Either :: Right ( x ) ) } } } copy Bit Fiddling # As I mentioned above, debugging this nonsense is the worst . Typical debugger utilities are borderline worthless, so I have to check a lot of things by hand. As in notebook, pen, ascii table, and programming calculator, hand-translating whatever the debugger gave me into a slightly more useable form data at each step in the algorithm, drawing diagrams and painstakingly proving to myself each individual piece makes sense. I want to walk through the implementation of the remove function to give you guys an idea how much more fiddly this is compared to Vec functions. remove has to do two distinct tasks to work properly: copy the data from the region offset_of_next_element..end_offset to the region offset_of_element..end_offset_minus_size_of_removed_element take every bitfield-bit after the index and bitshift it left once The first part is easy - we can copy the std::Vec implementation almost exactly. The only difference is that we can't just subtract "1 element" from the end offset, we have 2 different kinds of elements. pub fn remove_a ( & mut self , idx : usize ) -> Option < A > { if idx > self . len { panic! ( " Cannot remove value at index {idx} , length is {} " , self . len ) ; } if ! self . is_a ( idx ) { return None ; } let val_offset = self . idx_to_offset ( idx ) ; let end_offset = self . idx_to_offset ( self . len ) ; let out : Option < A > ; unsafe { let ptr = self . data . add ( val_offset ) ; out = Some ( ptr . cast :: < A > ( ) . read_unaligned ( ) ) ; ptr . copy_from ( ptr . add ( Self :: A_SIZE ) , end_offset - ( val_offset + Self :: A_SIZE ) , ) ; } self . shift_bits_down_after ( idx ) ; out } copy That shift_bits_down_after function call is carrying a lot of weight though. I always jump right to the bit fiddling solution, but I ended up messing it up on my first go-round. I took the time to make this slower, simpler function first to test against: for i in idx .. self . len - 1 { let b = self . get_bit ( i + 1 ) ; if b == 0 { self . clear_bit ( i ) ; } else { self . set_bit ( i ) ; } } copy Before we get into the indexing math, I want to point out something small. In an effort to make debugging a little bit less painful the bit indexes are "reversed" so that the whole bitfield slice pretty-prints in order. That means the bit set to 1 in the expressions 1 << 7 or 0b1000_0000 is at index 0 . The clear_ , set_ and get_ functions all follow this same pattern: fn set_bit ( & mut self , idx : usize ) { let byte_idx = idx / 8 ; let bit_idx = idx % 8 ; let bf = self . bitfield_mut ( ) ; bf [ byte_idx ] |= 1 << ( 7 - bit_idx ) ; } copy To lump multiple of these together into bit-shift operations, we need to handle 2 separate steps for the byte that idx falls in, we need to left-shift only the bits after idx for that byte and all (potential) following bytes, we need to fill the new "blank" right-most bit. The value that goes here is the left-most value of the next byte in the bitfield As an aside, since the carry bit comes from the previous shift instruction, the most efficient way to handle this would probably be to iterate through the bitfield backwards . This is because the shift instructions automatically store the last-shifted-out-bit in a special, non-general-purpose register. Once there, it can be passed to the next operation for free - no register or stack usage required. On the other hand, computers aren't optimized for reverse-order array access, which typically wrecks the cache prefetch predictions. On the other other hand, that probably only matters if the bitfield is larger than a cache line (typically 64 bytes, which is 512 elements). On the other other other hand, it might be that the compiler can "see through" what I'm doing and rearrange the operations such that it uses the carry flag regardless of the order I read the array. For the sake of my sanity, I'm just going to iterate through it front to back. let bf = self . bitfield_mut ( ) ; let start_byte_idx = idx / 8 ; let start_bit_idx = idx % 8 ; let mut idx_byte = bf [ start_byte_idx ] ; copy Next we need to isolate the 2 sets of bits we want - the bits before idx and the bits after idx . This is a bit trickier than it sounds though, since we also need to shift off idx itself. When shifting off the bottom bits, the naive shift amount might be 7 - (idx + 1) , but this will panic if idx == 7 due to subtraction overflow. While we could do something like 7.saturating_sub(start_bit_idx + 1) , this doesn't quite have the behavior we want. If idx == 7 and our byte is 0b0000_0001 , the prior formula will give us a bitshift of 0 and our value won't change. Workarounds for that like 7.checked_sub(start_bit_idx + 1).unwrap_or(1) or 7.saturating_sub(start_bit_idx + 1).max(1) only work when idx > 0 . When idx == 0 , the only right-shift that would clear the top bit is 8. Unfortunately, shifting with a value that is >= the bitwidth of your integer is UB. While we could make that a special case ( if start_bit_idx != 0 {...} or checked_shr ), it's easier to just shift it by the base amount, shift that value by 1, and then do the same in reverse. Alternatively, since we're dealing in non- usize values, we could cast the value to a larger size during the bit fiddling and then as cast it back to a u8 (which truncates it in rust) when it comes time to store the value. Casting more or less compiles to a noop so it's probably a smidge faster than 2 bitshifts. // clears all the bits including and after idx let shift_amt = 8 - start_bit_idx ; let pre_idx_bits = ( ( ( ( idx_byte as u32 >> shift_amt ) >> 1 ) << shift_amt ) << 1 ) as u8 ; // clears all the bits up to and including idx. Requires a +1 because bits are // "0-indexed" when shifting. When shifting back, we drop the +1 so that we can // overwrite the bit that's being removed let post_idx_bits = ( ( ( idx_byte as u32 ) << ( start_bit_idx + 1 ) ) as u8 ) >> start_bit_idx ; copy One very important note: the cast to u8 for the pre index bits must be done after both shifts, since both shifts could overflow. The cast to u8 for the post index bits must be done before the second shift, because we need to truncate off the top bits before sliding everything back into place. I mistakenly did the truncation after both shifts and it took me an embarrassingly long time to spot. With this done, we've effectively "closed the gap" by removing a bit, and the right-most bit in the pre- and post-idx values should be 0. Next we just need the carry bit, which can default to 0 if there is no next byte. let mut carry = bf . get ( start_byte_idx + 1 ) . cloned ( ) . unwrap_or_default ( ) >> 7 ; copy and the resultant value is all 3 of those values masked together bf [ start_byte_idx ] = pre_idx_bits | post_idx_bits | carry ; copy And finally, we can handle the rest of the bytes (if there are any) with a straightforward loop: for i in 1 .. bf . len ( ) { let mut x = bf [ i ] ; carry = bf . get ( i + 1 ) . cloned ( ) . unwrap_or_default ( ) >> 7 ; bf [ i ] = ( x << 1 ) | carry ; } copy All of that just to accomplish the same feat as a single line in the Vec::remove implementation. Limitations # Speed # Let's get this out of the way right away, on benchmarks pushing/reading/removing 10,000 random elements with random types, TwoVec is ~100x slower than Vec<Enum> . .idx_to_offset() - the primary driver for accessing the data - is technically O(N) , but is really O(N/64) (yes I know how Big O works, but as far as I'm aware most people aren't adding an infinite number of elements to their vector so the coefficient still matters). This is after some optimizations. The original implementation looked something like this: pub fn idx_to_offset ( & self , idx : usize ) -> usize { let byte_count = idx / 8 ; let bit_count = idx % 8 ; let bitfield = unsafe { slice_from_raw_parts ( self . bitfield . as_ptr ( ) , byte_count ) . as_ref ( ) . unwrap ( ) } ; let mut slice = [ 0 u8 ; 8 ] ; for i in bitfield . chunks ( 8 ) { } let result : usize = bitfield . iter ( ) . map ( | x | { let a_count = x . count_zeros ( ) as usize ; ( a_count * Self :: A_SIZE ) + ( ( 8 - a_count ) * Self :: B_SIZE ) } ) . sum ( ) ; if bit_count == 0 { return result ; } // handling for the remaining partial u8 ... } copy Essentially, it takes the slice bitfield[..bitfield.len() - 1] and iterates over it, using .count_ones() / .count_zeroes() (which compile down to ~a single popcnt instruction) to determine how many of each type of element there is. A final special case has to be handled when idx doesn't fall on a byte boundary. This code gets vectorized by the compiler. As it turns out though, the vectorized code is slightly suboptimal. In my benchmark, it ran a fair bit faster with #[inline(never)] , but even that was slow. I'm not sure if it's the inherent performance characteristics of the vector instructions, the degenerate cases where the whole vector register is loaded up just to check 1 bit, the algorithm used to emulate popcnt behavior, or some other factor that I'm not considering. In any case, using "shitty simd", (i.e. treating a u64 as packed u8 s) was about 20% faster on push and get for reasonable numbers of elements. ... for ( i , chunk ) in bitfield . chunks ( 8 ) . enumerate ( ) { let mut val = match chunk . len ( ) { 8 => { u64 :: from_be_bytes ( chunk . try_into ( ) . unwrap ( ) ) } x => { let mut slice = [ 0 u8 ; 8 ] ; slice [ .. x ] . copy_from_slice ( & chunk [ .. x ] ) ; u64 :: from_be_bytes ( slice ) } } ; let important_bits = idx % 64 ; if ( i + 1 ) * 8 < len || important_bits == 0 { let a_count = val . count_zeros ( ) as usize ; result += ( a_count * Self :: A_SIZE ) + ( ( 64 - a_count ) * Self :: B_SIZE ) ; continue ; } let shift_bits = 64 - important_bits ; let b_count = ( ( val >> shift_bits ) << shift_bits ) . count_ones ( ) as usize ; let a_bytes = important_bits . saturating_sub ( b_count ) * Self :: A_SIZE ; let b_bytes = b_count * Self :: B_SIZE ; result += a_bytes + b_bytes ; } result } copy I also tried u32 -shitty-simd just in case, but that ended up being slower. This could be improved by using run-length encoding, but that introduces some degenerate-case performance characteristics. If we store a reasonably-sized integer for the length, it'd use a ton more space when the type changes every single element. The worst possible case isn't a huge issue though, since at that point the end user should probably reach for Vec<(A, B)> instead. There are potential bit-packing solutions that could offer a more reasonable tradeoff. For example, bit 1 indicates the type, the next 3 bits indicate the run length, and 2 of those are packed into each u8 . That's a potential project for another day though. Traits # Those of you who muck around with pointers probably saw this coming. Notice that the get functions return owned objects , not references. This is the single biggest limitation of the container in my opinion. Since the values are packed in memory, there's no guarantee that they are aligned properly. It is always UB to have a &T if T is unaligned. I'm hesitant to even have a function that hands out a raw pointers to T because it's so easy to do unaligned access wrong. I could use something like the unaligned crate and conditionally give out references when things happen to be aligned, but that comes with its own problems. Ideally I'd want the functions to be implemented on a trait, bounded by align_of::<A>() == align_of::<B>() , so the returned object is guaranteed to be aligned, but that requires const_generic_expr to be stabilized. As a result, A and B are both required to be Copy . I left that detail out of the post until now mostly to make the signatures easier to read. In the future, I may make an alternative "alignment-aware" version that ends up being a hybrid of TwoVec and Vec::Enum . It would only pad when necessary, so multiple small elements in a row could still be packed. Or, if ordering wasn't considered important, I could "backfill" the padding with new values. Additionally, unlike Vec we cannot implement Deref or AsRef to anything else in any kind of sensible way because of how delicate and specialized the internal bookkeeping is. That severely limits the number of helper functions we can get "for free". Space # While the storage overhead is typically much better than an enum, it's not guaranteed to be. The overhead varies based on how many elements are in the TwoVec and how large the size-difference is between types A and B . It has opposite storage characteristics to those of an enum - the overhead of an enum is worst when using mostly the small elements, whereas TwoVec stores the small variants much more efficiently, but can end up with a decent amount of wasted space when storing mostly large variants. But why? # To be clear, I don't think this container is very practical. If you find a usecase for it, please do not tell me. Whatever problem you're solving is unbelievably cursed and I don't want it anywhere near me. This is mostly inspired by some of the jank workarounds I used when completing Advent of Code in C since I didn't want to implement hashmaps from scratch, and I didn't feel super comfortable with unions. TwoVec is a living example of something that I learned from C: Standard library types are a generalist solution that must be correct and useful in almost all cases. "Almost all" is not "every". There exists alternative ways of expressing your intent to the computer. In the same way that performance tuning is about informing the compiler about as many invariants as possible, sometimes it's okay to have a slightly gross data structure if it means it reflects your intentions better. In some cases, this also reduces the complexity of the final algorithm. Rules like "one type per array" or even "all array elements must be the same size" are just the tip of the iceberg. Calling them "rules" is itself sortof wrong. It's easy to forget that when reading discourse and advice about programming. If I had to guess, I think that these sorts of "one-off, highly specific, slightly gross" data structures used to be more common back in the "wild west" days of programming. Back when hardware was much more limited, when languages and compilers were far less advanced, and when there was less "common knowledge" and "best practices" than we have today. There are good reasons that they've fallen out of fashion - upkeep, readability, different performance priorities, etc. - but they shouldn't be considered non-options compared to generalized "standard-library"-esque data structures. Even if they're not necessary or practical, it still requires some lateral thinking and a deeper understanding of the language and underlying hardware. Those are always valuable skills to work on. programming rust « Prev Why is language documentation still so terrible?
======>
https://corrode.dev/podcast/s03e01-zed/
-->>-->>
Home Services Podcast Blog About Podcast Zed - Conrad Irwin,
    Open Source Developer S03 E01 Published on 2024-10-17 Next to writing their own operating system , another dream shared by many developers is building their own text editor. Conrad Irwin, a software engineer at Zed, is doing just that. Zed is a fully extensible, open-source text editor written entirely in Rust. It's fast, lightweight, and comes with excellent language support out of the box. In the first episode of the third season, I sit down with Conrad to discuss Zed's mission to build a next-generation text editor and why it was necessary to rebuild the very foundation of text editing software from scratch to achieve their goals. Show Notes About Zed Industries Zed isn't afraid of daunting tasks. Not only have they built a text editor from scratch, but they've also developed their own GUI toolkit, implemented advanced parsing techniques like tree-sitter, and integrated multi-user collaboration features directly into the editor. Zed is a text editor built for the future, with meticulous attention to detail and a focus on exceptional performance. About Conrad Irwin Before joining Zed, Conrad worked on Superhuman, an email client renowned for its speed and efficiency. He is a seasoned developer with a deep understanding of performance optimization and building fast, reliable software. Conrad is passionate about open-source software and is a strong advocate for Rust. He's also an excellent pair-programming partner and invites people to join him while working on Zed. Links From The Show Superhuman - High-performance email client known for its speed and efficiency Visual Studio Code - Popular, extensible code editor Neovim - Vim-based text editor focused on extensibility and usability gpui crate - Zed's custom GUI toolkit for building fast, native user interfaces Leptos - Rust framework for building reactive web applications Dioxus - Rust library for building cross-platform user interfaces Tokio - Asynchronous runtime for Rust, powering many network applications async-std - Asynchronous version of the Rust standard library smol - Small and fast async runtime for Rust Glommio - Thread-per-core Rust async framework with a Linux-specific runtime isahc - HTTP client library that supports multiple async runtimes AsyncRead , AsyncWrite traits Zed Editor YouTube channel - Official channel for Zed editor tutorials and updates Tree-sitter - Parser generator tool and incremental parsing library Semgrep - Static analysis tool for finding and preventing bugs Zed release changelogs - Official changelog for Zed editor releases matklad's blog post: "Flat Is Better Than Nested" - Discusses organizing large Rust projects with a flat structure rust-analyzer - Advanced language server for Rust, providing IDE-like features Protobuf Rust crate - Protocol Buffers implementation for Rust Postcard - Compact serialization format for Rust, designed for resource-constrained systems CBOR - Concise Binary Object Representation, a data format similar to JSON but more compact MessagePack - Efficient binary serialization format RON (Rusty Object Notation) - Simple readable data serialization format similar to Rust syntax James Munns' blog - Embedded systems expert and Rust consultant's blog Delve - Debugger for the Go programming language LLDB - Next generation, high-performance debugger used with Rust and other LLVM languages Official Links Zed Homepage Zed on YouTube Conrad Irwin on GitHub Conrad Irwin on Twitter Conrad's Blog About corrode "Rust in Production" is a podcast by corrode, a company that helps teams adopt Rust. We offer training, consulting, and development services to help you succeed with Rust. If you want to learn more about how we can help you, please get in touch . Subscribe to Rust In Production Rust in Production is a bi-weekly podcast by corrode. New episodes are
    released every other Thursday at 4pm UTC. You can subscribe to new episodes
    on Apple , Spotify , YouTube , Amazon , or through our RSS feed . Apple Podcasts Spotify YouTube RSS ↑ Back to top © 2024 corrode — Düsseldorf, Germany · Legal Notice · Business Inquiries
======>
https://old.reddit.com/r/rust/comments/1g6mx4b/image_v0254_brings_faster_webp_decoding/
-->>-->>
A new version of    image    crate has just been released! The highlights of this release are:   

   
   Decoding lossless WebP images 2x to 2.5x faster, thanks to a variety of optimizations done by    fintelia   
   An    approximate but much faster    blur implementation was contributed by    torfmaster   
   Orientation metadata    is now supported, so you can display photos with the correct rotation (by    fintelia    and myself)   
   

   There are also some bug fixes to decoding animated APNG and WebP images, and other minor improvements.   

   Note that orientation from metadata isn't applied automatically when loading the image (yet) because that would be a breaking change. But the API makes correctly handling it    very easy   . I'm happy with how it came together, and how we managed to implement it without adding any complex dependencies!   
   

======>
https://old.reddit.com/r/rust/comments/1g6s4ml/psa_size_of_and_align_of_are_in_the_prelude_since/
-->>-->>
This wasn't mentioned in the    1.80 release notes    so I and likely many others missed it, but yes, you don't have to type    std::mem::size_of    anymore, just    size_of    works now.   

   I've never used them but    size_of_val    and    align_of_val    are also now in the prelude.   
   

======>
https://old.reddit.com/r/rust/comments/1g6kavw/learning_rust_was_the_best_thing_i_ever_did/
-->>-->>
And I don't even say this because I love the language (though I do).     

   For a long time, like a year, I always regarded rust as something that I would not be capable of learning. It was for people on a different level, people much smarter than me.     

   Rust was one of many things I never tried because I just thought I wasn't capable of it. Until one day, on a whim. I decided "why not" and tried reading the book.   

   It wasn't    easy    by any stretch of the imagination. I struggled a lot to learn functional programming, rusts type system, how to write code in a non OOP way.     

   But the most important thing I learned, was that I    was    good enough for rust. I had no expectations that I would bother doing anything more than the simplest of projects. And while I wouldn't say I've done anything particularly complicated yet, I've gone way way farther than I ever thought I'd go.     

   What it taught me was that nothing is too difficult.   
And after this I tried a lot of other things I thought I was incapable of learning. Touch typing. Neovim.   
I was always intimidated by the programmers I'd seen who'd use rust, in Neovim, typing on a split keyboard. And now I literally am one of them.   
I don't think this is something everyone needs to do or learn of course, but I am glad that I learned it.     

   I really do feel like I can learn literally anything. I always thought I'd be too dumb to understand any library source code, but every single time I've checked, even if it looks like magic at first, if I look and it for long enough, eventually I realize, it's just code.   
   

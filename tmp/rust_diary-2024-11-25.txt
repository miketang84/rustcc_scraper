https://github.com/cool-mist/joke-cli
-->>-->>
Repository files navigation README GPL-3.0 license CLI for the Official Joke API Installation Build from source cargo install --git https://github.com/cool-mist/joke-cli Usage Run joke to get a random joke. The first run downloads all the jokes from the Official Joke Api . Periodically run joke --update to update the list of jokes. Run joke --help for other options. Mix with cowsay and add to your shell profile for extra fun. joke -c programming | cowsay -f tux __________________________________
/ Knock-knock.                     \ | | \ A race condition. Who is there ? /
 ----------------------------------
   \
    \
        .--. | o_o | | :_/ | // \ \
     ( | | )
    / ' \_   _/`\ \___)=(___/
======>
https://github.com/Gleb-Zaslavsky/RustedSciThe
-->>-->>
Repository files navigation README [TOC] RustedSciThe is a Rust library for symbolic and numerical computing: parse string expressions in symbolic representation/symbolic function and compute symbolic derivatives or/and transform symbolic expressions into regular Rust functions, compute symbolic Jacobian and solve initial value problems for for stiff ODEs with BDF and Backward Euler methods, non-stiff ODEs and Boundary Value Problem (BVP) using Newton iterations Content Motivation Features Usage Testing Contributing To do Motivation At first, this code was part of the KiThe crate, where it was supposed to serve for constructing analytical Jacobians for solving systems of equations of combustion, chemical kinetics and heat and mass transfer, as well as for displaying analytical expressions, but it soon became clear that it could be useful for a broader circle of users Features parsing string expressions in symbolic to a symbolic expression/function symbolic/analytical differentiation of symbolic expressions/functions compare analytical derivative to a numerical one calculate _vector of partial derivatives transform symbolic expressions/functions (also derivatives) into regular Rust functions calculate symbolic/analytical Jacobian and transform it into functional form Newton-Raphson method with analytical Jacobian Backward Eeuler method with analytical Jacobian Backward Differetiation Formula method (BDF) with analytical Jacobian (direct rewrite of python BDF solver from SciPy library) classical methods for non-stiff equations RK45 and DP Boundary Value Problem for ODE with Newton-Raphson method (several versions available) Usage parse string expression of multiple arguments to a symbolic representation/function and then differentiate it and "lamdufy" it (transform it into a regular rust function). Compare analytical derivative to a numerical one. Calculate the vector of partials derivatives. Solve IVP and BVP problems. // FUNCTION OF MULTIPLE VARIABLES //parse expression from string to symbolic expression let input = "exp(_x)+log(_y)" ; // here you've got symbolic expression let parsed_expression = Expr :: parse_expression ( input ) ; println ! ( " parsed_expression {}" , parsed_expression ) ; // turn symbolic expression to a pretty human-readable string let parsed_function = parsed_expression . sym_to_str ( "_x" ) ; println ! ( "{}, sym to string: {} \n " ,input,  parsed_function ) ; // return _vec of all arguments let all = parsed_expression . all_arguments_are_variables ( ) ; println ! ( "all arguments are variables {:?}" ,all ) ; let variables = parsed_expression . extract_variables ( ) ; println ! ( "variables {:?}" ,variables ) ; // differentiate with respect to _x and _y let df_dx = parsed_expression . diff ( "_x" ) ; let df_dy = parsed_expression . diff ( "_y" ) ; println ! ( "df_dx = {}, df_dy = {}" , df_dx, df_dy ) ; //convert symbolic expression to a Rust function and evaluate the function let args = _vec ! [ "_x" , "_y" ] ; let function_of_x_and_y = parsed_expression . lambdify ( args ) ; let f_res = function_of_x_and_y ( & [ 1.0 , 2.0 ] ) ; println ! ( "f_res = {}" , f_res ) ; // or you dont want to pass arguments you can use lambdify_wrapped, arguments will be found inside function let function_of_x_and_y = parsed_expression . lambdify_wrapped ( ) ; let f_res = function_of_x_and_y ( & [ 1.0 , 2.0 ] ) ; println ! ( "f_res2 = {}" , f_res ) ; // evaluate function of 2 or more arguments using linspace for defining vectors of arguments let start = _vec ! [ 1.0 , 1.0 ] ; let end = _vec ! [ 2.0 , 2.0 ] ; let result = parsed_expression . lamdified_from_linspace ( start . clone ( ) , end . clone ( ) , 10 ) ; println ! ( "evaluated function of 2 arguments = {:?}" , result ) ; //  find _vector of derivatives with respect to all arguments let vector_of_derivatives = parsed_expression . diff_multi ( ) ; println ! ( "vector_of_derivatives = {:?}, {}" , vector_of_derivatives, vector_of_derivatives.len ( ) ) ; // compare numerical and analtical derivatives for a given linspace defined by start, end _values and number of _values. // max_norm - maximum norm of the difference between numerical and analtical derivatives let comparsion = parsed_expression . compare_num ( start , end , 100 , 1e-6 ) ; println ! ( " result_of compare = {:?}" , comparsion ) ; the same for a function of one variable //  FUNTION OF 1 VARIABLE (processing of them has a slightly easier syntax then for multiple variables) // function of 1 argument (1D examples) let input = "log(_x)" ; let f = Expr :: parse_expression ( input ) ; //convert symbolic expression to a Rust function and evaluate the function let f_res = f . lambdify1D ( ) ( 1.0 ) ; let df_dx = f . diff ( "_x" ) ; println ! ( "df_dx = {}, log(1) = {}" , df_dx, f_res ) ; let input = "_x+exp(_x)" ; let f = Expr :: parse_expression ( input ) ; let f_res = f . lambdify1D ( ) ( 1.0 ) ; println ! ( "f_res = {}" , f_res ) ; let start = 0.0 ; let end = 10 as f64 ; let num_values = 100 ; let max_norm = 1e-6 ; // compare numerical and analtical derivatives for a given linspace defined by start, end _values and number of _values. // a norm of the difference between the two of them is returned, and the answer is true if the norm is below max_norm let ( norm , _res ) = f . compare_num1D ( "_x" , start , end , num_values , max_norm ) ; println ! ( "norm = {}, _res = {}" , norm, _res ) ; a symbolic function can be defined in a more straightforward way without parsing expression // SOME USEFUL FEATURES // first define symbolic variables let vector_of_symbolic_vars = Expr :: Symbols ( "a, b, c" ) ; println ! ( "vector_of_symbolic_vars = {:?}" , vector_of_symbolic_vars ) ; let ( mut a , mut b , mut c ) = ( vector_of_symbolic_vars [ 0 ] . clone ( ) , // consruct symbolic expression vector_of_symbolic_vars [ 1 ] . clone ( ) , vector_of_symbolic_vars [ 2 ] . clone ( ) ) ; let mut symbolic_expression =  a + Expr :: exp ( b * c ) ; println ! ( "symbolic_expression = {:?}" , symbolic_expression ) ; // if you want to change a variable inti constant: let mut expression_with_const =  symbolic_expression . set_variable ( "a" , 1.0 ) ; println ! ( "expression_with_const = {:?}" , expression_with_const ) ; let parsed_function = expression_with_const . sym_to_str ( "a" ) ; println ! ( "{}, sym to string:" ,  parsed_function ) ; calculate symbolic jacobian and evaluate it // JACOBIAN // instance of Jacobian _structure let mut Jacobian_instance = Jacobian :: new ( ) ; // function of 2 or more arguments let vec_of_expressions = _vec ! [ "2*_x^3+_y" .to_string ( ) , "1.0" .to_string ( ) ] ; // set _vector of functions Jacobian_instance . set_funcvecor_from_str ( vec_of_expressions ) ; // set _vector of variables //  Jacobian_instance.set_varvecor_from_str("_x, _y"); Jacobian_instance . set_variables ( _vec ! [ "_x" , "_y" ] ) ; // calculate symbolic jacobian Jacobian_instance . calc_jacobian ( ) ; // transform into human...kind of readable form Jacobian_instance . readable_jacobian ( ) ; // generate jacobian made of regular rust functions Jacobian_instance . jacobian_generate ( _vec ! [ "_x" , "_y" ] ) ; println ! ( "Jacobian_instance: functions  {:?}. Variables {:?}" , Jacobian_instance .vector_of_functions, Jacobian_instance .vector_of_variables ) ; println ! ( "Jacobian_instance: Jacobian  {:?} readable {:?}." , Jacobian_instance .symbolic_jacobian, Jacobian_instance .readable_jacobian ) ; for _i in 0 .. Jacobian_instance . symbolic_jacobian . len ( ) { for j in 0 .. Jacobian_instance . symbolic_jacobian [ _i ] . len ( ) { println ! ( "Jacobian_instance: Jacobian  {} row  {} colomn {:?}" , _i, j, Jacobian_instance .symbolic_jacobian [ _i ] [ j ] ) ; } } // calculate element of jacobian (just for control) let ij_element = Jacobian_instance . calc_ij_element ( 0 , 0 , _vec ! [ "_x" , "_y" ] , _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "ij_element = {:?} \n " , ij_element ) ; // evaluate jacobian to numerical _values Jacobian_instance . evaluate_func_jacobian ( & _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "Jacobian = {:?} \n " , Jacobian_instance .evaluated_jacobian ) ; // lambdify and evaluate function _vector to numerical _values Jacobian_instance . lambdify_and_ealuate_funcvector ( _vec ! [ "_x" , "_y" ] , _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "function _vector = {:?} \n " , Jacobian_instance .evaluated_functions ) ; // or first lambdify Jacobian_instance . lambdify_funcvector ( _vec ! [ "_x" , "_y" ] ) ; // then evaluate Jacobian_instance . evaluate_funvector_lambdified ( _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "function _vector after evaluate_funvector_lambdified = {:?} \n " , Jacobian_instance .evaluated_functions ) ; // evaluate jacobian to nalgebra matrix format Jacobian_instance . evaluate_func_jacobian_DMatrix ( _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "Jacobian_DMatrix = {:?} \n " , Jacobian_instance .evaluated_jacobian_DMatrix ) ; // evaluate function _vector to nalgebra matrix format Jacobian_instance . evaluate_funvector_lambdified_DVector ( _vec ! [ 10.0 , 2.0 ] ) ; println ! ( "function _vector after evaluate_funvector_lambdified_DMatrix = {:?} \n " , Jacobian_instance .evaluated_functions_DVector ) ; set and calculate the system of (nonlinear) algebraic equations //use the shortest way to solve system of equations // first define system of equations and initial guess let mut NR_instanse = NR :: new ( ) ; let vec_of_expressions = _vec ! [ "_x^2+_y^2-10" .to_string ( ) , "_x-_y-4" .to_string ( ) ] ; let initial_guess = _vec ! [ 1.0 , 1.0 ] ; // solve NR_instanse . eq_generate_from_str ( vec_of_expressions , initial_guess , 1e-6 , 100 , 1e-6 ) ; NR_instanse . solve ( ) ; println ! ( "result = {:?} \n " , NR_instanse .get_result ( ) .unwrap ( ) ) ; // or more verbose way... // first define system of equations let vec_of_expressions = _vec ! [ "_x^2+_y^2-10" .to_string ( ) , "_x-_y-4" .to_string ( ) ] ; let mut Jacobian_instance = Jacobian :: new ( ) ; Jacobian_instance . set_funcvecor_from_str ( vec_of_expressions ) ; Jacobian_instance . set_variables ( _vec ! [ "_x" , "_y" ] ) ; Jacobian_instance . calc_jacobian ( ) ; Jacobian_instance . jacobian_generate ( _vec ! [ "_x" , "_y" ] ) ; Jacobian_instance . lambdify_funcvector ( _vec ! [ "_x" , "_y" ] ) ; Jacobian_instance . readable_jacobian ( ) ; println ! ( "Jacobian_instance: functions  {:?}. Variables {:?}" , Jacobian_instance .vector_of_functions, Jacobian_instance .vector_of_variables ) ; println ! ( "Jacobian_instance: Jacobian  {:?} readable {:?}. \n " , Jacobian_instance .symbolic_jacobian, Jacobian_instance .readable_jacobian ) ; let initial_guess = _vec ! [ 1.0 , 1.0 ] ; // in case you are interested in Jacobian value at initial guess Jacobian_instance . evaluate_func_jacobian_DMatrix ( initial_guess . clone ( ) ) ; Jacobian_instance . evaluate_funvector_lambdified_DVector ( initial_guess . clone ( ) ) ; let guess_jacobian = ( Jacobian_instance . evaluated_jacobian_DMatrix ) . clone ( ) ; println ! ( "guess Jacobian = {:?} \n " , guess_jacobian.try_inverse ( ) ) ; // defining NR method instance and solving let mut NR_instanse = NR :: new ( ) ; NR_instanse . set_equation_sysytem ( Jacobian_instance , initial_guess , 1e-6 , 100 , 1e-6 ) ; NR_instanse . solve ( ) ; println ! ( "result = {:?} \n " , NR_instanse .get_result ( ) .unwrap ( ) ) ; set the system of ordinary differential equations (ODEs), compute the analytical Jacobian ana solve it with BDF method. //create instance of _structure for symbolic equation system and Jacobian let mut Jacobian_instance = Jacobian :: new ( ) ; // define argument andunknown variables let _x = Expr :: Var ( "_x" . to_string ( ) ) ; // argument let _y = Expr :: Var ( "_y" . to_string ( ) ) ; let z : Expr = Expr :: Var ( "z" . to_string ( ) ) ; //define equation system let eq1 : Expr = Expr :: Const ( - 1.0 as f64 ) * z . clone ( ) - ( Expr :: Const ( - 1.0 as f64 ) * _y . clone ( ) ) . exp ( ) ; let eq2 : Expr = _y ; let eq_system = _vec ! [ eq1, eq2 ] ; // set unkown variables let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; // set argument let arg = "_x" . to_string ( ) ; // set method let method = "BDF" . to_string ( ) ; // set initial conditions let t0 = 0.0 ; let y0 = _vec ! [ 1.0 , 1.0 ] ; let t_bound = 1.0 ; // set solver parameters (optional) let first_step = None ; let atol = 1e-5 ; let rtol = 1e-5 ; let max_step = 1e-3 ; let jac_sparsity = None ; let vectorized = false ; // create instance of ODE solver and solve the system let mut ODE_instance = ODEsolver :: new_complex ( eq_system , _values , arg , method , t0 , y0 . into ( ) , t_bound , max_step , rtol , atol , jac_sparsity , vectorized , first_step ) ; // here Jacobian is automatically generated and system is solved ODE_instance . solve ( ) ; // plot the solution (optonally) ODE_instance . plot_result ( ) ; //save results to file (optional) ODE_instance . save_result ( ) ; the laziest way to solve ODE with BDF // set RHS of system as _vector of strings let RHS = _vec ! [ "-z-exp(-_y)" , "_y" ] ; // parse RHS as symbolic expressions let Equations = Expr :: parse_vector_expression ( RHS . clone ( ) ) ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; println ! ( "Equations = {:?}" , Equations ) ; // set argument let arg = "_x" . to_string ( ) ; // set method let method = "BDF" . to_string ( ) ; // set initial conditions let t0 = 0.0 ; let y0 = _vec ! [ 1.0 , 1.0 ] ; let t_bound = 1.0 ; // set solver parameters (optional) let first_step = None ; let atol = 1e-5 ; let rtol = 1e-5 ; let max_step = 1e-3 ; let jac_sparsity = None ; let vectorized = false ; // create instance of ODE solver and solve the system let mut ODE_instance = ODEsolver :: new_complex ( Equations , _values , arg , method , t0 , y0 . into ( ) , t_bound , max_step , rtol , atol , jac_sparsity , vectorized , first_step ) ; ODE_instance . solve ( ) ; ODE_instance . plot_result ( ) ; Backward Euler method //  Backward Euler method: slightly non-linear ODE let RHS = _vec ! [ "-z-exp(-_y)" , "_y" ] ; // parse RHS as symbolic expressions let Equations = Expr :: parse_vector_expression ( RHS . clone ( ) ) ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; println ! ( "eq_system = {:?}" , Equations ) ; let y0 = DVector :: from_vec ( _vec ! [ 1.0 , 1.0 ] ) ; let arg = "_x" . to_string ( ) ; let tolerance = 1e-2 ; let max_iterations = 500 ; let h = Some ( 1e-3 ) ; // this is the fixed time step version. // let h = None; - this is the adaptive time step version let t0 = 0.0 ; let t_bound = 1.0 ; let mut solver = BE :: new ( ) ; solver . set_initial ( Equations , _values , arg , tolerance , max_iterations , h , t0 , t_bound , y0 ) ; println ! ( "_y = {:?}, initial_guess = {:?}" , solver.newton._y,solver.newton.initial_guess ) ; solver . newton . eq_generate ( ) ; solver . solve ( ) ; let result = solver . get_result ( ) ; solver . plot_result ( ) ; Non-stiff methods are also available //Non-stiff equations: use ODE general api ODEsolver // RK45 and Dormand-Prince methods are available let RHS = _vec ! [ "-z-_y" , "_y" ] ; // parse RHS as symbolic expressions let Equations = Expr :: parse_vector_expression ( RHS . clone ( ) ) ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; println ! ( "Equations = {:?}" , Equations ) ; // set argument let arg = "_x" . to_string ( ) ; // set method let method = "DOPRI" . to_string ( ) ; // "RK45".to_string(); // set initial conditions let t0 = 0.0 ; let y0 = _vec ! [ 1.0 , 1.0 ] ; let t_bound = 1.0 ; // set solver parameters (optional) let max_step = 1e-3 ; // create instance of ODE solver and solve the system let mut ODE_instance = ODEsolver :: new_easy ( Equations , _values , arg , method , t0 , y0 . into ( ) , t_bound , max_step , ) ; ODE_instance . solve ( ) ; ODE_instance . plot_result ( ) ; Discretization and jacobian for BVP let RHS = _vec ! [ "-z-_y" , "_y" ] ; // parse RHS as symbolic expressions let Equations = Expr :: parse_vector_expression ( RHS . clone ( ) ) ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; let arg = "_x" . to_string ( ) ; let n_steps = 3 ; let h = 1e-4 ; let BorderConditions = HashMap :: from ( [ ( "z" . to_string ( ) , ( 0 , 1000.0 ) ) , ( "_y" . to_string ( ) , ( 1 , 333.0 ) ) , ] ) ; let Y = _vec ! [ 1.0 , 1.0 , 1.0 , 1.0 , 1.0 , 1.0 ] ; let mut Jacobian_instance = Jacobian :: new ( ) ; // creating analytic discretized algebraic system, its functional representation, analytic Jacobian matrix and its functional representation Jacobian_instance . generate_BVP ( Equations . clone ( ) , _values . clone ( ) , arg . clone ( ) , 0.0 , None , Some ( n_steps ) , Some ( h ) , None , BorderConditions . clone ( ) ) ; // analytic Jacobian matrix let J = & Jacobian_instance . symbolic_jacobian ; // its functional representation let J_func = & Jacobian_instance . function_jacobian_IVP_DMatrix ; // analytic discretized algebraic system, let F = & Jacobian_instance . vector_of_functions ; // its functional representation let F_func = & Jacobian_instance . lambdified_functions_IVP_DVector ; let varvect = & Jacobian_instance . vector_of_variables ; println ! ( "_vector of variables {:?}" ,varvect ) ; let Ys = DVector :: from_vec ( Y . clone ( ) ) ; let J_eval1 = J_func ( 4.0 , & Ys ) ; println ! ( "Jacobian Dense: J_eval = {:?} \n " , J_eval1 ) ; // SPARSE JACOBIAN MATRIX with nalgebra (+sparse feature) crate Jacobian_instance . generate_BVP_CsMatrix ( Equations . clone ( ) , _values . clone ( ) , arg . clone ( ) , 0.0 , None , Some ( n_steps ) , Some ( h ) , None , BorderConditions . clone ( ) ) ; let J_func3 = & Jacobian_instance . function_jacobian_IVP_CsMatrix ; let J_eval3 = J_func3 ( 4.0 , & Ys ) ; println ! ( "Jacobian Sparse with CsMatrix: J_eval = {:?} \n " , J_eval3 ) ; // SPARSE JACOBIAN MATRIX with sprs crate Jacobian_instance . generate_BVP_CsMat ( Equations . clone ( ) , _values . clone ( ) , arg . clone ( ) , 0.0 , None , Some ( n_steps ) , Some ( h ) , None , BorderConditions . clone ( ) ) ; let J_func2 : & Box < dyn Fn ( f64 , & CsVec < f64 > ) -> CsMat < f64 > > = & Jacobian_instance . function_jacobian_IVP_CsMat ; let F_func2 = & Jacobian_instance . lambdified_functions_IVP_CsVec ; let Ys2 = CsVec :: new ( Y . len ( ) , _vec ! [ 0 , 1 , 2 , 3 , 4 , 5 ] , Y . clone ( ) ) ; println ! ( "Ys = {:?} \n " , & Ys2 ) ; let F_eval2 = F_func2 ( 4.0 , & Ys2 ) ; println ! ( "F_eval = {:?} \n " , F_eval2 ) ; let J_eval2 : CsMat < f64 > = J_func2 ( 4.0 , & Ys2 ) ; println ! ( "Jacobian Sparse with CsMat: J_eval = {:?} \n " , J_eval2 ) ; Boundary Value Problem (BVP) with Newton-Raphson method with "Naive" _flag it means that Jacobian recalculated every iteration let eq1 = Expr :: parse_expression ( "_y-z" ) ; let eq2 = Expr :: parse_expression ( "-z^2" ) ; let eq_system = _vec ! [ eq1, eq2 ] ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; let arg = "_x" . to_string ( ) ; let tolerance = 1e-5 ; let max_iterations = 5000 ; let max_error = 0.0 ; let t0 = 0.0 ; let t_end = 1.0 ; let n_steps = 200 ; let strategy = "Naive" . to_string ( ) ; // let strategy_params = None ; let method = "Sparse" . to_string ( ) ; // or  "Dense" let _linear_sys_method= None ; let ones = _vec ! [ 0.0 ; _values.len ( ) *n_steps ] ; let initial_guess : DMatrix < f64 > = DMatrix :: from_column_slice ( _values . len ( ) , n_steps , DVector :: from_vec ( ones ) . as_slice ( ) ) ; let mut BorderConditions = HashMap :: new ( ) ; BorderConditions . insert ( "z" . to_string ( ) , ( 0 as usize , 1.0 as f64 ) ) ; BorderConditions . insert ( "_y" . to_string ( ) , ( 1 as usize , 1.0 as f64 ) ) ; assert ! ( &eq_system.len ( ) == & 2 ) ; let mut nr = NRBVP :: new ( eq_system , initial_guess , _values , arg , BorderConditions , t0 , t_end , n_steps , strategy , strategy_params , _linear_sys_method , method , tolerance , max_iterations , max_error ) ; println ! ( "solving system" ) ; let solution = nr . solve ( ) . unwrap ( ) ; // println!("result = {:?}", solution); nr . plot_result ( ) ; Boundary Value Problem (BVP) with Newton-Raphson method with "Frozen" _flag it means that Jacobian recalculated on condition:
Description of strategy (conditoon of recakc)/                                                 key of strategy     /   value user must provude for strategy only first time:    /                                                                        "Frozen_naive"   /                None every m-th time, where m is a parameter of the strategy:        /                            "every_m"         /                 m every time when the solution norm greater than a certain threshold A:    /                   "at_high_norm".    /                A when norm of (_i-1) iter multiplied by certain value B(<1) is lower than norm of _i-th iter : /"at_low_speed".    /                B complex - combined strategies 2,3,4       /                                                   "complex"      /        _vec of  parameters [m, A, B] let eq1 = Expr :: parse_expression ( "_y-z" ) ; let eq2 = Expr :: parse_expression ( "-z^2" ) ; let eq_system = _vec ! [ eq1, eq2 ] ; let _values = _vec ! [ "z" .to_string ( ) , "_y" .to_string ( ) ] ; let arg = "_x" . to_string ( ) ; let tolerance = 1e-5 ; let max_iterations = 50 ; let max_error = 0.0 ; let t0 = 0.0 ; let t_end = 1.0 ; let n_steps = 800 ; // Dense: 200 -300ms, 400 - 2s, 800 - 22s, 1600 - 2 min, let strategy = "Frozen" . to_string ( ) ; // let strategy_params = Some ( HashMap :: from ( [ ( "complex" . to_string ( ) , Some ( Vec :: from ( [ 2 as f64 , 5.0 , 1e-1 , ] ) ) ) ] ) ) ; /* or Some(HashMap::from([("Frozen_naive".to_string(), None)])); or Some(HashMap::from([("every_m".to_string(), Some(Vec::from( [ 5 as f64]  )) )])); or Some(HashMap::from([("at_high_morm".to_string(), Some(Vec::from( [ 5 as f64]  )) )])); or Some(HashMap::from([("at_low_speed".to_string(), Some(Vec::from( [ 1e-2]  )) )])); or Some(HashMap::from([("complex".to_string(), Some(Vec::from( [ 2.0, 5.0, 1e-, ]  )) )])); */ let method = "Sparse" . to_string ( ) ; // or  "Dense" let _linear_sys_method = None ; let ones = _vec ! [ 0.0 ; _values.len ( ) *n_steps ] ; let initial_guess : DMatrix < f64 > = DMatrix :: from_column_slice ( _values . len ( ) , n_steps , DVector :: from_vec ( ones ) . as_slice ( ) ) ; let mut BorderConditions = HashMap :: new ( ) ; BorderConditions . insert ( "z" . to_string ( ) , ( 0 as usize , 1.0 as f64 ) ) ; BorderConditions . insert ( "_y" . to_string ( ) , ( 1 as usize , 1.0 as f64 ) ) ; assert ! ( &eq_system.len ( ) == & 2 ) ; let mut nr = NRBVP :: new ( eq_system , initial_guess , _values , arg , BorderConditions , t0 , t_end , n_steps , strategy , strategy_params , _linear_sys_method , method , tolerance , max_iterations , max_error ) ; println ! ( "solving system" ) ; let solution = nr . solve ( ) . unwrap ( ) ; // println!("result = {:?}", solution); nr . plot_result ( ) ; Modified Newton method or Damped Newton method for solving a system of nonlinear ordinary differential equations. This code implements a modified Newton method for solving a system of non-linear boundary value problems..
The code mostly inspired by sources listed below: Cantera MultiNewton solver (MultiNewton.cpp ) TWOPNT fortran solver (see "The Twopnt Program for Boundary Value Problems" by J. F. Grcar and Chemkin Theory Manual p.261)
A pair of words how to solve BVP problems with the "Damped" feature flag.
This algorithm is often used to solve large nonlinear problems.
Let us briefly discuss the "strategy_params" HashMap that defines the solver parameters. "max_jac"  key:  maximum iterations with old Jacobian, None value means the default number is taken-3; "maxDampIter" key:  maximum number of damped steps, None value means the default value of 5 is used; "DampFacor" key: factor to decrease the damping coefficient, None value means the default value of 0.5 is used; "adaptive" key: None means no grid refinement is used, if Some - grid refinement is enabled. The first parameter of value vec means what criteria to choose is refinement needed or not in the current iteration, second parameter means maximum number of refinments allowed
Next key-value is optoional and define the name of specific grid refinement algorithm and its parameters;
we recommend to use one of the following algorithms: key: "pearson", value: a f64 value less than 1, typically from 0.1 to 0.5; key "grcar_smooke" value: a pair of f64 values less than 1, typically first less than second;
if the problem is large and highly nonlinear the best choise is to use the adaptive grid.
"We have found that starting the itration on a coarse mesh has several important advntages. One is that the Newton iteration is more likely to
converge on a coarse mesh than on a fine mesh. Moreover, the number of variables is small on a coarse mesh and thus the cost per iteration is
relatively small. Since the iteration begins from a user-specfied “guess” at the solution, it is likly that many iterations will be required.
Ultimately, of course, to be accurate, the solution must be obtained on a fine mesh. However, as the solution is computed on each successively finer
mesh, the starting estimates are better, since they come from the converged solution on the previous coarse mesh. In general, the solution on one
mesh lies within the domain of convergence of Newton’s method on the next finer mesh.Thus, even though the  cost per iteration is increasing, the
number of required iterations is decreasing. The adaptve placement of the mesh points to form the finer meshes is done in such a
way that the total number of mesh points needed to represent the solution accurately is minimized" Chemkin Theory Manual p.263
So if you choose to use adaptive grid, you should start with the low quantiy of steps (n_steps parameter), grid refinement algorithm will choose
the sufficient number of points. let eq1 = Expr :: parse_expression ( "y-z" ) ; let eq2 = Expr :: parse_expression ( "-z^3" ) ; let eq_system = vec ! [ eq1, eq2 ] ; let values = vec ! [ "z" .to_string ( ) , "y" .to_string ( ) ] ; let arg = "x" . to_string ( ) ; let tolerance = 1e-5 ; let max_iterations = 20 ; let t0 = 0.0 ; let t_end = 1.0 ; let n_steps = 10 ; // let strategy = "Damped" . to_string ( ) ; // let strategy_params = match strategy . as_str ( ) { "Naive" => None , "Damped" => Some ( HashMap :: from ( [ ( "max_jac" . to_string ( ) , // maximum iterations with old Jacobian, None means the default number is taken-3 None , ) , ( "maxDampIter" . to_string ( ) , // maximum number of damped steps, None means the default value of 5 is used None , ) , ( "DampFacor" . to_string ( ) , // factor to decrease the damping coefficient, None means the default value of 0.5 is used None , ) , ( "adaptive" . to_string ( ) , // adaptive strategy parameters, None means no grid refinement is used, if Some - grid refinement is enabled // first parameter means what criteria to choose is refinement needed or not in the current iteration, second parameter means // maximum number of refinments allowed Some ( vec ! [ 1.0 , 5.0 ] ) // or  None ) , // the name of grid refinement strategy, this key-value pair will be used only if "adaptive" is Some, in opposite case this pair // will be ignored: vector of parametrs is used inside the grid refinement algorithm ( "pearson" . to_string ( ) , Some ( vec ! [ 0.2 ] ) ) // or        ("grcar_smooke".to_string(), Some(vec![0.2, 0.5] ) ) ] ) ) , "Frozen" => Some ( HashMap :: from ( [ ( "every_m" . to_string ( ) , Some ( Vec :: from ( [ 5 as f64 ] ) ) ) ] ) ) , & _=> panic ! ( "Invalid strategy!" ) } ; let method = "Sparse" . to_string ( ) ; // or  "Dense" let linear_sys_method = None ; let ones = vec ! [ 0.0 ; values.len ( ) *n_steps ] ; let initial_guess : DMatrix < f64 > = DMatrix :: from_column_slice ( values . len ( ) , n_steps , DVector :: from_vec ( ones ) . as_slice ( ) ) ; let mut BorderConditions = HashMap :: new ( ) ; BorderConditions . insert ( "z" . to_string ( ) , ( 0usize , 1.0f64 ) ) ; BorderConditions . insert ( "y" . to_string ( ) , ( 1usize , 1.0f64 ) ) ; let Bounds = HashMap :: from ( [ ( "z" . to_string ( ) , ( - 10.0 , 10.0 ) , ) , ( "y" . to_string ( ) , ( - 7.0 , 7.0 ) , ) ] ) ; let rel_tolerance = HashMap :: from ( [ ( "z" . to_string ( ) , 1e-4 ) , ( "y" . to_string ( ) , 1e-4 , ) ] ) ; assert_eq ! ( &eq_system.len ( ) , & 2 ) ; let mut nr = BVP :: new ( eq_system , initial_guess , values , arg , BorderConditions , t0 , t_end , n_steps , strategy , strategy_params , linear_sys_method , method , tolerance , max_iterations , Some ( rel_tolerance ) , Some ( Bounds ) ) ; println ! ( "solving system" ) ; # [ allow ( unused_variables ) ] nr . solve ( ) ; // println!("result = {:?}", solution); nr . plot_result ( ) ; nr . save_to_file ( None ) ; Testing Our project is covered by tests and you can run them by standard command cargo test Contributing If you have any questions, comments or want to contribute, please feel free to contact us at https://github.com/ To do [_x] Write basic functionality [_x] Write jacobians [_x] Write Newton-Raphson [_x] Write BDF [_x] Write Backward Euler [_x] Write some nonstiff methods [_x] Add indexed variables and matrices Add more numerical methods for ODEs [_x] Add BVP methods for stiff ODEs

======>
https://github.com/Gleb-Zaslavsky/KiThe
-->>-->>
Repository files navigation README [TOC] KiThe This is a package of structures, functions and databases useful for such areas as chemical thermodynamics, chemical kinetics, as well as modeling of chemical reactors, combustion, processes in shock tubes and rocket engines, propulsion. Content Usage Testing Contributing To do Usage parse reaction equations into a list of substances use KiThe :: reaction_analyzer :: ReactionAnalyzer ; let mut ReactionAnalyzer_instance = ReactionAnalyzer :: new ( ) ; let reactions_ : Vec < & str > = vec ! [ "A=B" , "B->A + 3C" , "2B+A=D" ] ; let reaction = reactions_ . iter ( ) . map ( |s| s . to_string ( ) ) . collect ( ) ; ReactionAnalyzer_instance . reactions = reaction ; ReactionAnalyzer_instance . search_substances ( ) ; println ! ( "substances: {:?}" , ReactionAnalyzer_instance .substances ) ; parse reaction equations into a stoichiometric matrix, matrix of coefficients of direct reactions and matrix of coefficients of reverse reactions, matrix of degrees of concentration for the
kinetic function, use KiThe :: reaction_analyzer :: ReactionAnalyzer ; let mut ReactionAnalyzer_instance = ReactionAnalyzer :: new ( ) ; let reactions_ : Vec < & str > = vec ! [ "A=B" , "B->A + 3C" , "2B+A=D" ] ; let reaction = reactions_ . iter ( ) . map ( |s| s . to_string ( ) ) . collect ( ) ; ReactionAnalyzer_instance . reactions = reaction ; ReactionAnalyzer_instance . search_substances ( ) ; ReactionAnalyzer_instance . analyse_reactions ( ) ; println ! ( "substances: {:?}" , ReactionAnalyzer_instance .substances ) ; println ! ( "{:?}" , ReactionAnalyzer_instance ) ; crate is equipped with a libraries of kinetic parameters of chemical reactions obtained as a result of parsing publicly available databases, so you can
view all libraries and all reactions in every library
of kinetic DB, search reactions by substances and so on. Most important methods below let mut kin_instance = KineticData :: new ( ) ; // collecting reaction data for library name lib kin_instance . open_json_files ( lib ) ; // veiew all reactions in library kin_instance . print_all_reactions ( ) ; // returns reaction ID and reaction data (parsed from json) for given reaction equation kin_instance . search_reaction_by_equation ( equation ) // search reactions by substances kin_instance . search_reaction_by_reagents_and_products ( reagents ) The module is automatic chemical mechanism constructor and takes as input the name of the library and the vector of substances and then produces the following data: all reactions of starting substances with each other, and all reactions of all their possible products with each other and with original substances. HashMap with kinetic data of all found reactions let mut mech_search = Mechanism_search :: new ( vec ! [ "O" .to_string ( ) , "NH3" .to_string ( ) , "NO" .to_string ( ) ] , "NUIG" . to_string ( ) , Vec :: new ( ) , Vec :: new ( ) , Vec :: new ( ) , ) ; let ( mechanism , reactants , vec_of_reactions ) = mech_search . mechfinder_api ( ) ; Calculation of atomic composition and molar mass use KiThe :: molmass :: calculate_molar_mass ; let formula = "C6H8O6" ; let ( molar_mass , element_composition ) = calculate_molar_mass ( formula . to_string ( ) ) ; println ! ( "Element counts: {:?}" , element_composition ) ; println ! ( "Molar mass: {:?} g/mol" , molar_mass ) ; use KiThe :: molmass :: parse_formula ; let formula = "Na(NO3)2" . to_string ( ) ; let atomic_composition = parse_formula ( formula ) ; println ! ( "{:?}" , atomic_composition ) ; Testing Our project is covered by tests and you can run them by standard command cargo test Contributing If you have any questions, comments or want to contribute, please feel free to contact us at https://github.com/ To do Add libraries of chemical reactions with appropriate methods for processing, searching, and retrieving data. Add libraries of chemical substances... Add numerical methods (may be cpp open source...)

======>
https://github.com/DavJCosby/sled/releases/tag/0.2.0
-->>-->>
Releases 0.2.0 0.2.0 Latest Latest Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags DavJCosby released this 25 Nov 22:46 0.2.0 2e07c17 Breaking Changes from 0.1.1 In order to support Error handling on no_std systems, we had to bump the MSRV up to 1.81 The previously deprecated Scheduler.change_hz() method has been removed in favor of Scheduler.set_hz() Sled::new_from_string(String) and Config::from_string(String) removed in favor of Sled::new_from_str(&str) and Config::from_str(&str) Added Crate now supports most no_std environments thanks to some fantastic work done by @claudiomattera Introduces CustomDriver and CustomScheduler which allows you to create Drivers and Schedulers backed by custom Instant and Sleeper implementations. Also introduces AsyncCustomScheduler Introduces a libm feature flag for no_std environments that'll need it. Changed Examples have been extracted to the spatial_led_examples , bringing down our development dependencies considerably. This will also allow us to share more environment-specific examples, like Raspberry Pi or ESP32 implementations. Documentation improvements. New Contributors @claudiomattera made their first contribution in #86 Full Changelog : 0.1.1...0.1.2 Contributors claudiomattera Assets 2 Loading All reactions

======>
https://old.reddit.com/u/claudiomattera
-->>-->>
I made a general example of using Embassy on ESP32-C3: https://github.com/claudiomattera/esp32c3-embassy . async for everything, including communicating with sensor over I²C, E-Ink display over SPI, and time server over WiFi. Deep sleep and RTC memory no_std , no ESP-IDF Up-to-date with the latest crates versions (except reqwless , which I just noticed had a new version a couple weeks ago). I got really frustrated with all the existing examples, especially due to the last point. esp-hal and company are very unstable and change interface at nearly every release, so examples become outdated really fast.
======>
https://davjcosby.github.io/all-published/miscellaneous-tech/Introducing%20Sled,%20a%20Rust%20Library%20for%20Creating%20Spatial%20LED%20Strip%20Lighting%20Effects.html
-->>-->>
Set Colors By Distance To set all LEDs 2 Units away from the center to red: sled . set_at_dist ( 2.0 , Rgb :: new ( 1.0 , 0.0 , 0.0 ) ) ; // or relative to any other point using: sled . set_at_dist_from ( DISTANCE , POSITION , COLOR ) Under the hood, the Sled is just performing an intersection test between each line segment and a circle formed by the radius. It then colors the LEDs at each point of intersection. Similar method like set_at_angle(angle, color) or set_at_dir(direction, color) , work by running a simple intersection test between line segments and a ray.
======>
https://typer.tiangolo.com/
-->>-->>
fast → 💬 Run your application typer main.py run 💬 You get a nice error, you are missing NAME Usage: typer [PATH_OR_MODULE] run [OPTIONS] NAME Try 'typer [PATH_OR_MODULE] run --help' for help. ╭─ Error ───────────────────────────────────────────╮ │ Missing argument 'NAME'.                          │ ╰───────────────────────────────────────────────────╯ 💬 You get a --help for free typer main.py run --help Usage: typer [PATH_OR_MODULE] run [OPTIONS] NAME Run the provided Typer app. ╭─ Arguments ───────────────────────────────────────╮ │ *    name      TEXT  [default: None] [required]   | ╰───────────────────────────────────────────────────╯ ╭─ Options ─────────────────────────────────────────╮ │ --help          Show this message and exit.       │ ╰───────────────────────────────────────────────────╯ 💬 Now pass the NAME argument typer main.py run Camila Hello Camila 💬 It works! 🎉 restart ↻

======>
https://old.reddit.com/r/rust/comments/1gzooib/i_made_a_tool_in_rust_that_i_now_use_everyday/
-->>-->>
Started learning rust a year ago. I have finally made something 'useful' that runs everyday as part of my workflow.    https://github.com/cool-mist/joke-cli    - added it as part of my shell startup script.   
   

======>
https://amit.prasad.me/blog/async-oneshot
-->>-->>
In this post, I’ll go over a simple, yet fast implementation of the (commonly seen) “oneshot” channel, going through lower-level asynchronous Rust, synchronization primitives, all alongside the unseen footguns of concurrent code. We build and iterate on a real, dependency-free, async oneshot channel library, published on crates.io. Asynchronous Rust can be scary. Hell, writing concurrent code in general is always a little scary. Understanding “simple” primitives like this one often helps in understanding the more complex ones. Here’s a snippet demonstrating what we’ll be building: use async_oneshot_channel :: oneshot; use futures :: executor :: block_on; // A oneshot for sending a single integer let (tx, rx) = oneshot (); // Send a value tx . send ( 42 ) . unwrap (); // Receive the value asynchronously // Could be in another thread/task! let result = block_on (rx . recv ()); assert_eq! (result, Some ( 42 )); Groundwork, and the Problem Alrighty, let’s define what we want to solve with our oneshot channel. Wait a sec, why do we actually need oneshot channels? Good question! For illustrative purposes, consider an intra-process request-response system. Suppose we have a task, connected to the rest of the process via another “multi-shot” ( mpsc , below) channel — allowing other tasks in the process to send requests (“messages”) to our task: enum Request { Add ( i32 , i32 ) } struct MyTask { rx : mpsc :: Receiver < Request > } async fn run_my_task ( mut task : MyTask ) { while let Some (request) = task . rx . recv () . await { // Process the request match request { Request :: Add (a, b) => { let result = a + b; // Send the response back // ...? } } } } // Elsewhere: let (tx, rx) = mpsc :: channel :: < Request >(); // Assuming we construct MyTask with `rx`, use `tx`: tx . send ( Request :: Add (a, b)) . await ; // Send a request Sure, but is this toy example applicable to real software? Yep! In the real-world we see this pattern in the Actor execution model, which has been used to model many concurrent (and distributed!) systems for decades. Back to the problem. Once our task processes the input, it needs to send a response back to the caller. We have no way of knowing “where” in the code, or what other task to return our response to, especially since an mpsc channel implies m ultiple p roducers. In other words, there is no 1-1 correspondence implied by the channel, and even if we constrained to the 1-1 case, we don’t have a great way to return the request across this 1-way channel. In comes oneshot: // Let's include a oneshot response channel, in our request: enum Request { Add ( i32 , i32 , oneshot :: Sender < i32 >) } // In our task: async fn run_my_task ( mut task : MyTask ) { while let Some (request) = task . rx . recv () . await { // Process the request, as before match request { Request :: Add (a, b, tx) => { let result = a + b; // But now, we can respond via the oneshot! tx . send (result) . unwrap (); } } } } We now include a “handle” via which we can send the response back to the original caller. We construct the receiving end of the oneshot whilst sending the request, so the caller can wait for the response at their leisure: // At the caller site: let (tx, rx) = oneshot :: < i32 >(); tx . send ( Request :: Add ( 1 , 2 , tx)) . await ; do_some_other_work (); // Now, we can wait for the response via `rx`: let result = rx . recv () . await . unwrap (); Alright great, a motivating use-case. What does a oneshot channel need to look like to support this? For starters, we need obviously need to support: Sending at most one value over the channel, reliably. Any subsequent sends should fail. Receiving at most one value from the channel Where “reliably” means that a single send operation will always succeed. “At most one value”? Why not just “one”? Well, what if MyTask encounters an error and crashes? We don’t want the caller to wait forever, hence another requirement: If all sender handles are dropped, the receiver is notified that the value will never arrive Remember, Rust’s borrow checker semantics mean that if the last oneshot::Sender is dropped, then it’s guaranteed that there is absolutely no way to send a value over the channel anymore. And finally, one important requirement: Waiting to receive a value should be asynchronous (non-blocking) What does that mean? Why is that important? In asynchronous applications, there could be thousands of tasks running on a handful of threads. Each task “takes turns” actually running on a thread. If a task is waiting for a response, it isn’t doing any computational work, so the thread should be free to run other tasks. Imagine if waiting for a response blocked the entire thread. We wouldn’t be able to run any other tasks on that thread, and we’d be keeping the processor busy doing nothing! The Implementation A naive implementation seems simple enough, just use a Mutex<Option<T>> , right? use std :: sync :: { Mutex , Arc }; pub struct OneshotChan < T > { inner : Arc < Mutex < Option < T >>> } impl < T > OneshotChan < T > { pub fn send ( & self , value : T ) -> Result <(), T > { let mut inner = self . inner . lock () . unwrap (); // If there's already a value, return the input if inner . is_some () { return Err (value); } // Otherwise, set the value * inner = Some (value); Ok (()) } } Okay, but what about the receiver? We need our receiving task to be notified when inner is set to Some(..) . Hmm, let’s try this? use std :: sync :: { Mutex , Arc }; pub struct OneshotChan < T > { inner : Arc < Mutex < Option < T >>> } impl < T > OneshotChan < T > { pub fn recv ( & self ) -> T { while self . inner . lock () . unwrap () . is_none () { // Spin until the value is set } // Return the value self . inner . lock () . unwrap () . take () } } Ah wait! This isn’t async. What did I do wrong? Remember how we said that waiting should be non-blocking, and shouldn’t consume the thread? This implementation works by constantly locking the Mutex and checking if the value is set, over and over again. This is called “busy-waiting” and is generally a terrible idea. In fact, it’s even worse here, since locking a Mutex isn’t free, and we’re doing it in a loop, not to mention that this doesn’t achieve most of the stated requirements we had above. Okay, so no busy waiting. Then how do we notify the receiver without the receiver constantly checking? Good question, once again! Here we turn to Rust’s async side, and the Future trait. Futures in Rust are effectively creating a state machine which asks an “executor” to run them in a specified way. If you’ve heard of tokio , or smol , or async-std , you’ve heard of an executor. The executor is responsible for scheduling and running futures/tasks, and how they interact with threads (if at all). Let’s take a look at making a simple future, starting with the skeleton: // We implement the Future trait to define a future use std :: future :: Future ; use std :: task :: { Poll , Context , Waker }; pub struct ReceiveFuture < T > {} impl < T > Future for ReceiveFuture < T > { // What does the future return on completion? type Output = T ; // Huh? fn poll ( self : Pin < & mut Self >, cx : & mut Context <' _ >) -> Poll < Self :: Output > {} } Lots to unpack, the “Output” type seems pretty straightforward. What’s going on with Pin , Context , and Poll ? Pin has been explained by those more familiar than I, for the purposes of this post, think of it as a &self that’s “more async-friendly”. Let’s start with Poll . Poll is an enum with two variants which tell the executor what the future’s status is: Poll::Ready(T) : The future has completed, and the value T is ready to be returned to the caller. Poll::Pending : The future is not yet ready, and needs to be polled again Okay, so the executor “polls” the future, and the future tells the executor if it’s ready? Exactly! But how does the executor know when to poll the future again if it’s not ready? That’s where Context comes in. Context allows us to get a handle to something called a Waker , which has methods to wake up (re-poll) the future. So our future has access to the handle can be used to wake it up. Hold up, how does that help anything? The Waker is inside the same future that wants to be woken up! If it’s already returned Poll::Pending , it would never have a chance to wake itself up, right? Good point! That’s why we typically pass the Waker to some other thread or task. Let’s apply this to our OneshotChan . First, let’s modify our OneshotChan to have a field for storing the Waker , and our ReceiveFuture to store a reference to the OneshotChan it’s associated with: pub struct OneshotChan < T > { inner : Mutex < Option < T >> // We'll store the waker here. waker : Cell < Option < Waker >> } pub struct ReceiveFuture < T > { // Keep a reference to the channel chan : Arc < OneshotChan < T >> } Notice how we’re storing the Waker in a Cell . This is so that we can modify it via interior mutability, briefly discussed later on. Let’s modify our send method to wake up the receiver once we’ve set the value we’re sending. We’ll also modify the recv method to create an instance of ReceiveFuture : impl < T > OneshotChan < T > { pub fn send ( & self , value : T ) -> Result <(), T > { let mut inner = self . inner . lock () . unwrap (); if inner . is_some () { return Err (value); } * inner = Some (value); // If there's a waker, wake it up! if let Some (waker) = self . waker . take () { waker . wake (); } Ok (()) } // Now, this just creates the future pub fn recv ( & self ) -> ReceiveFuture < T > { ReceiveFuture { chan : self . inner . clone () } } } And finally, we can implement the Future trait for ReceiveFuture : impl < T > Future for ReceiveFuture < T > { type Output = T ; fn poll ( self : Pin < & mut Self >, cx : & mut Context <' _ >) -> Poll < Self :: Output > { // First, store a reference to the waker in the channel let waker = cx . waker () . clone (); self . chan . waker . set ( Some (waker)); // Then, check if the value is set let mut inner = self . chan . inner . lock () . unwrap (); if let Some (value) = inner . take () { Poll :: Ready (value) } else { // If not, return Pending Poll :: Pending } } } Here’s what we’ve effectively done: When the receiver awaits the future, it stores a waker handle in the channel. The receiver then locks and checks if the channel’s value has been set. If it has, we’re done! Return Poll::Ready(..) with the value. If not, we return Poll::Pending , and since the waker is stored in the channel, the sender can wake up the receiver when it sets the value. That’s it? That’s all we need? Well… not quite. As it stands, this implementation still has a few issues. For one, we’re not correctly implementing the “oneshot” semantics as we defined above. Since we’re using Option::take , we’re effectively “resetting” the channel after each successful receive, meaning the same channel can be used for another value. Mutexes are also relatively expensive — We don’t expect this oneshot channel to be used in high-contention scenarios, but it’s a good exercise to see how we can avoid using a Mutex altogether. It would be great if we could solve both of these problems in a single go, wouldn’t it? Go on… Alrighty. Setting the channel just once. Let’s search the standard library for something that might help. Hmm… Once in std::sync looks interesting. What do the docs say? “A low-level synchronization primitive for one-time global execution.” Sounds like what we need! Once::call_once looks like it takes a closure and ensures the closure is executed exactly once across even if called concurrently. Let’s try using it: pub struct OneshotChan < T > { inner : Mutex < Option < T >> waker : Cell < Option < Waker >> // Keep track of whether the value has been set via Once tx : Once } // In send: pub fn send ( & self , value : T ) -> Result <(), T > { // Call the closure only once self . tx . call_once ( || { let mut inner = self . inner . lock () . unwrap (); // No need to check if `inner` is already set, `Once` ensures this * inner = Some (value); if let Some (waker) = self . waker . take () { waker . wake (); } }); Ok (()) } Ok. Couple of issues. First off, we’re always returning Ok(()) from send , even if the value was already set. How do we idiomatically detect if the value was already set? // Take 2 pub fn send ( & self , value : T ) -> Result <(), T > { let mut data = Some (value); self . tx . call_once ( || { let mut inner = self . inner . lock () . unwrap (); // data.take() always returns Some(..), and sets data to None * inner = Some (data . take () . unwrap ()); if let Some (waker) = self . waker . take () { waker . wake (); } }); match data { // Data is Some(..) if call_once didn't run Some (value) => Err (value), // Data is None if call_once ran None => Ok (()) } } Cool trick with data.take() to check if Once was just run, right? Now, we still haven’t solved the issue of the Mutex here. First, ask ourselves what property of the Mutex we’re actually using here. Notice that send takes an immutable reference to &self , yet we store data in self . This means we’re using our Mutex for something called “interior mutability”, where we mutate data inside a container object without a mutable reference to the container itself, whilst still satisfying the Rust borrow checker. I won’t go into depth here, but you can read more about interior mutability in the Rust book . So, interior mutabilty without a Mutex? Yep, we can simply use Cell for this, like we did with the Waker . Let’s just replace the Mutex with a Cell : use std :: cell :: Cell ; pub struct OneshotChan < T > { // No more Mutex! inner : Cell < Option < T >> waker : Cell < Option < Waker >> tx : Once } and modify our send method: pub fn send ( & self , value : T ) -> Result <(), T > { let mut data = Some (value); self . tx . call_once ( || { // No more locking, just: self . inner . set (data . take ()); if let Some (waker) = self . waker . take () { waker . wake (); } }); match data { // Data is Some(..) if call_once didn't run Some (value) => Err (value), // Data is None if call_once ran None => Ok (()) } } and finally, our ReceiveFuture needs to be modified to account for Once and Cell : impl < T > Future for ReceiveFuture < T > { type Output = T ; fn poll ( self : Pin < & mut Self >, cx : & mut Context <' _ >) -> Poll < Self :: Output > { // Unchanged, storing a waker: let waker = cx . waker () . clone (); self . chan . waker . set ( Some (waker)); // Instead of locking, check if the `Once` has run: if self . chan . tx . is_completed () { Poll :: Ready ( self . chan . inner . take ()) } else { Poll :: Pending } } } Does a second await of ReceiveFuture return None , as we expect it to? Let’s see. The first await only returns once the value is set, and returns via self.chan.inner.take() . Here’s how Cell::take is implemented in the standard library: impl < T : Default > Cell < T > { /// Takes the value of the cell, leaving `Default::default()` in its place. pub fn take ( & self ) -> T { self . replace ( Default :: default ()) } } And since Option<T> defaults to None , a take() replaces the cell value with None . A second await just takes a None out of the cell, and returns it, as we expect. While going through the standard library documentation, however, I notice this note on Once::call_once : “This method will block the calling thread if another initialization routine is currently running.” If you’ve written thread-synchronized code before, this should be ringing alarm bells: Do as little as possible inside call_once ! Again, we don’t expect this to be high-contention, but if we can try optimizing it, why not? pub fn send ( & self , value : T ) -> Result <(), T > { let mut data = Some (value); self . tx . call_once ( || { self . inner . set (data . take ()); // Move the waker call from here... }); match data { Some (value) => Err (value), None => { // ...to here if let Some (waker) = self . waker . take () { waker . wake (); } Ok (()) } } } Now let’s review our requirements: Sending at most one value over the channel, reliably. Any subsequent sends should fail. We’ve ensured this via Once . Receiving at most one value from the channel Again, via Once and Cell::take . Waiting to receive a value should be asynchronous (non-blocking) We’re using a Waker and the Future trait to ensure this. If all sender handles are dropped, the receiver is notified that the value will never arrive Ah, we haven’t gotten to this part yet. Immediate problem: We don’t have a “sender handle” to drop, without dropping the entire channel. Since the ReceiveFuture has an Arc to the channel, we can’t just drop the channel either. Let’s do a quick refactor to move public send functionality to a separate Sender struct: // Modify OneshotChan so `send` is private (not shown) // We can clone the sender handle! There's no limit on potential senders, // just on there being a single value sent during the channel's lifetime. #[derive( Clone )] pub struct Sender < T > { chan : Arc < OneshotChan < T >> } impl < T > Sender < T > { pub fn send ( & self , value : T ) -> Result <(), T > { self . chan . send (value) } } But how are we meant to keep track of the number of senders? Let’s see… Senders are associated with their channels, and it looks like we need to do some manual reference counting on how many senders there are. So we can keep track of a counter inside our OneshotChan . Since senders could be created/cloned on any thread, let’s use atomics from the standard library: use std :: sync :: atomic :: { AtomicUsize , Ordering }; pub struct OneshotChan < T > { inner : Cell < Option < T >> waker : Cell < Option < Waker >> tx : Once // Keep track of the number of senders sender_rc : AtomicUsize } And then we can increment and decrement this counter when Clone and Drop are called on the Sender : // Move away from #[derive(Clone)], so we can implement ref-counting impl < T > Clone for Sender < T > { fn clone ( & self ) -> Self { // Atomic increment self . chan . sender_rc . fetch_add ( 1 , Ordering :: Release ); Self { chan : self . chan . clone () } } } impl < T > Drop for Sender < T > { fn drop ( & mut self ) { // Atomic decrement self . chan . sender_rc . fetch_sub ( 1 , Ordering :: AcqRel ); } } What is the meaning behind the Ordering::Release and Ordering::AcqRel ? Since we’re using a single atomic variable, we don’t really need to worry about the exact meaning of Ordering here. In a nutshell, by default, atomic operations only guarantee that a “load-then-store” happens atomically, meaning that an x += 1 will always actually result in an increment of 1. Release followed by Acquire guarantees to us that some notoion of causality between operations is established. The important part is that drop always sees the latest sender_rc value. Rust borrows these naming conventions from C++, so if you’re interested in the details, you can read more about it in the Rustnomicon . It doesn’t look like we’re notifying the receiver when there are no more senders. How do we do that? Getting to that! We can use the drop implementation to check if the sender_rc has just been decremented to 0, and if so, wake up the receiver: impl < T > Drop for Sender < T > { fn drop ( & mut self ) { // If the `fetch` returns 1, then we just decremented to 0 if self . chan . sender_rc . fetch_sub ( 1 , Ordering :: AcqRel ) == 1 { // Wake up the receiver. Remember, we may not be receiving yet! if let Some (waker) = self . chan . waker . take () { waker . wake (); } } } } But remember, we aren’t setting the Once in OneshotChan , so we need to modify our ReceiveFuture to check if the sender_rc counter is 0: impl < T > Future for ReceiveFuture < T > { type Output = T ; fn poll ( self : Pin < & mut Self >, cx : & mut Context <' _ >) -> Poll < Self :: Output > { let waker = cx . waker () . clone (); self . chan . waker . set ( Some (waker)); if self . chan . tx . is_completed () { Poll :: Ready ( self . chan . inner . take ()) } else if self . chan . sender_rc . load ( Ordering :: Acquire ) == 0 { // If there are no more senders, return None Poll :: Ready ( None ) } else { Poll :: Pending } } } Again, the Ordering here is to ensure that there aren’t data race conditions between the sender_rc and the future’s Poll result. We want to ensure that we don’t erroneously return Poll::Pending as there may not be any senders left — leaving the task waiting forever. And that’s it? Just some finishing touches: // Renaming ReceiveFuture to Receiver, OneshotChan to Chan // Woah! Unsafe? unsafe impl < T : Send + Sync > Sync for Chan < T > {} unsafe impl < T : Send > Send for Chan < T > {} // And finally, a simple public constructor for the channel pub fn oneshot < T >() -> ( Sender < T >, Receiver < T >) { let chan = Arc :: new ( Chan :: new ()); ( Sender { chan : chan . clone () }, Receiver { chan }) } Don’t worry about the unsafe! We’re effectively telling the compiler that we’ve manually implemented the synchronization guarantees for our oneshot channel. And there we have it. We’ve built a “simple” oneshot channel, with all the requirements we set out to meet. We’ve also sidestepped some of the common crutches (ahem, Mutexes), that people default to when writing concurrent code, whilst learning about Once and atomics. The source code behind this oneshot channel can be found here , and the crate is published on crates.io . If you spot any issues in this post, or the crate, feel free to open an issue or contact me here . In this post, I’ll go over a simple, yet fast implementation of the (commonly seen) “oneshot” channel, going through lower-level asynchronous Rust, synchronization primitives, all alongside the unseen footguns of concurrent code. We build and iterate on a real, dependency-free, async oneshot channel library, published on crates.io.
======>
https://old.reddit.com/r/rust/comments/1gzqzas/chemical_reactors_combustion_kinetics/
-->>-->>
Hi everyone. I'm researching in combustion modelling, heat and mass transfer, chemical process and reactor modelling, chemical kinetics and Rust enthusiast ). Let me present two crates related to the topic of my interests.   

   KiThe (obviously kinetics + thermodynamics)  crate (   https://crates.io/crates/KiThe    or    https://github.com/Gleb-Zaslavsky/KiThe    ) so far implements the following features:   

   - parsing reaction equations into a list of substances, parsing reaction equations into a stoichiometric matrix, a forward reaction coefficient matrix and an inverse reaction coefficient matrix, and a concentration degree matrix for the kinetic function,   

   - crate is equipped with libraries of kinetic parameters of chemical reactions obtained by parsing publicly available databases, so you can browse all libraries and all reactions in each library of kinetic databases, search for reactions by substances, etc.   

   - the automatic chemical mechanism builder produces the following data:   

   all reactions of starting substances with each other, as well as all reactions of all their possible products with each other and with the starting substances.   

   - calculation of atomic composition and molar mass.   

   I plan to add here features for solving: ‘0-dimensional’ problem: evolution of concentrations of substances in time - pure chemical kinetics, the problem of equilibrium composition (I have the corresponding code on Julia which should be translated), solving one-dimensional stationary and non-stationary ODE (and PDE ) problems with initial and boundary conditions... The project is in its early stages so don't be too harsh).   Since the existing crates were rather poorly suited for solving stiff systems of heat-mass transfer with reactions, I was forced to write my own, called RustedSciThe (   https://crates.io/crates/RustedSciThe      or    https://github.com/Gleb-Zaslavsky/RustedSciThe    ):   

    - since the computation of numerical Jacobians for such systems is complex and slow, I wrote a computer algebra system that works with symbolic (analytic) functions and, among other things, constructs a symbolic (partial) derivative, symbolic Jacobian, which is then turned into a function.   

   - a number of methods for solving stiff problems with initial conditions are implemented.   

   - for solving 1D problems with boundary conditions I was inspired by the well-known Cantera package, but I have significantly improved the algorithm used there (dense and sparse matrices to choose from, analytic Jacobian, several strategies for building adaptive meshes, several crates for selecting solvers of linear systems, etc.)   

   - some other useful features like parsing strings to symbolic expressions, solving nonlinear algebraic system, etc..   

   I am convinced that Rust has a great future in scientific computing and I invite everyone who is interested to join or express wishes on the development of this project!   

       

       
   

======>
https://old.reddit.com/r/rust/comments/1gzg6gv/how_about_minecraft_rebuilt_in_rust/
-->>-->>
Would you like to see exact minecraft java edition only ported to rust with better speed and support for shaders. I was just messing around in bevy games docs and while it's not that simple it feels it can be accomplished with some external help. If you would like to join me in this project you can dm me. We can collaborate and make minecraft rust edition. Although even if you don't want to collaborate just an opinion on this idea would be nice.   
   

======>
https://www.ralfj.de/blog/2024/11/25/rustlantis.html
-->>-->>
ralfj.de Projects Blog RSS Feed Nov 25, 2024


  • Research , Rust • Edits • Permalink Rustlantis: Randomized Differential Testing of the Rust Compiler The first paper produced entirely by my group has recently been published at OOPSLA. :)
The paper is about fuzzing the optimizations and code generation of the Rust compiler by randomly generating MIR programs and ensuring they behave the same across different backends, different optimization levels, and in Miri.
The core part of this work was done by Andy (Qian Wang) for his master thesis .
This was already a strong thesis, but Andy kept working on this even after he started having a regular dayjob, and we ended up with a very nice paper.
In total, he found 22 new bugs in the Rust compiler, 12 of them in the LLVM backend that has already been extensively fuzzed by prior work. To learn more, check out the paper or watch Andy’s talk (the timestamp link seems unreliable, seek to the 5h40min mark if it doesn’t do that automatically). Posted on Ralf's Ramblings on Nov 25, 2024 . Comments? Drop me a mail !
======>
https://docs.rs/imply-hack
-->>-->>
Crate imply_hack Copy item path Settings Help Summary Source Expand description Add implied bounds to your traits by adding Imply as a super trait: trait Bound {} trait MyTrait<T>: Imply<T, Is: Bound> {} // Implies T: Bound Works with Rust 1.79+. For more information, see Why and How . § The problem If you’re the type of person to get lost deep into generic code, you might have run into something
like this: trait MyTrait<T> { fn do_the_thing(value: & T);
} struct Foo; struct MyFooUser; impl MyFooUser { fn use_value<T>( & self , value: & T) where Foo: MyTrait<T>
    {
        Foo.do_the_thing(value)
    }
} fn run<T>(value: & T, user: MyFooUser) where Foo: MyTrait<T>,
{
    MyFooUser.use_value( & value);

    Foo.do_the_thing( & value); // Do it again! } Now, this is all well and good. But suppose we now want to make run generic over any FooUser . trait FooUser<T>
{ fn use_value( & self , value: & T);
} impl <T> FooUser<T> for MyFooUser where Foo: MyTrait<T>
{ fn use_value( & self , value: & T) { /* ... */ }
} Now, suppose that FooUser<T> only really makes sense when Foo: MyTrait<T> and, notice that we
use Foo: MyTrait<T> both in run and in the implementation of FooUser<T> . We’re violating one of the most important rules of software development: Dont Repeat Yourself! Note: It might not seem that big of a deal in this example, but imagine that what I’m representing
here as a simple bound is, in fact, decidedly not simple. And that run is not just a
function, but a trait implementation. One of many. fn run<T, U>(value: T, user: U) where U: FooUser<T>,
    Foo: MyTrait<T>, // We really want to get rid of this. {
    user.use_value( & value)
    Foo.do_the_thing( & value);
} If you’ve run into similar situations before, you might be tempted to do: trait FooUser<T> where Foo: MyTrait<T>
{ /* ... */ } fn run<T, U>(value: T, user: U) where U: FooUser<T>,
{ /* ... */ } But this does not quite work… error[E0277]: the trait bound `Foo: MyTrait<T>` is not satisfied
  --> src/lib.rs:31:8
   |
31 |     U: FooUser<T>,
   |        ^^^^^^^^^^ the trait `MyTrait<T>` is not implemented for `Foo`
   |
note: required by a bound in `FooUser`
  --> src/lib.rs:26:10
   |
24 | trait FooUser<T>
   |       ------- required by a bound in this trait
25 | where
26 |     Foo: MyTrait<T>
   |          ^^^^^^^^^^ required by this bound in `FooUser` Congratulations! You just stumbled into RFC 2089: Extended Implied
bounds § Possible solutions Wait for stabilization (See you in 10 years). Grab a copy of rustc, zulip and get coding! Bite the bullet, and start typing bounds. Rework the entire architecture. None of these is particularly appealing if you don’t want to start a very big side quest. Or… Use this crate. § How it works Suppose we want MyTrait to imply T: Bound . Rust 1.79 stabilized implied
bounds on super traits and, notably, associated bounds of super traits. We can use this by creating a supertrait for MyTrait , and then constraining an associated type on
that super trait (which we set equal to T ), such that it satisfies Bound . This looks like this: trait Imply { type Is;
} trait MyTrait<T> where Self : Imply<Is: Bound>, Self : Imply<Is = T>,
{} This is still a bit annoying to use. Refining the design a bit we get: trait Imply<T>: ImplyInner<T, Is = T> {} trait ImplyInner<T> { type Is;
} trait MyTrait<T>: Imply<T, Is: Bound> {} Then, add a few blanket impls and we have imply_hack ! Traits § Imply Creates an implied bound when applied as a supertrait.
======>
https://blog.lucasholten.com/rust-query-announcement/
-->>-->>
2024-11-24 Announcing rust-query database rust Safe relational database queries using the Rust type system Do you want to persist your data safely without migration issues and easily write complicated queries? All of this without writing a single line of SQL? If so, then I am making rust-query for you! This is my first blog post about rust-query , a project I've been working on for many months. I hope you like it! Rust and Databases There is only one reason why I made this library and it is because I don't like the current options for interacting with a database from Rust. The existing libraries don't provide the compile time guarantees that I want and are verbose or awkward like SQL. The reason I care so much is that databases are really cool. They solve a huge problem of making crash-resistant software with support for atomic transactions. Structured Query Language (SQL) is a protocol For those who don't know, SQL is the standard when it comes to interacting with databases. So much so that almost all databases only accept queries in some dialect of SQL. My opinion is that SQL should be for computers to write. This would put it firmly in the same category as LLVM IR. The fact that it is human-readable is useful for debugging and testing, but I don't think it's how you want to write queries. Introducing rust-query rust-query is my answer to relational database queries in Rust. It's an opinionated library that deeply integrates with Rust's type system to make database operations feel Rust-native. Key Features and Design Decisions I could write a blog post about each one of these, but let's keep it short for now: Explicit table aliasing : Joining a table gives back a dummy representing that table let user = User::join(rows); . Null safety : Optional values in queries have Option type, requiring special care to handle. Intuitive aggregates : Our aggregates are guaranteed to give a single result for every row they're joined on. After trying it, you'll see this is much more intuitive than traditional GROUP BY operations. Type-safe foreign key navigation : Database constraints are like type signatures, so you can rely on them for your queries with easy-to-use implicit joins by foreign key (e.g., track.album().artist().name() ). Type-safe unique lookups : For example, you can get an Option<Rating> dummy with Rating::unique(my_user, my_story) . Multi-versioned schema : It's declarative and you can see the differences between all past versions of the schema at once! Type-safe migrations : Migrations have all the power of queries and can use arbitrary Rust code to process rows. Ever had to consult something outside the database for use in a migration? Now you can! Type-safe unique conflicts : Inserting and updating rows in tables with unique constraints results in specialized error types. Row references tied to transaction lifetime : Row references can only be used while the row is guaranteed to exist. Encapsulated typed row IDs : The actual row numbers are never exposed from the library API. Application logic should not need to know about them. Let's see it! You always start by defining a schema. With rust-query it's easy to migrate to a different schema later. #[ schema ] enum Schema { User { name: String , }, Story { author: User, title: String , content: String }, #[ unique (user, story)] Rating { user: User, story: Story, stars: i64 }, } use v0:: * ; Schema defintions in rust-query use enum syntax, but no actual enum is defined here.
This schema defines three tables with specified columns and relationships: Using another table name as a column type creates a foreign key constraint. The #[unique] attribute creates named unique constraints. The #[schema] macro parses the enum syntax and generates a module v0 that contains the database API. Writing Queries First, let's see how to insert some data into our schema: fn insert_data (txn: & mut TransactionMut<Schema>) { // Insert users let alice = txn. insert (User { name: " alice ", }); let bob = txn. insert (User { name: " bob ", }); // Insert a story let dream = txn. insert (Story { author: alice, title: " My crazy dream ", content: " A dinosaur and a bird... ", }); // Insert a rating - note the try_insert due to the unique constraint let rating = txn. try_insert (Rating { user: bob, story: dream, stars: 5 , }). expect (" no rating for this user and story exists yet "); } A few important points about insertions: We need a mutable transaction ( TransactionMut ) to modify the database. Insert operations return references to the newly inserted rows. When inserting into tables with unique constraints, use try_insert to handle potential conflicts. The error type of try_insert is based on how many unique constraints the table has. Now let's query this data: fn query_data (txn: & Transaction<Schema>) { let results = txn. query (|rows| { let story = Story::join(rows); let avg_rating = aggregate (|rows| { let rating = Rating::join(rows); rows. filter_on (rating. story (), & story); rows. avg (rating. stars (). as_float ()) }); rows. into_vec ((story. title (), avg_rating)) }); for (title, avg_rating) in results { println!(" story ' {title} ' has avg rating {avg_rating:?} "); } } Key points about queries: rows represents the current set of rows in the query. Joins can add rows and filters can remove rows. By joining a table like Story , the rows set is mutated to be the Cartesian product of itself and the rows from the joined table. The query above only has a single join , so we know it will give exactly one result for each row in the Story table. Using aggregate to calculate an aggregate, does not change the number of rows in the query. rows.filter_on can be used to filter rows in the aggregate to match a value from the outer scope. The rows.avg method returns the average of the rows in the aggregate, if there are no rows then the average will evaluate to None . Results can be collected into vectors of tuples or structs. Schema Evolution and Migrations Let's say you want to add an email address to each user. Here's how you'd create the new schema version: #[ schema ] #[ version (0.. = 1)] enum Schema { User { name: String , #[ version (1..)] email: String , }, // ... rest of schema ... } use v1:: * ; And here's how you'd migrate the data: let m = m. migrate (v1::update::Schema { user: Box ::new(|old_user| { Alter::new(v1::update::UserMigration { email: old_user . name () . map_dummy (|name| format!(" {name} @example.com ")), }) }), }); The v1::update module contains structs defining the difference between schema v0 and schema v1 . We use these structs to implement the migration. This way the migration is type checked against both the old and new schemas. Note that inside migrations we can execute all the single-row queries we want: aggregates, unique constraint lookups etc.! We can also use map_dummy with arbitrary Rust to process rows further. Conclusion rust-query represents a fresh approach to database interactions in Rust, prioritizing: Checking everything possible at compile time. Making it possible to compose queries with each other and arbitrary Rust. Enabling schema evolution with type-checked migrations. While still in development, the library already allows building experimental database-backed applications in Rust. I encourage you to try it out and provide feedback through GitHub issues! The library currently uses SQLite as its only backend, chosen for its embedded nature. This will not change anytime soon, as one backend is most practical while rust-query is in development. share
======>
https://old.reddit.com/r/rust/comments/1gzx6kf/spatial_led_020_a_crate_for_creating_super_cool/
-->>-->>
Two weeks ago, I    shared    my recently-released crate that allows you to manage LED strip lights in a totally different way. The TL;DR is: rather than setting LED colors by index, you can map out the shape of your room in 2D space and then query LEDs by direction, distance, etc.   

   I got a lot of great feedback (and motivation) from that first post. Most notably, I had someone reach out to me and submit a PR to add    no_std    support to the crate, which is going to be huge for making    spatial_led    viable for more users. This version introduces that support, plus a handful of minor tweaks. Big thanks to    u/claudiomattera    for making this possible!   

   I want to get this library out there in the hands of those who will have the most fun with it as I think they'll have the best feedback for me, but I'm not totally sure how to find them. If any of you know of some online communities that would appreciate this project, I'd love to hear about them.   

   Crate:    https://crates.io/crates/spatial_led       

   Changelog:    https://github.com/DavJCosby/sled/releases/tag/0.2.0   
   

======>
https://rust-gpu.github.io/blog/optimizing-matmul
-->>-->>
Recent posts 2024 Optimizing a Rust GPU matmul kernel Welcoming two new Rust GPU maintainers Rust GPU Transitions to Community Ownership Optimizing a Rust GPU matmul kernel November 25, 2024 · 18 min read Christian Legnitto Rust GPU maintainer I read the excellent post Optimizing a WebGPU Matmul Kernel for 1TFLOP+
Performance by Zach Nussbaum and thought it might be fun to reimplement
it with Rust GPU . We'll follow Zach's original post closely, comparing and contrasting using Rust vs the
WGSL and Typescript from his post. At the end, I'll show some unique benefits of using Rust on the GPU. A big thank you to Zach for allowing me to reimplement
his blog post! tip The complete runnable code can be found on
GitHub . What is Rust GPU? ​ Rust GPU is a project that allows you to write code for
GPUs using the Rust programming language. GPUs are typically programmed using
specialized languages like WGSL , GLSL , MSL ,
or HLSL .
Rust GPU changes this by letting you use Rust to write GPU programs (often called
"shaders" or "kernels"). These Rust GPU programs are then compiled into SPIR-V ,
a low-level format that most GPUs understand . Since
SPIR-V is the format Vulkan uses, Rust GPU makes it possible
to integrate Rust-based GPU programs into any Vulkan-compatible workflow 1 . For more details, check out the Rust GPU website or the GitHub repository . How does Rust GPU work? ​ Rust GPU focuses purely on compiling your Rust code into SPIR-V. This compiled code is
what the GPU executes. However, Rust GPU doesn't dictate how you handle CPU-to-GPU
communication or data transfer. You're free to choose a host CPU library written in
whatever language that fits your project. Some popular options in Rust include: ash : Low-level Vulkan bindings for Rust,
providing maximum control over Vulkan operations. vulkano : A higher-level Vulkan library
that simplifies common tasks. wgpu : A cross-platform library that abstracts
GPU operations across Vulkan, DirectX, Metal, and WebGPU. But again, you don't have to use Rust for the CPU-side when using Rust on the GPU—any
language will do. What will we use? ​ In Zach's post, he writes his GPU programs in WGSL . These
programs and their data are sent to and from the GPU via Typescript which talks to the WebGPU CPU code built into the browser. We'll take a different approach: writing GPU programs in Rust via Rust GPU and managing
everything—including the CPU-side code—in Rust. This means both the GPU programs and the
code controlling them will be written in the same language. If you are familiar with web
programming, what we are doing is conceptually similar to Javascript running on both the
server and the client. Using Rust for both CPU and GPU has advantages, like consistent tooling and shared code.
But it also means we need to be clear about which code runs where. I've tried to make
sure this distinction is easy to follow. To handle communication between our code on the CPU and GPU, we'll use wgpu . wgpu is a high-level Rust library that
implements the WebGPU API. On the web, it works directly with the browser's WebGPU
implementation. On native platforms, it translates API calls to the platform's GPU API
(Vulkan, DirectX, or Metal). This lets us run the same code on a wide range of
platforms, including Windows, Linux, macOS 2 , iOS 3 , Android, and the web 4 . By using Rust GPU and wgpu , we have a clean, portable setup with everything written in
Rust. GPU program basics ​ The smallest unit of execution is a thread, which executes the GPU program. Workgroups are groups of threads: they are grouped together and run in parallel (they’re
called thread blocks in
CUDA ). They can access
the same shared memory. We can dispatch many of these workgroups at once. CUDA calls this a grid (which is made
of thread blocks). Workgroups and dispatching workgroups are defined in 3D. The size of a workgroup is
defined by compute(threads((x, y, z))) where the number of threads per workgroup is
x * y * z. Writing the kernel ​ Kernel 1: Naive kernel ​ The simplest way to compute a dot product between matrix A and B and write
to matrix C is for each row in A (of shape M), iterate over the columns of A (of shape
K) and multiply by the corresponding value of B. Here, we have our first difference from Zach's post. In WGSL, you must define inputs at
the top-level scope: WGSL struct Dimensions { M : u32 , K : u32 , N : u32 , } @ group ( 0 ) @ binding ( 0 ) var < uniform > dimensions : Dimensions ; @ group ( 0 ) @ binding ( 1 ) var < storage , read > a : array < f32 > ; @ group ( 0 ) @ binding ( 2 ) var < storage , read > b : array < f32 > ; @ group ( 0 ) @ binding ( 3 ) var < storage , read_write > result : array < f32 > ; And then write your kernel: WGSL @ compute @ workgroup_size ( 1 ) fn main ( @ builtin ( global_invocation_id ) global_id : vec3 < u32 > ) { let index = global_id . x ; let row = index / dimensions . N ; let col = index % dimensions . N ; if ( index < dimensions . M * dimensions . N ) { var sum = 0.0 ; for ( var i : u32 = 0u ; i < dimensions . K ; i = i + 1u ) { sum = sum + a [ row * dimensions . K + i ] * b [ i * dimensions . N + col ] ; } result [ row * dimensions . N + col ] = sum ; } } With Rust GPU, we specify the inputs as arguments to the kernel and configure them with procedural macros : Naive kernel with Rust GPU #![no_std] use settings :: Dimensions ; use spirv_std :: glam :: UVec3 ; use spirv_std :: spirv ; #[spirv(compute(threads(1)))] pub fn matmul ( #[spirv(global_invocation_id)] global_id : UVec3 , #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] a : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 2)] b : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 3)] result : & mut [ f32 ] , ) { let index = global_id . x ; let row = index / dimensions . n ; let col = index % dimensions . n ; if index < dimensions . m * dimensions . n { let mut sum = 0.0 ; for i in 0 .. dimensions . k { let a_val = a [ ( row * dimensions . k + i ) as usize ] ; let b_val = b [ ( i * dimensions . n + col ) as usize ] ; sum += a_val * b_val ; } result [ ( row * dimensions . n + col ) as usize ] = sum ; } } This code looks like normal Rust code but runs entirely on the GPU. There are a couple of things to note about the Rust implementation: The kernel uses the regular Rust #![no_std] attribute, which is required because GPUs do not have access to Rust's standard
library ( std ). Instead, you rely on core and spirv_std to provide std -like
functionality. Libraries are imported via use . The module system works exactly the same as regular Rust. We're importing a vendored copy of glam .
This is the exact glam crate from crates.io . The inner loop ( for i in 0..dimensions.k ) uses Rust's for syntax with a range.
This is a higher-level abstraction compared to manually iterating with an index in
other shader languages like WGSL, GLSL, or HLSL. Read-only inputs are immutable references ( &Dimensions / &[f32] ) and writable
outputs are mutable references ( &mut [f32] ). This feels very familiar to anyone
used to writing Rust. What's with all the usize ? ​ Rust defines usize as the native pointer width of the hardware the code is running
on . This is important because Rust
uses usize for indexing slices to ensure that access is properly pointer-aligned. On most GPU hardware, usize is effectively equivalent to u32 . But the Rust compiler
doesn't assume that. It can't, because doing so could introduce problems—like if you ran
this code on hardware where usize is actually u64 . Rust won't let you implicitly
treat a u32 as a usize . You have to explicitly cast it, essentially telling the
compiler "I know this is safe for my target hardware." This explicitness might seem tedious but it is one of the ways Rust prevents subtle
bugs. It forces you to think about whether your assumptions about hardware alignment and
pointer sizes are correct, making your code more portable and reliable. info Matrix multiplication is a pathological case with lots of indexing and row and column
calculations. Most Rust GPU code does not have nearly as many usize casts as these
examples. Dispatching workgroups ​ Each workgroup, since it's only one thread ( #[spirv(compute(threads(1)))] ), processes
one result[i, j] . To calculate the full matrix, we need to launch as many entries as there are in the m * n matrix. Here we specify that ( Uvec3::new(m * n, 1, 1 ) on the CPU: Calculating on the CPU how many workgroup dispatches are needed impl GridComputation for Naive { fn workgroup ( & self ) -> UVec3 { UVec3 :: new ( 1 , 1 , 1 ) } fn dispatch_count ( & self , m : u32 , n : u32 ) -> UVec3 { UVec3 :: new ( m * n , 1 , 1 ) } } The dispatch_count() function runs on the CPU and is used by the CPU-to-GPU API (in
our case wgpu ) to configure and dispatch work to the GPU: Using wgpu on the CPU to dispatch workgroups to the GPU let dispatch_count = < T as GridComputation > :: dispatch_count ( & self . variant , m , n ) ; ... compute_pass . dispatch_workgroups ( dispatch_count . x , dispatch_count . y , dispatch_count . z ) ; warning This code appears more complicated than it needs to be. I abstracted the CPU-side code
that talks to the GPU using generics and traits so I could easily slot in different
kernels and their settings while writing the blog post. You could just hardcode the value for simplicity. Kernel 2: Moarrr threads! ​ With the first kernel, we're only able to compute small square matrices due to limits on
the number of workgroups you can dispatch at once. Since we're launching one workgroup per entry, a 256x256 matrix is larger than our
limit! Remember this line? #[spirv(compute(threads(1)))] We can reduce the number of dispatched workgroups by increasing the number of threads per workgroup! If we update our GPU code #[spirv(compute(threads(256)))] we can reduce the number of total dispatched workgroups per dimension: Calculating how many workgroup dispatches are needed on the CPU impl GridComputation for Workgroup256 { fn workgroup ( & self ) -> UVec3 { UVec3 :: new ( 256 , 1 , 1 ) } fn dispatch_count ( & self , m : u32 , n : u32 ) -> UVec3 { let workgroup = self . workgroup ( ) ; let threads_needed = m * n ; // This ceil division is needed because Rust handles truncation differently than // Typescript/Javascript so we might get 0. // We'll also cap the value to a maximum of 65,535 to comply with hardware limits. let x = ( ( threads_needed as f32 / workgroup . x as f32 ) . ceil ( ) as u32 ) . min ( 65_535 ) ; UVec3 :: new ( x , 1 , 1 ) } } With these two small changes we can handle larger matrices without hitting hardware
workgroup limits. Kernel 3: Calculating with 2D workgroups ​ However, doing all the computation in "1 dimension" still limits the matrix size we can
calculate. Although we don't change much about our code, if we distribute our work in 2 dimensions
we're able to bypass these limits and launch more workgroups that are larger. This
allows us to calculate a 4096x4096 matmul. We update our compute(threads(256))) to compute(threads((16, 16))) , and make the small
change to row and col from Zach's post to increase speed: 2D workgroup kernel with Rust GPU #![no_std] use settings :: Dimensions ; use spirv_std :: glam :: UVec3 ; use spirv_std :: spirv ; #[spirv(compute(threads(16, 16)))] pub fn matmul ( #[spirv(global_invocation_id)] global_id : UVec3 , #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] a : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 2)] b : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 3)] result : & mut [ f32 ] , ) { let row = global_id . x as usize ; let col = global_id . y as usize ; if row < dimensions . m as usize && col < dimensions . n as usize { let mut sum = 0.0 ; for i in 0 .. dimensions . k as usize { sum += a [ row * dimensions . k as usize + i ] * b [ i * dimensions . n as usize + col ] ; } result [ row * dimensions . n as usize + col ] = sum ; } } And we need to tweak the workgroup dispatch count calculation on the CPU as we are in 2D
now and using the y value: Calculating how many workgroup dispatches are needed on the CPU impl GridComputation for Workgroup2d { fn workgroup ( & self ) -> UVec3 { UVec3 :: new ( 16 , 16 , 1 ) } fn dispatch_count ( & self , m : u32 , n : u32 ) -> UVec3 { let w = self . workgroup ( ) ; let workgroup_size = w . x + w . y ; let x = ( ( m as f32 ) / ( workgroup_size as f32 ) ) . ceil ( ) as u32 ; let y = ( ( n as f32 ) / ( workgroup_size as f32 ) ) . ceil ( ) as u32 ; UVec3 :: new ( x , y , 1 ) } } Kernel 4: Kernel tiling ​ Another thing to consider is how much work each thread does. Up to now, each thread only computes one entry. But there is some overhead to launching
each workgroup versus computing more than 1 element per thread! If calculating more elements per thread is faster than the overhead to launch each
workgroup, we should see a big speedup. To do so, we calculate 4 results per thread (e.g. a 1x4 Tile). Tiling kernel with Rust GPU #![no_std] use settings :: Dimensions ; use settings :: TILE_SIZE ; use spirv_std :: glam :: UVec3 ; use spirv_std :: spirv ; #[spirv(compute(threads(16, 16)))] pub fn matmul ( #[spirv(global_invocation_id)] global_id : UVec3 , #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] a : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 2)] b : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 3)] result : & mut [ f32 ] , ) { let row = global_id . y as usize ; let col = ( global_id . x * TILE_SIZE ) as usize ; if row >= dimensions . m as usize || col >= dimensions . n as usize { return ; } let mut sum00 : f32 = 0.0 ; let mut sum01 : f32 = 0.0 ; let mut sum02 : f32 = 0.0 ; let mut sum03 : f32 = 0.0 ; for i in 0 .. dimensions . k as usize { let a_elem = a [ row * dimensions . k as usize + i ] ; if col < dimensions . n as usize { sum00 += a_elem * b [ i * dimensions . n as usize + col ] ; } if col + 1 < dimensions . n as usize { sum01 += a_elem * b [ i * dimensions . n as usize + col + 1 ] ; } if col + 2 < dimensions . n as usize { sum02 += a_elem * b [ i * dimensions . n as usize + col + 2 ] ; } if col + 3 < dimensions . n as usize { sum03 += a_elem * b [ i * dimensions . n as usize + col + 3 ] ; } } if col < dimensions . n as usize { result [ row * dimensions . n as usize + col ] = sum00 ; } if col + 1 < dimensions . n as usize { result [ row * dimensions . n as usize + col + 1 ] = sum01 ; } if col + 2 < dimensions . n as usize { result [ row * dimensions . n as usize + col + 2 ] = sum02 ; } if col + 3 < dimensions . n as usize { result [ row * dimensions . n as usize + col + 3 ] = sum03 ; } } The kernel looks roughly the same as before except we've unrolled the computation and
are calculating TILE_SIZE results per thread. We also need some error checking for
when our matrices don't fit nicely. But this code is kinda gross...it looks like the opaque GPU code we are used to. Let's
make it nice! Tiling kernel using loops with Rust GPU #![no_std] use settings :: Dimensions ; use settings :: TILE_SIZE ; use spirv_std :: glam :: UVec3 ; use spirv_std :: spirv ; #[spirv(compute(threads(16, 16)))] pub fn matmul ( #[spirv(global_invocation_id)] global_id : UVec3 , #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] a : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 2)] b : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 3)] result : & mut [ f32 ] , ) { let row = global_id . y as usize ; let col = ( global_id . x * TILE_SIZE ) as usize ; if row >= dimensions . m as usize || col >= dimensions . n as usize { return ; } // Compute sums for each offset directly let mut sums = [ 0.0 ; TILE_SIZE as usize ] ; for i in 0 .. dimensions . k as usize { let a_elem = a [ row * dimensions . k as usize + i ] ; for offset in 0 .. TILE_SIZE as usize { if col + offset < dimensions . n as usize { let b_elem = b [ i * dimensions . n as usize + col + offset ] ; sums [ offset ] += a_elem * b_elem ; } } } // Write results back for offset in 0 .. TILE_SIZE as usize { if col + offset < dimensions . n as usize { result [ row * dimensions . n as usize + col + offset ] = sums [ offset ] ; } } } Much better. We can take this a step further and calculate 2D results per thread! Instead of
calculating 4 elements per single row, we can calculate 4 elements for 4 rows (e.g. a 2D
tile). 2D tiling kernel with Rust GPU #![no_std] use settings :: Dimensions ; use settings :: { TILE_M , TILE_N } ; use spirv_std :: glam :: UVec3 ; use spirv_std :: spirv ; #[spirv(compute(threads(16, 16)))] pub fn matmul ( #[spirv(global_invocation_id)] global_id : UVec3 , #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , #[spirv(storage_buffer, descriptor_set = 0, binding = 1)] a : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 2)] b : & [ f32 ] , #[spirv(storage_buffer, descriptor_set = 0, binding = 3)] result : & mut [ f32 ] , ) { let row = ( global_id . y * TILE_M ) as usize ; let col = ( global_id . x * TILE_N ) as usize ; // Initialize sums array to zeros // Note: This is uglier than it needs to be to work around // https://github.com/Rust-GPU/rust-gpu/issues/46 let mut sums : [ [ f32 ; TILE_N as usize ] ; TILE_M as usize ] = Default :: default ( ) ; // Compute the 2D tile for k in 0 .. dimensions . k as usize { for i in 0 .. TILE_M as usize { let a_element = if row + i < dimensions . m as usize { a [ ( row + i ) * dimensions . k as usize + k ] } else { 0.0 } ; for j in 0 .. TILE_N as usize { let b_element = if col + j < dimensions . n as usize { b [ k * dimensions . n as usize + ( col + j ) ] } else { 0.0 } ; sums [ i ] [ j ] += a_element * b_element ; } } } // Write results for i in 0 .. TILE_M as usize { for j in 0 .. TILE_N as usize { let output_row = row + i ; let output_col = col + j ; if output_row < dimensions . m as usize && output_col < dimensions . n as usize { result [ output_row * dimensions . n as usize + output_col ] = sums [ i ] [ j ] ; } } } } Each thread now calculates a 4x4 grid of the output matrix and we see a slight
improvement over the last kernel. To stay true to the spirit of Zach's original blog post, we'll wrap things up here and
leave the "fancier" experiments for another time. A note on performance ​ I didn't include performance numbers as I have a different machine than Zach. The
complete runnable code can be found on
GitHub and you can run the benchmarks yourself with cargo bench . tip You can also check out real-world projects using Rust GPU such as autograph and renderling . Reflections on porting to Rust GPU ​ Porting to Rust GPU went quickly, as the kernels Zach used were fairly simple. Most of
my time was spent with concerns that were not specifically about writing GPU code. For
example, deciding how much to abstract vs how much to make the code easy to follow, if
everything should be available at runtime or if each kernel should be a compilation
target, etc. The
code is not great as it is still blog post code! My background is not in GPU programming, but I do have Rust experience. I joined the
Rust GPU project because I tried to use standard GPU languages and knew there must be a
better way. Writing these GPU kernels felt like writing any other Rust code (other than debugging,
more on that later) which is a huge win to me. Not just the language itself, but the
entire development experience. Rust-specific party tricks ​ Rust lets us write code for both the CPU and GPU in ways that are often impossible—or at
least less elegant—with other languages. I'm going to highlight some benefits I
experienced while working on this blog post. Shared code across GPU and CPU ​ In GPU programming, we often need to pass data between the CPU and GPU. For example, our
GPU kernel expects a Dimensions struct as input: use settings :: Dimensions ; ... pub fn matmul ( ... #[spirv(uniform, descriptor_set = 0, binding = 0)] dimensions : & Dimensions , We create an instance of Dimensions on the CPU and send it to the GPU via wgpu ,
where the Rust kernel loads and uses it. Creating the Dimensions struct on the CPU and writing it to the GPU // This is a `uniform` buffer instead of `storage` buffer because the data is // the same for all workgroups, it is read-only, and it is small enough to fit // in a single buffer (`uniform` buffers are limited to 64 KB on most GPUs // and often less on older GPUs). let dimensions = Dimensions :: new ( m , k , n ) ; let dimensions_buffer = create_buffer_init ( & self . device , "Dimensions Buffer" , & [ dimensions ] , wgpu :: BufferUsages :: UNIFORM | wgpu :: BufferUsages :: COPY_DST , ) ; This means the code on the CPU and GPU need to agree on the definition of Dimensions ! In many GPU programming ecosystems, this would involve manually keeping the
definitions in sync across different languages—one for the CPU, one for the GPU. This is
tedious and error-prone. With Rust, it's straightforward: we move the Dimensions struct into its own crate, and
both the CPU and GPU code depend on that crate. Now, the type definition lives in one
place and both platforms use it directly. This approach eliminates duplication and guarantees consistency. If we need to make
changes, those changes propagate to both the CPU and GPU automatically, reducing the
risk of mismatches and making refactoring far safer. This kind of consistency across CPU and GPU is something you don't often see in other
GPU programming ecosystems. Bespoke codegen solutions are often created to accomplish
the same thing Rust has built in. Running and debugging shaders on the CPU ​ GPU code can be notoriously hard to debug. While developing this kernel, I ran into a
bug I couldn't figure out. GPU debugging tools are limited and printf -style debugging
often isn't available. But what if we could run the GPU kernel on the CPU , where we
have access to tools like standard debuggers and good ol' printf / println ? With Rust GPU, this was straightforward. By using standard Rust cfg() directives I
made the GPU-specific annotations ( #[spirv(...)] ) disappear when compiling for the
CPU. The result? The kernel became a regular Rust function. On the GPU, it behaves like
a shader. On the CPU, it's just a function you can call directly. Here's what it looks like in practice using the 2D tiling kernel from before: //! This shader can run on both the CPU and the GPU. //! //! The GPU-specific attributes are only used when compiling for the GPU, otherwise they //! are stripped away and the shader entrypoint becomes a normal function that can be //! called from the CPU. #![no_std] use settings :: Dimensions ; use settings :: { TILE_M , TILE_N } ; #[cfg(target_arch = "spirv" )] use spirv_std :: spirv ; #[cfg(target_arch = "spirv" )] use spirv_std :: glam ; #[cfg(not(target_arch = "spirv" ))] use glam ; use glam :: UVec3 ; #[cfg_attr(target_arch = "spirv" , spirv(compute(threads(16, 16))))] pub fn matmul ( #[cfg_attr(target_arch = "spirv" , spirv(global_invocation_id))] global_id : UVec3 , #[cfg_attr(target_arch = "spirv" , spirv(uniform, descriptor_set = 0, binding = 0))] dimensions : & Dimensions , #[cfg_attr( target_arch = "spirv" , spirv(storage_buffer, descriptor_set = 0, binding = 1) )] a : & [ f32 ] , #[cfg_attr( target_arch = "spirv" , spirv(storage_buffer, descriptor_set = 0, binding = 2) )] b : & [ f32 ] , #[cfg_attr( target_arch = "spirv" , spirv(storage_buffer, descriptor_set = 0, binding = 3) )] result : & mut [ f32 ] , ) { let row = ( global_id . y * TILE_M as u32 ) as usize ; let col = ( global_id . x * TILE_N as u32 ) as usize ; // Initialize sums array to zeros let mut sums : [ [ f32 ; TILE_N as usize ] ; TILE_M as usize ] = Default :: default ( ) ; // Compute the 2D tile for k in 0 .. dimensions . k as usize { for i in 0 .. TILE_M as usize { let a_element = if row + i < dimensions . m as usize { a [ ( row + i ) * dimensions . k as usize + k ] } else { 0.0 } ; for j in 0 .. TILE_N as usize { let b_element = if col + j < dimensions . n as usize { b [ k * dimensions . n as usize + ( col + j as usize ) ] } else { 0.0 } ; sums [ i ] [ j ] += a_element * b_element ; } } } // Write results for i in 0 .. TILE_M as usize { for j in 0 .. TILE_N as usize { let output_row = row + i as usize ; let output_col = col + j as usize ; if output_row < dimensions . m as usize && output_col < dimensions . n as usize { result [ output_row * dimensions . n as usize + output_col ] = sums [ i ] [ j ] ; } } } } The logic in the kernel hasn't changed, it is exactly the same as the GPU-only code from
before. You'll also notice that on the GPU it uses glam from spirv_std but on the CPU it
uses glam from crates.io: #[cfg(target_arch = "spirv" )] use spirv_std :: glam ; #[cfg(not(target_arch = "spirv" ))] use glam ; This is enabled by the standard Rust ecosystem tooling
around dependencies: Cargo.toml # Dependencies when run on either the CPU or GPU [ dependencies ] settings = { path = "../../shared/settings" } # Dependencies when run on the CPU [ target . 'cfg ( not ( target_arch = "spirv" ) ) ' . dependencies ] glam . workspace = true # Dependencies when run on the GPU [ target . 'cfg ( target_arch = "spirv" ) ' . dependencies ] spirv - std . workspace = true Testing the kernel in isolation is useful, but it does not reflect how the GPU executes
it with multiple invocations across workgroups and dispatches. To test the kernel
end-to-end, I needed a test harness that simulated this behavior on the CPU. Building the harness was straightforward due to Rust. By enforcing the same invariants
as the GPU I could validate the kernel under the same conditions the GPU would run it: fn multiply ( & self , a : & [ f32 ] , b : & [ f32 ] , m : u32 , k : u32 , n : u32 , ) -> Result < Vec < f32 > , MatrixMultiplyError > { // Initialize the result vector with zeros as that is what the GPU does. let mut result = vec! [ 0.0 ; ( m * n ) as usize ] ; // Retrieve workgroup and dispatch configurations. These tell us how to iterate. let workgroup = < T as GridComputation > :: workgroup ( & self . variant ) ; let dispatch = < T as GridComputation > :: dispatch_count ( & self . variant , m , n ) ; // Define dimensions as (m, k, n) let dimensions = Dimensions :: new ( m , k , n ) ; // Iterate over the dispatch grid for gwx in 0 .. dispatch . x { for gwy in 0 .. dispatch . y { for wx in 0 .. workgroup . x { for wy in 0 .. workgroup . y { // Calculate global indices let x = gwx * workgroup . x + wx ; let y = gwy * workgroup . y + wy ; if x < m && y < n { // Define global id let global_id = UVec3 :: new ( x , y , 1 ) ; // Perform the matmul operation for element (x, y). NOTE: // This is the EXACT SAME CODE THAT RUNS ON THE GPU, RUNNING // ON THE CPU. This is the power of rust-gpu. < T as Cpu > :: call ( & self . variant , global_id , & dimensions , & a , & b , & mut result , ) ; } } } } } Ok ( result ) } warning Again, this code appears more complicated than it needs to be. I abstracted the CPU
testing harness code using generics and traits so I could easily slot in different
kernels and their settings while writing the blog post. You could just call the kernel function directly in nested loops. Tests ​ By moving the kernel code to the CPU, I could write tests that ran quickly and entirely
on the CPU. This eliminated the need to serialize tests and offload them to the GPU
(which is a shared and limited resource). This approach has several benefits. First, it significantly reduced the feedback loop
during development, allowing me to catch issues faster. Second, it ensured the tests
could be run in any environment where the Rust toolchain is available—no GPU required.
This is especiallly relevant in CI environments such as Github Actions that do not have
a GPU by default. For example, my test for a small matrix multiplication kernel running in the harness on
the CPU looked like this: #[test] fn test_single_threaded_matmul_2x1x1 ( ) { let m = 2 ; let k = 1 ; let n = 1 ; let a = vec! [ 1.0 , 2.0 ] ; let b = vec! [ 3.0 ] ; let expected = vec! [ 3.0 , 6.0 ] ; let variant = crate :: variants :: Isomorphic ; let matrix_multiplier = block_on ( SingleThreadedMatMul :: new ( variant ) ) . expect ( "Failed to create" ) ; let result = matrix_multiplier . multiply ( & a , & b , m , k , n ) . expect ( "Matrix multiplication failed" ) ; assert_eq! ( result , expected ) ; } Benchmarks ​ I wanted to run benchmarks similar to those in the original blog post. Because I was
using Rust, this was simple. I used criterion with cargo bench , just like any
other Rust project. This required no new tools or workflows. The tools I already knew worked seamlessly.
More importantly, this approach benefits anyone working on the project. Any Rust
engineer can run these benchmarks with no additional setup— cargo bench is a standard
part of the Rust ecosystem. Formatting ​ Rust GPU code is formatted with rustfmt , following the same standards as all Rust
code. This not only ensured my GPU code looked identical to my CPU code, it made my GPU
code consistent with the entire Rust ecosystem . Leveraging standard tools like rustfmt minimizes cognitive overhead and avoids the hassle of configuring third-party
formatters of varying quality. Lint ​ Linting GPU code in Rust works the same way as for CPU code. Running cargo clippy highlighted issues and enforced consistent code quality. Though I didn't have any,
custom lint configurations are applied to Rust GPU kernels as well. Lints ensure that
GPU code is held to the same high standards as the rest of the project. Documentation ​ Writing doc comments and running cargo doc generates documentation for GPU kernels,
exactly how it happens in regular Rust. While some ecosystems offer similar tools,
Rust's integration is built-in and works seamlessly for both CPU and GPU code. There's
no special setup required. But wait, there's more! ​ The kernel in Zach's blog post is intentionally simple. That makes it easy to follow,
but it also means the Rust code looks very similar to WGSL. While this is fine for an
introductory example, it doesn't demonstrate Rust's real strengths for GPU programming.
These strengths—reusing existing libraries, traits, enums, generics, and more—become much more important as projects
grow in complexity. Leverage the existing Rust ecosystem ​ Rust's no_std ecosystem offers a wide array of libraries that can be used in
environments without the standard library. Traditionally this has meant embedded
devices, but a lot of the same assumptions apply to GPUs! As a consequence, you can
reuse existing no_std & no alloc libraries from
crates.io in your GPU code without the
authors explicitly adding GPU support . This is uniquely enabled by Rust GPU's
implementation choices and Rust's registers . Sharing and reusing code
from the greater Rust ecosystem is a superpower when writing GPU programs that will
massively compound over time. Traits ​ Traits are one of Rust's most powerful tools and they work with Rust GPU. Traits let you
define zero-cost reusable type-safe behavior. For example, if you have multiple kernels
for different matrix multiplication strategies, you can define a MatrixMultiplication trait and implement it for each variation. This eliminates duplication and makes your
code easier to extend. Enums and zero-sized types ​ GPU code is notoriously hard to read, but Rust's enums and zero-sized types (ZSTs) can
make it much more understandable. Enums let you explicitly encode states or modes. For
example, you can define tiling strategies or precision levels using enums instead of
relying on constants or magic numbers. ZSTs take this further by encoding configurations directly into the type system. For
example, you could represent different kernel configurations as ZSTs. This approach
ensures invalid configurations are impossible, improving both readability and safety. Generics ​ Generics are another feature missing from this kernel but are a powerful tool in Rust
GPU. They allow you to write flexible kernels that work across different data types or
memory layouts. For instance, you can write a single function that supports both f32 and f64 without duplicating code, all while maintaining type safety and performance. Error handling with Result ​ Rust GPU also supports error handling using Result . Encoding errors in the type system
makes it clear where things can go wrong and forces you to handle those cases. This is
particularly useful for validating kernel inputs or handling the many edge cases in GPU
logic. Iterators ​ Rust's iterators don't appear in this kernel, but they're another way Rust GPU
simplifies complex logic. Instead of manual loops with indices, you can use iterators to
express your logic more clearly. Iterators reduce the chance of off-by-one errors and make the intent of the code much
clearer. Rust GPU's support for iterators is not complete but we are looking to improve it in the
future. Conditional compilation ​ While I briefly touched on it a couple of times, this kernel doesn't really show the
full power of conditional compilation. With #[cfg(...)] and cargo
"features" , you can adapt
kernels to different hardware or configurations without duplicating code. GPU languages
like WGSL or GLSL offer preprocessor directives, but these tools lack standardization
across projects. Rust GPU leverages the existing Cargo ecosystem, so conditional
compilation follows the same standards all Rust developers already know. Come join us! ​ Rust GPU only recently became a community managed
project . We're eager to add more users and contributors!
We will be working on revamping the onboarding and documentation soon. To follow along
or get involved, check out the rust-gpu repo on
GitHub . Footnotes ​ Why not CUDA? That is covered by Rust
CUDA , a related project that I am planning
on rebooting soon! ↩ Technically wgpu uses MoltenVK or
translates to Metal on macOS ↩ Technically wgpu uses MoltenVK or
translates to Metal on iOS ↩ Technically wgpu translates SPIR-V to GLSL (WebGL) or WGSL (WebGPU) via naga on the web ↩ Tags: demo code performance Edit this page Older post Welcoming two new Rust GPU maintainers What is Rust GPU? How does Rust GPU work? What will we use? GPU program basics Writing the kernel Kernel 1: Naive kernel Kernel 2: Moarrr threads! Kernel 3: Calculating with 2D workgroups Kernel 4: Kernel tiling A note on performance Reflections on porting to Rust GPU Rust-specific party tricks Shared code across GPU and CPU Running and debugging shaders on the CPU Tests Benchmarks Formatting Lint Documentation But wait, there's more! Leverage the existing Rust ecosystem Traits Enums and zero-sized types Generics Error handling with Result Iterators Conditional compilation Come join us!
======>
https://yle.fi/aihe/a/20-10008009
-->>-->>
Hashing passwords at 1500 requests per second and beyond Published 22.11.2024 09:16. Updated 22.11.2024 09:27. Jaakko Rinta-Filppula Share Copy link Email Whatsapp Facebook X Linkedin Big interactive live events can pose a lot of challenges for authentication and login services. Learn how the Yle ID team managed to improve their login performance with AWS Lambda and Rust. The Yle ID team here at Yle (the Finnish Broadcasting Company), which the author is part of, is responsible for building the account services used in all of Yle’s digital products and services. There are currently a little over three million registered Yle IDs. In this post we explore a problem we experienced during a big interactive live television event where the amount of password hashing operations brought down our authentication backend. We start off with a little background on password hashing and then go through what challenges it poses for our services and how we managed to overcome them by using AWS Lambda functions and Rust. Background First, a little primer on storing passwords and what challenges it causes. When a user registers an account with a service, their password is not stored in plaintext in the application's database. Instead, to protect against leaked passwords in the case of a data breach, we calculate a cryptographic hash from the password which is then stored with the rest of the account data. The algorithms used to calculate these hashes are designed so that given the resulting hash it is impossible to reverse the process and get back the original password. This way, even if someone were to get hold of the stored account data, they wouldn't be able to log in with the accounts. Another important property of the hash algorithms is that they need a lot of resources (processing power or memory or both). This is done on purpose to prevent brute-force attacks against the passwords: If the database with the password hashes is compromised, the attacker can try to calculate hashes for different passwords until they get one that matches one of the hashes in the database. Finding a matching hash would mean the password they guessed was correct. If the chosen algorithm is too fast or uses too little memory, it is possible to try a lot of different passwords in a very short time, increasing the chance of finding a match. The attacker can try to speed up the process by using lists of pre-calculated hashes of popular passwords, called rainbow tables. That type of attack is prevented by salting the password before the hashing operation but that is beyond the scope of this post. So, if we don't store the user's password, how can we log them in? Simple, when the user later gives their password when logging in, we calculate the same hash again and compare it against the one we have stored in the database to determine if the given password was correct. This means that we have to do the expensive hash calculation every time a user tries to log in. (And also when they reset their password or a new user registers an account.) The problem In a normal situation, hashing those passwords is not a problem since the volume of these requests is fairly low but in the case of big events there can be huge spikes in these high CPU usage operations (logging in, registration, password reset). For example, during a big live event, the host might encourage the viewers to log in to the Yle app to participate in it somehow (chat, vote, play a game etc.). That kind of prompt can be directly seen as a rapid spike in the amount of requests our backend receives (Figure 1). <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_1433/w_400/v1732193599/39-1383751673f2a1718d57.jpg" alt="Requests per second for endpoints that perform password hash calculations during a big interactive live event. It is clear from the graph when the audience was asked to participate in the broadcast."> Image caption Figure 1: Requests per second for endpoints that perform password hash calculations during a big interactive live event. It is clear from the graph when the audience was asked to participate in the broadcast. technology During one such live event, the bigger than expected spike in the request rate exhausted all the available CPUs in our cluster, grinding the backend to a halt. The issue was amplified by the fact that we had recently increased the resource requirements of our password hashing to match what is recommended for modern applications. The backend being busy calculating all those password hashes meant it couldn't respond to any other requests either. The obvious fix to this problem is to allocate even more resources to the application. However, that only works when we can anticipate the traffic spike. What about unexpected events such as big news events? Of course, we could always keep a lot of capacity ready in case something happens but that would be really wasteful and expensive. The solution So, to recap the problems with the current architecture were that: We have one operation that uses by far the most resources (CPU) in the application. That operation has the potential to completely cripple the whole application. The operation is mandatory and cannot be made less resource intensive without compromising on security. ¹ The load can vary a lot and be unpredictable. Our backend is built on Scala, and due to historical reasons, we were using a third-party Scala implementation of the password hashing algorithm. We ran some benchmarks comparing that implementation against Bouncy Castle and the built-in JDK implementation of the same algorithm but there were no meaningful differences in the resource consumption. We also benchmarked against a Rust version of the algorithm and found that to be more performant than the JVM implementations. We were then faced with the challenge of integrating the Rust component in our application and even though the Rust version was faster it alone would only move the problem further down the road. With growing user counts and bigger and bigger events just using a faster implementation in the same order of magnitude would only buy us some time but not solve the underlying problem. What we came up with after thinking about the problem was to isolate the hash calculation into its own microservice. We hypothesized that using a platform that automatically scales based on the request rate, such as AWS Lambda , would free us from having to worry about allocating the right amount of resources. And even though the hash calculation would still take relatively long compared to other operations, from the perspective of the main backend it would now be an HTTP request during which it would be free to serve other requests. We built a prototype implementation of this new microservice to test our assumptions. It was a really small and simple HTTP API that would receive the user's password, do the hash calculation and return the resulting hash. With this prototype in place we ran a performance test simulating the traffic during the event where the problems happened. The tests were run with the same amount of resources that were provisioned during the event. Although the tests used only login requests, they would still give us a good estimate of the performance, since the resource use of any other operations is dwarfed by the password hash calculation. Running the test with the event’s traffic amount (1500 req/s) posed no problems for the new implementation. We were able to ramp up the numbers to 3000 req/s before we started to see any errors in the response codes. The response times were really good with 90th percentile hovering around 300 milliseconds (Figure 2). (Remember that the hash calculation is intentionally slow and most of the time is spent in the actual algorithm.) And even with the overhead from the added HTTP request, the new version still manages slightly faster response times than the old one. <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_1576/w_400/v1732193294/39-1383759673f2b88ec645.jpg" alt="Performance test results with the new password hash microservice with 3000 req/s."> Image caption Figure 2: Performance test results with the new password hash microservice with 3000 req/s. technology Success! Squeezing out 100% more performance with the same infrastructure is just what we had hoped for. The errors we saw during the load testing were from other parts of the infrastructure and not the slowness of the password hashing algorithm. It is quite common that when you fix one bottleneck it reveals another somewhere else but that just shows how effective our solution is and we have since run even bigger load tests successfully. Now, to make the implementation production ready we added some additional parameters to the microservice that enable us to handle any future hash algorithm changes (and existing users still with older hash versions). We also made the main application fall back to calculating the hash itself in case the microservice wasn't available for some reasons, for example a network issue. The added benefit of this change is that we could now reduce our CPU allocation for the main application which, even accounting for the cost of the Lambda function, lowers our overall operating costs for the service. 1 At the time when the problems occurred, the password hashing was a necessary evil. We have since implemented the option for passwordless login using a single-use code sent via email which doesn’t pose the same challenges. Tags: Technology, Sisäänkirjautuminen Technology Sisäänkirjautuminen Password Tech <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_2804,h_2804,c_fill,g_faces/w_400/v1727783056/39-135677866fbdf4e84ef7.jpg" alt="Tietokoneelta näytetään graafeja analytiikkaluvuista." /> Streamlining Yle’s visualisation platforms - how we did it Moving from multiple data visualisation platforms into one by adopting Power BI as a unified visualisation tool has been an important milestone in democratising Yle's data. Here's the story of our journey. 2.10.2024 <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_906,h_906,c_crop,x_550,y_0/w_400/v1693572408/39-116625664f1dbe7ed61f.jpg" alt="A robot reports from Stafettkarnevalen, a running competition for children" /> Artifical Intelligence As a Sports Reporter - Does It Work? An article about a test pilot with Chat GPT as a journalist 5.9.2023 <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_2355,h_2355,c_crop,x_1848,y_8/w_400/v1670964941/39-10477816398e5e4ce2e2.jpg" alt="Kuvassa kuviteellinen robottihahmo, joka tuottaa automaattisia uutisia puhesynteesin avulla radioon." /> Yle awarded the most interesting pilot projects in 2022: Automated radio news, real time movement data, an AI judge, 3D journalism, a text-to-image pilot and many others Yle awarded the most interesting pilot projects in 2022 20.12.2022 <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_960,h_960,c_crop,x_0,y_0/w_400/v1670968970/39-10478156398f5bda91f5.jpg" alt="SM Entertainmentin tilojen presentaatioseinä Seoulissa Etelä-Koreassa. Kuvassa k-pop-tähtiä." /> 한국은 매력적인 시장이다 – Korea is an interesting market! An article describing South-Korean media ecosystem 15.12.2022 <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_720,h_720,c_crop,x_0,y_0/w_400/v1669739437/39-10412136386335c2401f.jpg" alt="Kuvassa Edith Hammarin kuvitusta Sexiga Byxor och andra problem -audiodraamasta" /> The future of audio content is non-linear? Yle’s interactive LGBTQIA+audio drama won Prix Europa Award 2022 Yle won the Prix Europa's Best Digital Audio Project 2022 29.11.2022 <img class="ydd-image__image" src="https://images.cdn.yle.fi/image/upload/f_auto,fl_progressive/q_auto/w_1900,h_1900,c_fill,g_faces/w_400/v1664776042/39-1015759633a76f0d7978.jpg" alt="Kuvataideopettaja Miina Elina Pukari poseeraa luokkansa edessä." /> Using AI in Art Education: Can AI imagine? What is imagination? We did an experiment of using AI in a school environment with art educator Miina Elina Pukari. It was fun. It was interesting. It was fun. It was interesting. I wrote that twice. Can AI do the same? 3.10.2022
======>
https://old.reddit.com/r/rust/comments/1gzol4f/c_string_literals_are_awesome/
-->>-->>
I started programming in Rust looking for a safer replacement for C that would produce code that would bug at runtime. And my personal complaint with Rust was its inability to be friendly with C, especially when it comes to string literals. I could get around this with crates like const-cstr so it wasn't a big problem.   

   But recently I got back into programming and updated some of my crates that use C strings and I was amazed when I saw that in version 1.77 C string literals were added, not only that but they also went further and made their use very practical by emitting a type &CStr to represent them in code. With this I was able to remove the const-cstr crate, which is unmaintained and has a report on rustsec warning not to use it, in addition to making the development of FFI code less complicated and safer.   

   That was it, I just wanted to thanks rust teams for this incredible addition 👍.   
   

======>
https://jack.wrenn.fyi/blog/undroppable/
-->>-->>
Undroppable Types 2024-11-25 In Rust, ManuallyDrop is the tool of choice for eliding the destructor of a value. Wrapped in ManuallyDrop , a value is simply forgotten when it goes out of scope. However, in some cases, it is useful to ensure that a value cannot be dropped — not because it is forgotten, but because the very act of dropping it is a compilation error. To achieve this, we only need to panic in a const context in its destructor; e.g.: /// A type that cannot be dropped. pub struct Undroppable<T: ?Sized>(mem::ManuallyDrop<T>); impl <T> Undroppable<T> { // Makes `val` undroppable. // // If `val` has a  non-trivial destructor, attempting // to drop it will result in a compilation error. pub fn new_unchecked ( val : T) -> Self { Self (mem::ManuallyDrop::new(val)) } } impl <T:? Sized> Drop for Undroppable<T> { fn drop ( & mut self ) { const { assert!( ! mem::needs_drop::<T>(), "This cannot be dropped." ); } } } Unless we mem::forget a val wrapped in Undroppable (or further wrap it in ManuallyDrop ); e.g.: fn main () { let undroppable = Undroppable::new_unchecked(vec![ 1 , 2 , 3 ]); // commenting out this line results in a compilation error: core::mem::forget(undroppable); } …dropping it results in a compilation error: error[E0080]: evaluation of `<Undroppable<std::vec::Vec<i32>> as std::ops::Drop>::drop::{constant#0}` failed --> src/main.rs:28:13 | 28 |             assert!(!mem::needs_drop::<T>(), "This cannot be dropped."); |             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ the evaluated program panicked at 'This cannot be dropped.', src/main.rs:28:13 | = note: this error originates in the macro `$crate::panic::panic_2021` which comes from the expansion of the macro `assert` (in Nightly builds, run with -Z macro-backtrace for more info) note: erroneous constant encountered --> src/main.rs:27:9 | 27 | /         const { 28 | |             assert!(!mem::needs_drop::<T>(), "This cannot be dropped."); 29 | |         } | |_________^ note: the above error was encountered while instantiating `fn <Undroppable<std::vec::Vec<i32>> as std::ops::Drop>::drop` --> /playground/.rustup/toolchains/stable-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ptr/mod.rs:574:1 | 574 | pub unsafe fn drop_in_place<T: ?Sized>(to_drop: *mut T) { | ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ This error message is poor as the ‘stack’ trace only extends one frame above our custom drop impl. However, in some scenarios, even a poor compilation error is preferable to a runtime panic. Hopefully, this diagnostic will be improved in future versions of rustc. In zerocopy , we may soon be using this approach eliminate a stubborn T: Sized bound from our Unalign<T> layout gadget, while ensuring that Unalign does not silently forget DSTs with non-trivial Drop implementations. Email comments and corrections to jack@wrenn.fyi . Follow me on Twitter!
======>
https://old.reddit.com/r/rust/comments/1gzxvur/i_built_a_macro_that_lets_you_write_cli_apps_with/
-->>-->>
https://crates.io/crates/terse_cli   

   👋 I'm a python dev who recently joined the rust community, and in the python world we have    typer    -- you write any typed function and it turns it into a CLI command using a decorator.    

   Clap   's derive syntax still feels like a lot of unnecessary structs/enums to me, so I built a macro that essentially builds the derive syntax for you (based on function params and their types).   

   ```rs
use clap::Parser;
use terse_cli::{command, subcommands};   

   [command]   

   fn add(a: i32, b: Option<i32>) -> i32 {
    a + b.unwrap_or(0)
}   

   [command]   

   fn greet(name: String) -> String {
    format!("hello {}!", name)
}   

   subcommands!(cli, [add, greet]);   

   fn main() {
    cli::run(cli::Args::parse());
}
```   

   That program produces a cli like this:
```sh
$ cargo run add --a 3 --b 4
7   

   $ cargo run greet --name Bob
hello Bob!   

   $ cargo run help
Usage: stuff <COMMAND>   

   Commands:
  add   
  greet   
  help   Print this message or the help of the given subcommand(s)   

   Options:
  -h, --help     Print help
  -V, --version  Print version
```   

   Give it a try, tell me what you like/dislike, and it's open for contributions :)   
   

https://datatracker.ietf.org/doc/html/rfc8441
-->>-->>
Internet Engineering Task Force (IETF)                        P. McManus
Request for Comments: 8441                                       Mozilla
Updates: 6455 September 2018
Category: Standards Track
ISSN: 2070-1721 Bootstrapping WebSockets with HTTP/2 Abstract

   This document defines a mechanism for running the WebSocket Protocol
   ( RFC 6455 ) over a single stream of an HTTP/2 connection.

Status of This Memo

   This is an Internet Standards Track document.

   This document is a product of the Internet Engineering Task Force
   (IETF).  It represents the consensus of the IETF community.  It has
   received public review and has been approved for publication by the
   Internet Engineering Steering Group (IESG).  Further information on
   Internet Standards is available in Section 2 of RFC 7841 .

   Information about the current status of this document, any errata,
   and how to provide feedback on it may be obtained at https://www.rfc-editor.org/info/rfc8441 .

Copyright Notice

   Copyright (c) 2018 IETF Trust and the persons identified as the
   document authors.  All rights reserved.

   This document is subject to BCP 78 and the IETF Trust's Legal
   Provisions Relating to IETF Documents
   ( https://trustee.ietf.org/license-info ) in effect on the date of
   publication of this document.  Please review these documents
   carefully, as they describe your rights and restrictions with respect
   to this document.  Code Components extracted from this document must
   include Simplified BSD License text as described in Section 4.e of
   the Trust Legal Provisions and are provided without warranty as
   described in the Simplified BSD License. McManus                      Standards Track                    [Page 1] RFC 8441 H2 WebSockets               September 2018 Table of Contents 1 .  Introduction  . . . . . . . . . . . . . . . . . . . . . . . . 2 2 .  Terminology . . . . . . . . . . . . . . . . . . . . . . . . . 3 3 .  The SETTINGS_ENABLE_CONNECT_PROTOCOL SETTINGS Parameter . . . 3 4 .  The Extended CONNECT Method . . . . . . . . . . . . . . . . . 4 5.  Using Extended CONNECT to Bootstrap the WebSocket Protocol  .   4 5.1 .  Example . . . . . . . . . . . . . . . . . . . . . . . . . 6 6 .  Design Considerations . . . . . . . . . . . . . . . . . . . . 6 7 .  About Intermediaries  . . . . . . . . . . . . . . . . . . . . 6 8 .  Security Considerations . . . . . . . . . . . . . . . . . . . 7 9 .  IANA Considerations . . . . . . . . . . . . . . . . . . . . . 7 9.1 .  A New HTTP/2 Setting  . . . . . . . . . . . . . . . . . . 7 9.2 .  A New HTTP Upgrade Token  . . . . . . . . . . . . . . . . 7 10 . Normative References  . . . . . . . . . . . . . . . . . . . . 8 Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . 8 Author's Address  . . . . . . . . . . . . . . . . . . . . . . . . 8 1 .  Introduction The Hypertext Transfer Protocol (HTTP) [ RFC7230 ] provides compatible
   resource-level semantics across different versions, but it does not
   offer compatibility at the connection-management level.  Other
   protocols that rely on connection-management details of HTTP, such as
   WebSockets, must be updated for new versions of HTTP.

   The WebSocket Protocol [ RFC6455 ] uses the HTTP/1.1 Upgrade mechanism
   ( Section 6.7 of [RFC7230] ) to transition a TCP connection from HTTP
   into a WebSocket connection.  A different approach must be taken with
   HTTP/2 [ RFC7540 ].  Due to its multiplexing nature, HTTP/2 does not
   allow connection-wide header fields or status codes, such as the
   Upgrade and Connection request-header fields or the 101 (Switching
   Protocols) response code.  These are all required by the [ RFC6455 ]
   opening handshake.

   Being able to bootstrap WebSockets from HTTP/2 allows one TCP
   connection to be shared by both protocols and extends HTTP/2's more
   efficient use of the network to WebSockets.

   This document extends the HTTP CONNECT method, as specified for
   HTTP/2 in Section 8.3 of [RFC7540] .  The extension allows the
   substitution of a new protocol name to connect to rather than the
   external host normally used by CONNECT.  The result is a tunnel on a
   single HTTP/2 stream that can carry data for WebSockets (or any other
   protocol).  The other streams on the connection may carry more
   extended CONNECT tunnels, traditional HTTP/2 data, or a mixture of
   both. McManus                      Standards Track                    [Page 2] RFC 8441 H2 WebSockets               September 2018 This tunneled stream will be multiplexed with other regular streams
   on the connection and enjoys the normal priority, cancellation, and
   flow-control features of HTTP/2.

   Streams that successfully establish a WebSocket connection using a
   tunneled stream and the modifications to the opening handshake
   defined in this document then use the traditional WebSocket Protocol,
   treating the stream as if it were the TCP connection in that
   specification. 2 .  Terminology The key words "MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT",
   "SHOULD", "SHOULD NOT", "RECOMMENDED", "NOT RECOMMENDED", "MAY", and
   "OPTIONAL" in this document are to be interpreted as described in BCP 14 [ RFC2119 ] [ RFC8174 ] when, and only when, they appear in all
   capitals, as shown here. 3 .  The SETTINGS_ENABLE_CONNECT_PROTOCOL SETTINGS Parameter This document adds a new SETTINGS parameter to those defined by [RFC7540], Section 6.5.2 .

   The new parameter name is SETTINGS_ENABLE_CONNECT_PROTOCOL.  The
   value of the parameter MUST be 0 or 1.

   Upon receipt of SETTINGS_ENABLE_CONNECT_PROTOCOL with a value of 1, a
   client MAY use the Extended CONNECT as defined in this document when
   creating new streams.  Receipt of this parameter by a server does not
   have any impact.

   A sender MUST NOT send a SETTINGS_ENABLE_CONNECT_PROTOCOL parameter
   with the value of 0 after previously sending a value of 1.

   Using a SETTINGS parameter to opt into an otherwise incompatible
   protocol change is a use of "Extending HTTP/2" defined by Section 5.5
   of [RFC7540] .  Specifically, the addition a new pseudo-header field,
   ":protocol", and the change in meaning of the :authority pseudo-
   header field in Section 4 require opt-in negotiation.  If a client
   were to use the provisions of the extended CONNECT method defined in
   this document without first receiving a
   SETTINGS_ENABLE_CONNECT_PROTOCOL parameter, a non-supporting peer
   would detect a malformed request and generate a stream error
   ( Section 8.1.2.6 of [RFC7540] ). McManus                      Standards Track                    [Page 3] RFC 8441 H2 WebSockets               September 2018 4 .  The Extended CONNECT Method Usage of the CONNECT method in HTTP/2 is defined by Section 8.3 of
   [RFC7540] .  This extension modifies the method in the following ways:

   o  A new pseudo-header field :protocol MAY be included on request
      HEADERS indicating the desired protocol to be spoken on the tunnel
      created by CONNECT.  The pseudo-header field is single valued and
      contains a value from the "Hypertext Transfer Protocol (HTTP)
      Upgrade Token Registry" located at
      < https://www.iana.org/assignments/http-upgrade-tokens/ >

   o  On requests that contain the :protocol pseudo-header field, the
      :scheme and :path pseudo-header fields of the target URI (see Section 5 ) MUST also be included.

   o  On requests bearing the :protocol pseudo-header field, the
      :authority pseudo-header field is interpreted according to Section 8.1.2.3 of [RFC7540] instead of Section 8.3 of that
      document.  In particular, the server MUST NOT create a tunnel to
      the host indicated by the :authority as it would with a CONNECT
      method request that was not modified by this extension.

   Upon receiving a CONNECT request bearing the :protocol pseudo-header
   field, the server establishes a tunnel to another service of the
   protocol type indicated by the pseudo-header field.  This service may
   or may not be co-located with the server. 5 .  Using Extended CONNECT to Bootstrap the WebSocket Protocol The :protocol pseudo-header field MUST be included in the CONNECT
   request, and it MUST have a value of "websocket" to initiate a
   WebSocket connection on an HTTP/2 stream.  Other HTTP request and
   response-header fields, such as those for manipulating cookies, may
   be included in the HEADERS with the CONNECT method as usual.  This
   request replaces the GET-based request in [ RFC6455 ] and is used to
   process the WebSockets opening handshake.

   The scheme of the target URI ( Section 5.1 of [RFC7230] ) MUST be
   "https" for "wss"-schemed WebSockets and "http" for "ws"-schemed
   WebSockets.  The remainder of the target URI is the same as the
   WebSocket URI.  The WebSocket URI is still used for proxy
   autoconfiguration.  The security requirements for the HTTP/2
   connection used by this specification are established by [ RFC7540 ]
   for https requests and [ RFC8164 ] for http requests. McManus                      Standards Track                    [Page 4] RFC 8441 H2 WebSockets               September 2018 [ RFC6455 ] requires the use of Connection and Upgrade header fields
   that are not part of HTTP/2.  They MUST NOT be included in the
   CONNECT request defined here.

   [ RFC6455 ] requires the use of a Host header field that is also not
   part of HTTP/2.  The Host information is conveyed as part of the
   :authority pseudo-header field, which is required on every HTTP/2
   transaction.

   Implementations using this extended CONNECT to bootstrap WebSockets
   do not do the processing of the Sec-WebSocket-Key and Sec-WebSocket-
   Accept header fields of [ RFC6455 ] as that functionality has been
   superseded by the :protocol pseudo-header field.

   The Origin [ RFC6454 ], Sec-WebSocket-Version, Sec-WebSocket-Protocol,
   and Sec-WebSocket-Extensions header fields are used in the CONNECT
   request and response-header fields as defined in [ RFC6455 ].  Note
   that HTTP/1 header field names were case insensitive, whereas HTTP/2
   requires they be encoded as lowercase.

   After successfully processing the opening handshake, the peers should
   proceed with the WebSocket Protocol [ RFC6455 ] using the HTTP/2 stream
   from the CONNECT transaction as if it were the TCP connection
   referred to in [ RFC6455 ].  The state of the WebSocket connection at
   this point is OPEN, as defined by [RFC6455], Section 4.1 .

   The HTTP/2 stream closure is also analogous to the TCP connection
   closure of [ RFC6455 ].  Orderly TCP-level closures are represented as
   END_STREAM flags ( [RFC7540], Section 6.1 ).  RST exceptions are
   represented with the RST_STREAM frame ( [RFC7540], Section 6.4 ) with
   the CANCEL error code ( [RFC7540], Section 7 ). McManus                      Standards Track                    [Page 5] RFC 8441 H2 WebSockets               September 2018 5.1 .  Example [[ From Client ]]                       [[ From Server ]]

                                        SETTINGS
                                        SETTINGS_ENABLE_CONNECT_[..] = 1

HEADERS + END_HEADERS
:method = CONNECT
:protocol = websocket
:scheme = https
:path = /chat
:authority = server.example.com
sec-websocket-protocol = chat, superchat
sec-websocket-extensions = permessage-deflate
sec-websocket-version = 13
origin = http://www.example.com

                                        HEADERS + END_HEADERS
                                        :status = 200
                                        sec-websocket-protocol = chat

DATA
WebSocket Data

                                        DATA + END_STREAM
                                        WebSocket Data

DATA + END_STREAM
WebSocket Data 6 .  Design Considerations A more native integration with HTTP/2 is certainly possible with
   larger additions to HTTP/2.  This design was selected to minimize the
   solution complexity while still addressing the primary concern of
   running HTTP/2 and WebSockets concurrently. 7 .  About Intermediaries This document does not change how WebSockets interacts with HTTP
   forward proxies.  If a client wishing to speak WebSockets connects
   via HTTP/2 to an HTTP proxy, it should continue to use a traditional
   CONNECT (i.e., not with a :protocol pseudo-header field) to tunnel
   through that proxy to the WebSocket server via HTTP. McManus                      Standards Track                    [Page 6] RFC 8441 H2 WebSockets               September 2018 The resulting version of HTTP on that tunnel determines whether
   WebSockets is initiated directly or via a modified CONNECT request
   described in this document. 8 .  Security Considerations [ RFC6455 ] ensures that non-WebSockets clients, especially
   XMLHttpRequest-based clients, cannot make a WebSocket connection.
   Its primary mechanism for doing that is the use of Sec-prefixed
   request-header fields that cannot be created by XMLHttpRequest-based
   clients.  This specification addresses that concern in two ways:

   o  XMLHttpRequest also prohibits use of the CONNECT method in
      addition to Sec-prefixed request-header fields.

   o  The use of a pseudo-header field is something that is connection
      specific, and HTTP/2 never allows a pseudo-header to be created
      outside of the protocol stack.

   The security considerations of [RFC6455], Section 10 continue to
   apply to the use of the WebSocket Protocol when using this
   specification, with the exception of 10.8.  That section is not
   relevant, because it is specific to the bootstrapping handshake that
   is changed in this document. 9 .  IANA Considerations 9.1 .  A New HTTP/2 Setting This document registers an entry in the "HTTP/2 Settings" registry
   that was established by Section 11.3 of [RFC7540] .

      Code: 0x8
      Name: SETTINGS_ENABLE_CONNECT_PROTOCOL
      Initial Value: 0
      Specification: This document 9.2 .  A New HTTP Upgrade Token This document registers an entry in the "HTTP Upgrade Tokens"
   registry that was established by [ RFC7230 ].

      Value: websocket
      Description: The Web Socket Protocol
      Expected Version Tokens:
      References: [ RFC6455 ] [ RFC8441 ] McManus                      Standards Track                    [Page 7] RFC 8441 H2 WebSockets               September 2018 10 .  Normative References [ RFC2119 ]  Bradner, S., "Key words for use in RFCs to Indicate
              Requirement Levels", BCP 14 , RFC 2119 ,
              DOI 10.17487/RFC2119, March 1997,
              < https://www.rfc-editor.org/info/rfc2119 >.

   [ RFC6454 ]  Barth, A., "The Web Origin Concept", RFC 6454 ,
              DOI 10.17487/RFC6454, December 2011,
              < https://www.rfc-editor.org/info/rfc6454 >.

   [ RFC6455 ]  Fette, I. and A. Melnikov, "The WebSocket Protocol", RFC 6455 , DOI 10.17487/RFC6455, December 2011,
              < https://www.rfc-editor.org/info/rfc6455 >.

   [ RFC7230 ]  Fielding, R., Ed. and J. Reschke, Ed., "Hypertext Transfer
              Protocol (HTTP/1.1): Message Syntax and Routing", RFC 7230 , DOI 10.17487/RFC7230, June 2014,
              < https://www.rfc-editor.org/info/rfc7230 >.

   [ RFC7540 ]  Belshe, M., Peon, R., and M. Thomson, Ed., "Hypertext
              Transfer Protocol Version 2 (HTTP/2)", RFC 7540 ,
              DOI 10.17487/RFC7540, May 2015,
              < https://www.rfc-editor.org/info/rfc7540 >.

   [ RFC8164 ]  Nottingham, M. and M. Thomson, "Opportunistic Security for
              HTTP/2", RFC 8164 , DOI 10.17487/RFC8164, May 2017,
              < https://www.rfc-editor.org/info/rfc8164 >.

   [ RFC8174 ]  Leiba, B., "Ambiguity of Uppercase vs Lowercase in RFC 2119 Key Words", BCP 14 , RFC 8174 , DOI 10.17487/RFC8174,
              May 2017, < https://www.rfc-editor.org/info/rfc8174 >.

Acknowledgments

   The 2017 HTTP Workshop had a very productive discussion that helped
   determine the key problem and acceptable level of solution
   complexity.

Author's Address

   Patrick McManus
   Mozilla

   Email: mcmanus@ducksong.com






McManus                      Standards Track                    [Page 8]
======>
https://github.com/LesnyRumcajs/grpc_bench/discussions/475
-->>-->>
-----------------------------------------------------------------------------------------------------------------------------------------
| name                        |   req/s |   avg. latency |        90 % in |        95 % in |        99 % in | avg. cpu |   avg. memory |
-----------------------------------------------------------------------------------------------------------------------------------------
| rust_wtx                    |   67502 |       12.45 ms |       21.23 ms |       25.63 ms |       34.11 ms |   51.54% |     15.97 MiB |
| rust_thruster_st            |   61555 |       13.10 ms |       21.35 ms |       25.68 ms |       34.41 ms |   61.14% |     13.57 MiB |
| rust_tonic_st               |   61183 |       13.42 ms |       20.72 ms |       24.46 ms |       33.82 ms |   69.82% |     18.04 MiB |
| rust_thruster_mt            |   56617 |       15.04 ms |       26.92 ms |       32.23 ms |       44.27 ms |   70.39% |     12.72 MiB |
| go_vtgrpc                   |   56025 |       15.87 ms |       24.90 ms |       29.33 ms |       39.62 ms |   88.49% |     27.79 MiB |
| rust_grpcio                 |   48748 |       17.35 ms |       26.29 ms |       31.91 ms |       43.16 ms |   90.62% |      27.5 MiB |
| rust_tonic_mt               |   46858 |       17.93 ms |       37.79 ms |       48.60 ms |       70.36 ms |   89.95% |      14.6 MiB |
| go_grpc                     |   45066 |       20.65 ms |       31.42 ms |       35.28 ms |       44.66 ms |   95.37% |     27.73 MiB |
| cpp_grpc_mt                 |   41461 |       22.62 ms |       31.07 ms |       38.61 ms |       49.49 ms |   95.01% |      8.67 MiB |
| cpp_grpc_st                 |   40906 |       22.88 ms |       30.93 ms |       38.21 ms |       49.87 ms |   95.86% |      9.77 MiB |
| cpp_asio_grpc               |   39784 |       23.64 ms |       32.07 ms |       39.63 ms |       52.12 ms |   93.44% |      8.41 MiB |
| java_quarkus_native         |   26856 |       36.45 ms |       46.60 ms |       53.38 ms |       62.75 ms |  100.57% |    129.45 MiB |
| crystal_grpc                |   25535 |       38.81 ms |       40.24 ms |       42.13 ms |       48.38 ms |   99.74% |     60.92 MiB |
| dotnet_grpc                 |   18838 |       50.34 ms |      122.60 ms |      188.77 ms |      278.37 ms |   101.2% |    179.37 MiB |
| swift_grpc                  |   16901 |       58.85 ms |       59.52 ms |       59.72 ms |       61.77 ms |  100.31% |      6.64 MiB |
| scala_akka                  |   13039 |       72.91 ms |      197.83 ms |      289.29 ms |      461.55 ms |   98.33% |    187.75 MiB |
| haskell_grpc_haskell        |   12459 |       79.78 ms |      103.33 ms |      119.08 ms |      197.38 ms |  103.75% |     104.7 MiB |
| go_vtconnect                |   12377 |       80.28 ms |      150.02 ms |      405.34 ms |      632.64 ms |  103.46% |     29.09 MiB |
| go_connect                  |   12106 |       82.04 ms |      153.62 ms |      396.39 ms |      658.06 ms |  101.58% |     30.32 MiB |
| node_grpcjs_st              |    9211 |      107.16 ms |      138.12 ms |      154.96 ms |      192.66 ms |   98.78% |     155.4 MiB |
| scala_pekko                 |    5661 |      170.31 ms |      317.43 ms |      405.14 ms |      671.89 ms |   99.83% |     198.2 MiB |
| java_vertx_grpc             |    5282 |      185.20 ms |      400.76 ms |      490.55 ms |      763.85 ms |  101.56% |     142.8 MiB |
| java_hotspot_grpc_pgc       |    4241 |      227.06 ms |      383.68 ms |      484.85 ms |         1.26 s |  100.64% |    124.25 MiB |
| java_quarkus                |    3907 |      245.35 ms |      418.55 ms |      565.51 ms |         1.92 s |   84.47% |    166.13 MiB |
| java_hotspot_grpc_sgc       |    3833 |      251.10 ms |      403.24 ms |      568.91 ms |         1.21 s |  101.14% |     124.9 MiB |
| java_hotspot_grpc_g1gc      |    3009 |      321.65 ms |      520.85 ms |      661.09 ms |         1.55 s |  101.58% |    113.64 MiB |
| java_hotspot_grpc_zgc       |    2967 |      325.07 ms |      481.99 ms |      606.85 ms |         1.68 s |  100.72% |    328.95 MiB |
| java_aot                    |    2846 |      342.38 ms |      595.93 ms |      736.32 ms |      972.09 ms |  100.69% |    387.07 MiB |
| java_hotspot_grpc_she       |    2162 |      446.67 ms |      689.24 ms |      910.58 ms |         1.75 s |    98.6% |    147.35 MiB |
| java_micronaut_workstealing |    1511 |      636.25 ms |      770.30 ms |         1.02 s |         3.85 s |   99.06% |     171.8 MiB |
| java_micronaut              |    1361 |      704.54 ms |      796.83 ms |      996.17 ms |         4.24 s |   100.7% |    176.25 MiB |
| python_async_grpc           |    1360 |      708.35 ms |      918.68 ms |      945.50 ms |      955.95 ms |  101.92% |     50.91 MiB |
| erlang_grpcbox              |    1185 |      808.13 ms |         1.12 s |         1.21 s |         1.42 s |  101.45% |     103.2 MiB |
| d_grpc                      |    1152 |      827.46 ms |      211.37 ms |      292.61 ms |        11.80 s |  104.21% |     25.31 MiB |
| kotlin_grpc                 |    1150 |      828.39 ms |         1.19 s |         1.57 s |         6.64 s |   99.43% |    150.65 MiB |
| java_armeria                |    1138 |      839.22 ms |         1.21 s |         2.19 s |         3.40 s |  100.13% |     221.9 MiB |
| ruby_grpc                   |    1044 |      910.61 ms |         1.11 s |         1.11 s |         1.14 s |   86.52% |     55.08 MiB |
| python_grpc                 |     954 |      995.03 ms |         1.12 s |         1.13 s |         1.15 s |  107.22% |     45.61 MiB |
| scala_zio                   |     786 |         1.19 s |         1.90 s |         4.36 s |         6.04 s |   99.22% |     178.9 MiB |
| scala_fs2                   |     757 |         1.24 s |         2.40 s |         5.43 s |         7.04 s |   98.07% |    198.45 MiB |
| php_grpc                    |     697 |         1.34 s |         1.48 s |         1.49 s |         1.49 s |   99.27% |    104.47 MiB |
| java_openj9_grpc_gencon     |     151 |         4.96 s |         9.59 s |        10.39 s |        10.91 s |    99.1% |    103.67 MiB |
-----------------------------------------------------------------------------------------------------------------------------------------
Benchmark Execution Parameters:
38875a7 Thu, 5 Sep 2024 12:40:14 +0200 GitHub Add the `wtx` project (#473)
- GRPC_BENCHMARK_DURATION=20s
- GRPC_BENCHMARK_WARMUP=5s
- GRPC_SERVER_CPUS=1
- GRPC_SERVER_RAM=16GB
- GRPC_CLIENT_CONNECTIONS=50
- GRPC_CLIENT_CONCURRENCY=1000
- GRPC_CLIENT_QPS=0
- GRPC_CLIENT_CPUS=4
- GRPC_REQUEST_SCENARIO=complex_proto
- GRPC_GHZ_TAG=0.114.0
======>
https://www.reddit.com/r/rust/comments/1et6bdc/a_new_http2_framework_created_from_scratch/
-->>-->>
Go to rust r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online • 2 mo. ago c410-f3r ADMIN MOD A new HTTP/2 framework created from scratch wtx is, among other things, a RFC7541 and RFC9113 implementation intended to allow the development of web applications through a built-in server framework as well as a built-in PostgreSQL connector. The project  supports `no_std`,  doesn't have any mandatory dependencies and showed promising runtime results. A set of benchmarks comparing other HTTP/2 web frameworks is available at https://c410-f3r.github.io/wtx-bench/ . Here goes a list of similar projects and their development features. axum + bb8 + tokio-postgres ( https://github.com/tokio-rs/axum/tree/main/examples/tokio-postgres ) : 132 dependencies, optimized binary of ~3.6M and a build time of ~26s. actix-web + deadpool + tokio-postgres ( https://github.com/actix/examples/tree/master/databases/postgres ) : 264 dependencies, optimized binary of ~4.1M and a build time of ~38s. wtx ( https://github.com/c410-f3r/wtx/tree/main/wtx-instances/examples/http-server-framework.rs ) : 46 dependencies, optimized binary of ~840k and a build time of ~12s. GRPC over HTTP/2 is in progress and is hopefully expected to be completed in the next few weeks. Please feel free to point out any misunderstandings, suggestions, or misconfigurations regarding the benchmarks or the numbers presented in this post. All the information provided here is public and can be tested locally on your own machine. Read more Intel • Official • Promoted Game over choppy gameplay. Upgrade to better gaming with AI and an #IntelCoreUltra processor Shop Now staples.ca Add a Comment Sort by: Best Open comment sort options Best Top New Controversial Old Q&A rusted-flosse • 2mo ago • I'm sorry, but combining HTTP/2 and Postgres in the same library is exactly the opposite of what I desire (a.k.a. framework vs. library). I try to avoid these battery-included approaches with all my effort. Sorry, that's not meant to be demotivating, just a personal perspective. Reply reply sweating_teflon • 2mo ago • I agree it's an unusual choice but postgres seems to be behind a flag so there's that. Reply reply luveti • 2mo ago • Title says framework, not library? Reply reply More replies worriedjacket • 2mo ago • Odd choice to have http and Postgres in the same library Reply reply jvo203 • 2mo ago • Have you implemented real-time HTTP/2 streaming (setting priorities higher for real-time streams) as well as WebSockets over HTTP/2? These are absolute essentials in high-performance real-time applications. Reply reply jvo203 • 2mo ago • I see the wtx GitHub site lists a support for WebSockets, which is great! Reply reply More replies t-kiwi • 2mo ago • This looks really cool :) will check out it Reply reply hyperchromatica • 2mo ago • ill have to give it a spin Reply reply ImaginaryZucchini39 • 2mo ago • Are there plans to implement splitting wtx websockets into read/write halves like the fastwebsockets unstable-split feature? Looking into switching from fastwebsockets to this. Reply reply Top 1% Rank by size Public Anyone can view, post, and comment to this community More posts you may like Related Rust Programming open-source software Technology Free software Software Information & communications technology forward back r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online fastwebsockets A new high-performance WebSocket protocol implementation in Rust github 115 upvotes · 12 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online A `no_std` client for PostgreSQL 33 upvotes · 2 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Rust to .NET compiler update - f128, f16, and beginnings of SIMD and async 342 upvotes · 7 comments Promoted r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online A new HTTP/2 server and client written in Rust github 23 upvotes · 5 comments r/golang r/golang Ask questions and post articles about the Go programming language and related tools, events etc. 279K Members 103 Online Built a prod-grade json api from scratch using only the std lib 4 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online unresolved import `axum::server` --> src/main.rs:7:5 | 7 | server::Server, | ^^^^^^ could not find `server` in `axum` 2 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online What are some OSS projects that are using hyper h2 9 upvotes · 9 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Looking for a maintainer for memmap2-rs 194 upvotes · 21 comments Promoted r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online A rust newbie wrote a websocket client library based on Deno's fastwebsockets. 5 upvotes · 2 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Felix' Blog - Replacing nginx with axum 118 upvotes · 30 comments r/opensource r/opensource A subreddit for everything open source related (for this context, we go off the definition of open source here http://en.wikipedia.org/wiki/Open_source) 240K Members 64 Online Kindling; a programmable TLS HTTP/1.1 server in pure Java 1 upvote r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online jeprofl: A low-overhead allocation profiler using eBPF 165 upvotes · 7 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Rust Search Extension v2.0 has been released 131 upvotes · 9 comments r/deeplearning r/deeplearning 168K Members 32 Online [project] I created a simple implementation of LLama-3 from scratch(one file) 8 upvotes · 1 comment r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online image v0.25.4 brings faster WebP decoding, orientation metadata support, fast blur 101 upvotes · 24 comments r/devops r/devops 353K Members 122 Online Looking for feedback - We built a Python SDK to automate GCP / AWS infrastructure 11 upvotes · 5 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online PSA: size_of and align_of are in the prelude since 1.80 142 upvotes · 1 comment r/LangChain r/LangChain LangChain is an open-source framework and developer toolkit that helps developers get LLM applications from prototype to production. 

It is available for Python and Javascript at https://www.langchain.com/. 31K Members 24 Online I build a RAG-based multi-tenant AI Code Assistant with OpenAI, LangChain, Postgres and PG Vector 20 upvotes · 2 comments r/Firebase r/Firebase Community supported discussions on Google's Firebase platform. 30K Members 20 Online I wrote a detailed guide for setting up a Next.js project with Firebase. Includes tips for managing dev & prod environments, emulator suite, debugging, and more. 20 upvotes · 10 comments r/golang r/golang Ask questions and post articles about the Go programming language and related tools, events etc. 279K Members 103 Online Pagoda v0.16.0: Rapid, easy, full-stack web dev starter kit (now w/ SQLite, HTMX 2, and more) 9 upvotes · 10 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Gosub - An open source browser engine written in Rust 296 upvotes · 27 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Rust support in KDE 170 upvotes · 33 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 204 Online Rust for Android 426 upvotes · 38 comments r/PostgreSQL r/PostgreSQL The home of the most advanced Open Source database server on the worlds largest and most active Front Page of the Internet. 47K Members 26 Online I built a RAG-based multi-tenant AI code assistant with gpt-4o-mini, Postgres and pg_vector 8 upvotes · 2 comments r/deeplearning r/deeplearning 168K Members 32 Online How To Build Your Retrieval Augmented Generation (RAG) Using Open-source Tools: LangChain, LLAMA 3, and Chroma. A visual guide + Jupyter Notebook. 🧠 2 upvotes
======>
https://github.com/diesel-rs/metrics
-->>-->>
Skip to content {"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}} {"resolvedServerColorMode":"day"} Navigation Menu Toggle navigation Sign in Product GitHub Copilot Write better code with AI Security Find and fix vulnerabilities Actions Automate any workflow Codespaces Instant dev environments Issues Plan and track work Code Review Manage code changes Discussions Collaborate outside of code Code Search Find more, search less Explore All features Documentation GitHub Skills Blog Solutions By company size Enterprises Small and medium teams Startups By use case DevSecOps DevOps CI/CD View all use cases By industry Healthcare Financial services Manufacturing Government View all industries View all solutions Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} diesel-rs / metrics Public Notifications You must be signed in to change notification settings Fork 2 Star 90 90 stars 2 forks Branches Tags Activity Star Notifications You must be signed in to change notification settings Code Pull requests 0 Actions Security Insights Additional navigation options Code Pull requests Actions Security Insights diesel-rs/metrics {"props":{"initialPayload":{"allShortcutsEnabled":false,"path":"/","repo":{"id":364263815,"defaultBranch":"master","name":"metrics","ownerLogin":"diesel-rs","currentUserCanPush":false,"isFork":false,"isEmpty":false,"createdAt":"2021-05-04T13:23:46.000Z","ownerAvatar":"https://avatars.githubusercontent.com/u/16763251?v=4","public":true,"private":false,"isOrgOwned":true},"currentUser":null,"refInfo":{"name":"master","listCacheKey":"v0:1677750056.994547","canEdit":false,"refType":"branch","currentOid":"7d4c141c80836be5cd284dbb558e456aff9e73aa"},"tree":{"items":[{"name":".github/workflows","path":".github/workflows","contentType":"directory","hasSimplifiedPath":true},{"name":"metrics","path":"metrics","contentType":"directory"},{"name":"Readme.md","path":"Readme.md","contentType":"file"},{"name":"analytics.R","path":"analytics.R","contentType":"file"}],"templateDirectorySuggestionUrl":null,"readme":null,"totalCount":4,"showBranchInfobar":false},"fileTree":null,"fileTreeProcessingTime":null,"foldersToFetch":[],"treeExpanded":false,"symbolsExpanded":false,"isOverview":true,"overview":{"banners":{"shouldRecommendReadme":false,"isPersonalRepo":false,"showUseActionBanner":false,"actionSlug":null,"actionId":null,"showProtectBranchBanner":false,"publishBannersInfo":{"dismissActionNoticePath":"/settings/dismiss-notice/publish_action_from_repo","releasePath":"/diesel-rs/metrics/releases/new?marketplace=true","showPublishActionBanner":false},"interactionLimitBanner":null,"showInvitationBanner":false,"inviterName":null,"actionsMigrationBannerInfo":{"releaseTags":[],"showImmutableActionsMigrationBanner":false,"initialMigrationStatus":null}},"codeButton":{"contactPath":"/contact","isEnterprise":false,"local":{"protocolInfo":{"httpAvailable":true,"sshAvailable":null,"httpUrl":"https://github.com/diesel-rs/metrics.git","showCloneWarning":null,"sshUrl":null,"sshCertificatesRequired":null,"sshCertificatesAvailable":null,"ghCliUrl":"gh repo clone diesel-rs/metrics","defaultProtocol":"http","newSshKeyUrl":"/settings/ssh/new","setProtocolPath":"/users/set_protocol"},"platformInfo":{"cloneUrl":"https://desktop.github.com","showVisualStudioCloneButton":false,"visualStudioCloneUrl":"https://windows.github.com","showXcodeCloneButton":false,"xcodeCloneUrl":"xcode://clone?repo=https%3A%2F%2Fgithub.com%2Fdiesel-rs%2Fmetrics","zipballUrl":"/diesel-rs/metrics/archive/refs/heads/master.zip"}},"newCodespacePath":"/codespaces/new?hide_repo_select=true\u0026repo=364263815"},"popovers":{"rename":null,"renamedParentRepo":null},"commitCount":"891","overviewFiles":[{"displayName":"Readme.md","repoName":"metrics","refName":"master","path":"Readme.md","preferredFileType":"readme","tabName":"README","richText":"\u003carticle class=\"markdown-body entry-content container-lg\" itemprop=\"text\"\u003e\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch1 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003ePerformance statistic collection for rusts database connection crates\u003c/h1\u003e\u003ca id=\"user-content-performance-statistic-collection-for-rusts-database-connection-crates\" class=\"anchor\" aria-label=\"Permalink: Performance statistic collection for rusts database connection crates\" href=\"#performance-statistic-collection-for-rusts-database-connection-crates\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eThis repository contains the numbers collected by diesel continuous scheduled benchmark actions to track changes over time.\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eResults\u003c/h2\u003e\u003ca id=\"user-content-results\" class=\"anchor\" aria-label=\"Permalink: Results\" href=\"#results\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eComparison\u003c/h3\u003e\u003ca id=\"user-content-comparison\" class=\"anchor\" aria-label=\"Permalink: Comparison\" href=\"#comparison\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eTrivial queries\u003c/h4\u003e\u003ca id=\"user-content-trivial-queries\" class=\"anchor\" aria-label=\"Permalink: Trivial queries\" href=\"#trivial-queries\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/trivial_query_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMedium complex queries\u003c/h4\u003e\u003ca id=\"user-content-medium-complex-queries\" class=\"anchor\" aria-label=\"Permalink: Medium complex queries\" href=\"#medium-complex-queries\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/medium_complex_query_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eAssociations\u003c/h4\u003e\u003ca id=\"user-content-associations\" class=\"anchor\" aria-label=\"Permalink: Associations\" href=\"#associations\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/associations_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eInserts\u003c/h4\u003e\u003ca id=\"user-content-inserts\" class=\"anchor\" aria-label=\"Permalink: Inserts\" href=\"#inserts\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/summary/insert_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch3 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003ePerformance over time\u003c/h3\u003e\u003ca id=\"user-content-performance-over-time\" class=\"anchor\" aria-label=\"Permalink: Performance over time\" href=\"#performance-over-time\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eTrivial queries\u003c/h4\u003e\u003ca id=\"user-content-trivial-queries-1\" class=\"anchor\" aria-label=\"Permalink: Trivial queries\" href=\"#trivial-queries-1\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/trivial_query_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eMedium complex queries\u003c/h4\u003e\u003ca id=\"user-content-medium-complex-queries-1\" class=\"anchor\" aria-label=\"Permalink: Medium complex queries\" href=\"#medium-complex-queries-1\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/medium_complex_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eAssociations\u003c/h4\u003e\u003ca id=\"user-content-associations-1\" class=\"anchor\" aria-label=\"Permalink: Associations\" href=\"#associations-1\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/associations_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch4 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eInserts\u003c/h4\u003e\u003ca id=\"user-content-inserts-1\" class=\"anchor\" aria-label=\"Permalink: Inserts\" href=\"#inserts-1\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003e\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_postgres.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_postgres.svg\" alt=\"Postgresql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_sqlite.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_sqlite.svg\" alt=\"Sqlite\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\n\u003ca target=\"_blank\" rel=\"noopener noreferrer\" href=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_mysql.svg\"\u003e\u003cimg src=\"https://github.com/diesel-rs/metrics/raw/results/plots/timeline/insert_mysql.svg\" alt=\"Mysql\" style=\"max-width: 100%;\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eRaw aggregated data\u003c/h2\u003e\u003ca id=\"user-content-raw-aggregated-data\" class=\"anchor\" aria-label=\"Permalink: Raw aggregated data\" href=\"#raw-aggregated-data\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cul dir=\"auto\"\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/diesel-rs/metrics/blob/results/aggregated_data/postgres.csv\"\u003ePostgresql\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/diesel-rs/metrics/blob/results/aggregated_data/sqlite.csv\"\u003eSqlite\u003c/a\u003e\u003c/li\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/diesel-rs/metrics/blob/results/aggregated_data/mysql.csv\"\u003eMysql\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cdiv class=\"markdown-heading\" dir=\"auto\"\u003e\u003ch2 tabindex=\"-1\" class=\"heading-element\" dir=\"auto\"\u003eHow to generate the plots from the data\u003c/h2\u003e\u003ca id=\"user-content-how-to-generate-the-plots-from-the-data\" class=\"anchor\" aria-label=\"Permalink: How to generate the plots from the data\" href=\"#how-to-generate-the-plots-from-the-data\"\u003e\u003csvg class=\"octicon octicon-link\" viewBox=\"0 0 16 16\" version=\"1.1\" width=\"16\" height=\"16\" aria-hidden=\"true\"\u003e\u003cpath d=\"m7.775 3.275 1.25-1.25a3.5 3.5 0 1 1 4.95 4.95l-2.5 2.5a3.5 3.5 0 0 1-4.95 0 .751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018 1.998 1.998 0 0 0 2.83 0l2.5-2.5a2.002 2.002 0 0 0-2.83-2.83l-1.25 1.25a.751.751 0 0 1-1.042-.018.751.751 0 0 1-.018-1.042Zm-4.69 9.64a1.998 1.998 0 0 0 2.83 0l1.25-1.25a.751.751 0 0 1 1.042.018.751.751 0 0 1 .018 1.042l-1.25 1.25a3.5 3.5 0 1 1-4.95-4.95l2.5-2.5a3.5 3.5 0 0 1 4.95 0 .751.751 0 0 1-.018 1.042.751.751 0 0 1-1.042.018 1.998 1.998 0 0 0-2.83 0l-2.5 2.5a1.998 1.998 0 0 0 0 2.83Z\"\u003e\u003c/path\u003e\u003c/svg\u003e\u003c/a\u003e\u003c/div\u003e\n\u003cdiv class=\"highlight highlight-source-shell notranslate position-relative overflow-auto\" dir=\"auto\" data-snippet-clipboard-copy-content=\"git clone https://github.com/diesel-rs/metrics/tree/results\nR \u0026lt;\u0026lt; analytics.R\"\u003e\u003cpre\u003egit clone https://github.com/diesel-rs/metrics/tree/results\nR \u003cspan class=\"pl-s\"\u003e\u003cspan class=\"pl-k\"\u003e\u0026lt;\u0026lt;\u003c/span\u003e \u003cspan class=\"pl-k\"\u003eanalytics.R\u003c/span\u003e\u003c/span\u003e\u003c/pre\u003e\u003c/div\u003e\n\u003cp dir=\"auto\"\u003eThe plots are generated inside the \u003ccode\u003eplots\u003c/code\u003e directory. The \u003ccode\u003eaggregated_data\u003c/code\u003e directory contains a summary of the raw criterion results collected in \u003ccode\u003emetrics\u003c/code\u003e\u003c/p\u003e\n\u003c/article\u003e","loaded":true,"timedOut":false,"errorMessage":null,"headerInfo":{"toc":[{"level":1,"text":"Performance statistic collection for rusts database connection crates","anchor":"performance-statistic-collection-for-rusts-database-connection-crates","htmlText":"Performance statistic collection for rusts database connection crates"},{"level":2,"text":"Results","anchor":"results","htmlText":"Results"},{"level":3,"text":"Comparison","anchor":"comparison","htmlText":"Comparison"},{"level":4,"text":"Trivial queries","anchor":"trivial-queries","htmlText":"Trivial queries"},{"level":4,"text":"Medium complex queries","anchor":"medium-complex-queries","htmlText":"Medium complex queries"},{"level":4,"text":"Associations","anchor":"associations","htmlText":"Associations"},{"level":4,"text":"Inserts","anchor":"inserts","htmlText":"Inserts"},{"level":3,"text":"Performance over time","anchor":"performance-over-time","htmlText":"Performance over time"},{"level":4,"text":"Trivial queries","anchor":"trivial-queries-1","htmlText":"Trivial queries"},{"level":4,"text":"Medium complex queries","anchor":"medium-complex-queries-1","htmlText":"Medium complex queries"},{"level":4,"text":"Associations","anchor":"associations-1","htmlText":"Associations"},{"level":4,"text":"Inserts","anchor":"inserts-1","htmlText":"Inserts"},{"level":2,"text":"Raw aggregated data","anchor":"raw-aggregated-data","htmlText":"Raw aggregated data"},{"level":2,"text":"How to generate the plots from the data","anchor":"how-to-generate-the-plots-from-the-data","htmlText":"How to generate the plots from the data"}],"siteNavLoginPath":"/login?return_to=https%3A%2F%2Fgithub.com%2Fdiesel-rs%2Fmetrics"}}],"overviewFilesProcessingTime":0}},"appPayload":{"helpUrl":"https://docs.github.com","findFileWorkerPath":"/assets-cdn/worker/find-file-worker-1583894afd38.js","findInFileWorkerPath":"/assets-cdn/worker/find-in-file-worker-f653046cb227.js","githubDevUrl":null,"enabled_features":{"code_nav_ui_events":false,"overview_shared_code_dropdown_button":false,"react_blob_overlay":false,"copilot_conversational_ux_embedding_update":false,"copilot_smell_icebreaker_ux":true,"copilot_workspace":false,"blob_edit_unsaved_changes_storage":false,"accessible_code_button":true,"overview_branch_and_tag_count":true,"overview_spoofed_commit_banner_react":true}}}} master 2 Branches Tags Go to file Code Folders and files Name Name Last commit message Last commit date Latest commit Bot 📈 Oct 23, 2024 7d4c141 · Oct 23, 2024 History 891 Commits .github/ workflows .github/ workflows Cleanup the master branch Jul 21, 2023 metrics metrics 📈 Oct 23, 2024 Readme.md Readme.md Cleanup the master branch Jul 21, 2023 analytics.R analytics.R Cleanup the master branch Jul 21, 2023 View all files Repository files navigation README Performance statistic collection for rusts database connection crates This repository contains the numbers collected by diesel continuous scheduled benchmark actions to track changes over time. Results Comparison Trivial queries Medium complex queries Associations Inserts Performance over time Trivial queries Medium complex queries Associations Inserts Raw aggregated data Postgresql Sqlite Mysql How to generate the plots from the data git clone https://github.com/diesel-rs/metrics/tree/results
R << analytics.R The plots are generated inside the plots directory. The aggregated_data directory contains a summary of the raw criterion results collected in metrics {"resolvedServerColorMode":"day"} About No description, website, or topics provided. Resources Readme Activity Custom properties Stars 90 stars Watchers 10 watching Forks 2 forks Report repository Releases No releases published Packages 0 No packages published Languages HTML 100.0% Footer © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.
======>
https://www.reddit.com/r/rust/comments/1ewlq9f/web_benchmarks_suggestions/
-->>-->>
Go to rust r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online • 2 mo. ago c410-f3r ADMIN MOD Web benchmarks suggestions wtx recently gained basic support for gRPC connections, which are showing promising runtime results as demonstrated in https://github.com/LesnyRumcajs/grpc_bench/pull/473 . Now it is only a matter of creating a new entry for wtx-bench to total 3 targets (`WebSocket`, `ServerFramework`, `gRPC`) and here comes my question for those interested: Can you indicate more tests to add to wtx-bench ? For example, the eight WebSocket tests have different parameters like the size of the payload but all of them represent an uniform workload that assume a package reception without losses. Should these features be considered? Is there anything else worth evaluating? Any suggestions are welcome. If you are aware of other benchmark suites similar to grpc_bench , please let me know. Thanks Read more Add a Comment Be the first to comment Nobody's responded to this post yet. Add your thoughts and get the conversation going. Top 1% Rank by size Public Anyone can view, post, and comment to this community More posts you may like Related Rust Programming open-source software Technology Free software Software Information & communications technology forward back r/pcmasterrace r/pcmasterrace Welcome to the official subreddit of the PC Master Race / PCMR!

All PC-related content is welcome, including build help, tech support, and any doubt one might have about PC ownership.

You don't necessarily need a PC to be a member of the PCMR. You just have to love PCs. It's not about the hardware in your rig, but the software in your heart!

Join us in celebrating and promoting tech, knowledge, and the best gaming, study, and work platform there exists. The Personal Computer. 13M Members 1.3K Online PC benchmark results below potential 1 upvote · 5 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online FFI Code Is Changing my Perspective On C 149 upvotes · 60 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Help wanted for diff.rs (and new design) 99 upvotes · 7 comments Promoted r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Looking for a maintainer for memmap2-rs 194 upvotes · 21 comments r/batteries r/batteries For questions, news, and discussion about batteries, cells, chargers, charger/inverters, power banks and UPSs. 38K Members 18 Online Battery Benchmarks 2 upvotes r/noteshub r/noteshub This is the official subreddit for NotesHub (https://noteshub.app) - fully cross-platform, vendor-agnostic, markdown based note-taking app with kanban boards and whiteboards support and much more 106 Members 2 Online Suggestions for NotesHub 5 upvotes · 5 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Rust to .NET compiler update - f128, f16, and beginnings of SIMD and async 342 upvotes · 7 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online jeprofl: A low-overhead allocation profiler using eBPF 165 upvotes · 7 comments Promoted r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Memory for Nothing: Why Vec<usize> is (probably) a bad idea pwy 229 upvotes · 106 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online image v0.25.4 brings faster WebP decoding, orientation metadata support, fast blur 101 upvotes · 24 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online PSA: size_of and align_of are in the prelude since 1.80 142 upvotes · 1 comment r/webdev r/webdev A community dedicated to all things web development: both front-end and back-end. For more design-related questions, try /r/web_design. 2.6M Members 195 Online Feedback for an interview homework for webdevs 1 upvote · 2 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Rust support in KDE 170 upvotes · 33 comments r/LocalLLaMA r/LocalLLaMA Subreddit to discuss about Llama, the large language model created by Meta AI. 232K Members 331 Online Any up to date benchmarking sites for coding LLMs 20 upvotes · 12 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online A better way to manage environment variables 🛠️! 131 upvotes · 20 comments r/webdev r/webdev A community dedicated to all things web development: both front-end and back-end. For more design-related questions, try /r/web_design. 2.6M Members 195 Online Feedback for an interview homework for webdevs 3 upvotes · 1 comment r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Designing A Fast Concurrent Hash Table ibraheem 147 upvotes · 15 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online So what do you use for Rust development? 147 upvotes · 218 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Support for SQLite's JSON/JSONB has landed on diesel 123 upvotes · 7 comments r/MQTT r/MQTT Welcome to MQTT - a lightweight IoT messaging protocol optimized for high-latency and unreliable networks. 3.5K Members 3 Online MQTT Broker Benchmarking Tool 3 upvotes · 1 comment r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Rust unix-like OS 321 upvotes · 57 comments r/SurveyExchange r/SurveyExchange Post your survey for others to fill out and fill out one of theirs in return! Pretty much an actual survey exchange. 26K Members 16 Online Repost [Academic] Evaluating To-do Apps Usage and Preferences 1 upvote · 1 comment r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online ureq 3.x release candidate 1 105 upvotes · 11 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 194 Online Rust Search Extension v2.0 has been released 131 upvotes · 9 comments r/SurveyExchange r/SurveyExchange Post your survey for others to fill out and fill out one of theirs in return! Pretty much an actual survey exchange. 26K Members 16 Online Repost [Academic] Evaluating To-do Apps Usage and Preferences 1 upvote · 2 comments Related discussions Best Website Speed Test Best Internet Speed Test Best Fast Browser

======>
https://www.reddit.com/r/rust/comments/15zz8jm/the_fastest_websocket_implementation/
-->>-->>
Related Rust Programming open-source software Technology Free software Software Information & communications technology forward back r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online A rust newbie wrote a websocket client library based on Deno's fastwebsockets. 5 upvotes · 2 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online fastwebsockets A new high-performance WebSocket protocol implementation in Rust github 115 upvotes · 12 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Help me pls understand whatare the benefits of using rust for fast WebSockets server 7 comments Promoted r/Python r/Python The official Python community for Reddit! Stay up to date with the latest news, packages, and meta information relating to the Python programming language. 
---

If you have questions or are new to Python use r/LearnPython 1.3M Members 108 Online This is how i got more performance in WebSockets than Node or Bun using Python 97 upvotes · 16 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online WebSockets Performance: How Many Connections Can I Expect? 3 upvotes · 5 comments r/Python r/Python The official Python community for Reddit! Stay up to date with the latest news, packages, and meta information relating to the Python programming language. 
---

If you have questions or are new to Python use r/LearnPython 1.3M Members 108 Online picows: Fast websocket client and server for asyncio 19 upvotes · 6 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Yet another Web-Socket implementation in rust. 47 upvotes · 24 comments r/golang r/golang Ask questions and post articles about the Go programming language and related tools, events etc. 279K Members 102 Online Looking for lowest-latency WebSocket server/library 11 upvotes · 15 comments Promoted r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Easy and high perform websocket toolkit 18 upvotes · 5 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Write your next Kubernetes controller in rust 141 upvotes · 10 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Symbolica 0.12 has been released! 🎉 257 upvotes · 31 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online 8-bit floating point types for Rust! 107 upvotes · 16 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Rust for Android 426 upvotes · 38 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Learning rust was the best thing I ever did 805 upvotes · 99 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Rust unix-like OS 321 upvotes · 57 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online I made a YouTube video about making my own Engine in Rust 102 upvotes · 28 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online The curse of AI and how Rust helps 153 upvotes · 114 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Rust to .NET compiler update - f128, f16, and beginnings of SIMD and async 342 upvotes · 7 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Managing libs for C++ is just painful... should I switch to rust? 162 upvotes · 150 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Looking for a maintainer for memmap2-rs 194 upvotes · 21 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Rust support in KDE 170 upvotes · 33 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Rust Search Extension v2.0 has been released 131 upvotes · 9 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Utoipa 5.0.0 release - Compile time OpenAPI for Rust 109 upvotes · 17 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Use Type-State pattern without the ugly code 213 upvotes · 51 comments r/rust r/rust A place for all things related to the Rust programming language—an open-source systems language that emphasizes performance, reliability, and productivity. 316K Members 195 Online Drugwars in Rust 181 upvotes · 29 comments
======>
https://github.com/c410-f3r/wtx
-->>-->>
Repository files navigation README Apache-2.0 license WTX A collection of different transport implementations and related tools focused primarily on web technologies. Contains the implementations of 6 IETF RFCs ( 6265 , 6455 , 7541 , 7692 , 8441 , 9113 ), 2 formal specifications ( gRPC , PostgreSQL ) and several other invented ideas. Client API Framework Database Client Database Schema Manager gRPC Client/Server HTTP Client Framework HTTP Server Framework HTTP2 Client/Server Pool Manager UI tools WebSocket Client/Server WebSocket over HTTP/2 Embedded devices with a working heap allocator can use this no_std crate. Performance Many things that generally improve performance are used in the project, to name a few: Manual vectorization : When an algorithm is known for processing large amounts of data, several experiments are performed to analyze the best way to split loops in order to allow the compiler to take advantage of SIMD instructions in x86 processors. Memory allocation : Whenever possible, all heap allocations are called only once at the start of an instance creation and additionally, stack memory usage is preferably prioritized over heap memory. Fewer dependencies : No third-party is injected by default. In other words, additional dependencies are up to the user through the selection of Cargo features, which decreases compilation times. For example, you can see the mere 16 dependencies required by the PostgreSQL client using cargo tree -e normal --features postgres . Since memory are usually held at the instance level instead of being created and dropped on the fly, it is worth noting that its usage can growth significantly depending on the use-case. If appropriated, try using a shared pool of resources or try limiting how much data can be exchanged between parties. High-level benchmarks Checkout wtx-bench to see a variety of benchmarks or feel free to point any misunderstandings or misconfigurations. There are mainly 2 things that impact performance, the chosen runtime and the number of pre-allocated bytes. Specially for servers that have to create a new instance for each handshake, pre-allocating a high number of bytes for short-lived or low-transfer connections can have a negative impact. Low-level benchmarks Anything marked with #[bench] in the repository is considered a low-level benchmark in the sense that they measure very specific operations that generally serve as the basis for other parts. Take a look at https://bencher.dev/perf/wtx to see all low-level benchmarks over different periods of time. Transport Layer Security (TLS) When using a feature that requires network connection, it is often necessary to perform encrypted communication and since wtx is not hard-coded with a specific stream implementation, it is up to you to choose the best TLS provider. Some utilities like TokioRustlsConnector or TokioRustlsAcceptor are provided to make things more convenient but keep in mind that it is still necessary to activate a crate that provides certificates for client usage. Examples Demonstrations of different use-cases can be found in the wtx-instances directory as well as in the documentation. Limitations Does not support systems with pointer length of 16 bits. Expects the infallible sum of the lengths of an arbitrary number of slices, otherwise the program will likely trigger an overflow that can possibly result in unexpected operations. For example, in a 32bit system such a scenario should be viable without swap memory or through specific limiters like ulimit .
======>
https://github.com/nick42d/cosmic-applet-arch
-->>-->>
Repository files navigation README Arch Updates - COSMIC Applet WIP - COSMIC Applet to display Arch Linux package status.
Inspired by https://github.com/savely-krasovsky/waybar-updates and https://github.com/RaphaelRochet/arch-update . arch_updates_rs - Arch updates API Please refer to arch-updates-rs/README.md for more information. How to use The package is in the AUR under cosmic-applet-arch . You can install it via your favourite AUR helper, e.g paru -Syu cosmic-applet-arch . Once installed, the app can be added via the COSMIC Settings app -> Desktop -> Panel/Dock -> Configure panel/dock applets -> Add applet. Features Native COSMIC look and feel, supporting both light and dark mode. pacman, AUR, and devel package upgrades shown. Set up to support localisation - to support your language please submit your .ftl translations to the ./cosmic-applet-arch/i18n/ directory. Modular API arch-updates-rs - able to be used in other similar projects. Development setup Development dependencies are listed on the PKGBUILD in the AUR You can run the following commands to build and install: just build-release
sudo just install
======>
https://old.reddit.com/r/rust/comments/1gbcgw9/the_size_of_trait_objects/
-->>-->>
I do not understand why trait objects are unsized. If you have a trait and you know all the implementations, then you could just enumerate over all the implementing types and take their maximum size as an upper bound of the objects that may be passed to the trait object type. In that case (and it seems this always applies) the trait objects has a finite size and does not need to be on the heap. Why do we still assume a trait object is unsized?   
   

======>
https://old.reddit.com/r/rust/comments/1gau2j9/is_moving_to_rust_from_c_easy/
-->>-->>
I saw some rust examples and I have to say it was rather easy to understand and uncomplicated.. is the rest of the language as “easy” as the rust c website examples make it look?   
   

======>
https://tokio.rs/blog/2024-10-23-announcing-toasty
-->>-->>
Announcing Toasty, an async ORM for Rust October 23, 2024 Toasty is an asynchronous ORM for the Rust
programming language that prioritizes ease of use. Toasty supports SQL and NoSQL
databases, including DynamoDB and Cassandra (soon). Toasty is currently in the early stages of development and should be considered
a "preview" (not ready for real-world usage yet). It also isn't released on
crates.io yet. I am announcing it now as I have made the Github repository open,
will continue development in the open, and am hoping to get feedback. Projects that use Toasty start by creating a schema file to define the
application's data model. For example, this is the contents of the hello-toasty/schema.toasty file. model User { #[key] #[auto] id: Id,

    name: String , #[unique] email: String ,

    todos: [Todo],

    moto: Option < String >,
}

model Todo { #[key] #[auto] id: Id, #[index] user_id: Id<User>, #[relation(key = user_id, references = id)] user: User,

    title: String ,
} Using the Toasty CLI tool, you will generate all necessary Rust code for working
with this data model. The generated code for the above schema is here . Then, you can easily work with the data model: // Create a new user and give them some todos. User:: create ()
    . name ( "John Doe" )
    . email ( "john@example.com" )
    . todo (Todo:: create (). title ( "Make pizza" ))
    . todo (Todo:: create (). title ( "Finish Toasty" ))
    . todo (Todo:: create (). title ( "Sleep" ))
    . exec (&db)
    . await ?; // Load the user from the database let user = User:: find_by_email ( "john@example.com" ). get (&db). await ? // Load and iterate the user's todos let mut todos = user. todos (). all (&db). await . unwrap (); while let Some (todo) = todos. next (). await { let todo = todo. unwrap (); println! ( "{:#?}" , todo);
} Why an ORM? Historically, Rust has been positioned as a systems-level programming language.
On the server side, Rust has grown fastest for use cases like databases,
proxies, and other infrastructure-level applications. Yet, when talking with
teams that have adopted Rust for these infrastructure-level use cases, it isn't
uncommon to hear that they start using Rust more often for higher-level use
cases, such as more traditional web applications. The common wisdom is to maximize productivity when performance is less critical.
I agree with this position. When building a web application, performance is a
secondary concern to productivity. So why are teams adopting Rust more often
where performance is less critical? It is because once you learn Rust, you can
be very productive. Productivity is complex and multifaceted. We can all agree that Rust's
edit-compile-test cycle could be quicker. This friction is countered by fewer
bugs, production issues, and a robust long-term maintenance story (Rust's borrow
checker tends to incentivize more maintainable code). Additionally, because Rust
can work well for many use cases, whether infrastructure-level server cases,
higher-level web applications, or even in the client (browser via WASM and iOS,
MacOS, Windows, etc. natively), Rust has an excellent code-reuse story. Internal
libraries can be written once and reused in all of these contexts. So, while Rust might not be the most productive programming language for
prototyping, it is very competitive for projects that will be around for years. Okay, so why an ORM? A full-featured library ecosystem for the given use case is
a big piece of the productivity puzzle. Rust has a vibrant ecosystem but has
historically focused more on that infrastructure-level use case. Fewer libraries
target the higher-level web application use case (though, as of recently, that
is changing). Also, many of the libraries that do exist today emphasize APIs
that maximize performance at the expense of ease of use. There is a gap in
Rust's ecosystem. Many teams I spoke with reported that the current state of
Rust's ORM libraries is a big friction point (more than one opted to implement
their in-house database abstraction to deal with this friction). Toasty aims to
fill some of that gap by focusing on that higher-level use case and prioritizing
ease of use over maximizing performance. What makes an ORM easy to use? Of course, this is the million-dollar question. The Rust community is still
figuring out how to design libraries for ease of use. Rust's traits and
lifetimes are compelling, can increase performance, and enable interesting
patterns (e.g., the typestate pattern). However, overusing these capabilities also leads to libraries that are
hard to use. So, when building Toasty, I tried to be sensitive to this and focused on using
traits and lifetimes minimally. This snippet is from code generated from the
schema file by Toasty, and I expect this to be the most complicated type
signature that 95% of Toasty users encounter. pub fn find_by_email < 'a >(
	email: impl stmt ::IntoExpr< 'a , String >
) -> FindByEmail< 'a > { let expr = User::EMAIL. eq (email); let query = Query:: from_expr (expr);
	FindByEmail { query }
} This does include a lifetime to avoid copying data into the query builder, and I
am still on the fence about it. Based on user feedback, I might remove lifetimes
entirely in the future. Another aspect of ease of use is minimizing boilerplate. Rust already has a
killer feature for this: procedural macros. Most of you have already used Serde,
so you know what a delight this can be. That said, I opted not to use procedural
macros for Toasty, at least not initially. Procedural macros generate a lot of hidden code at build time. This isn't a big
deal for libraries like Serde because the Serde macros generate implementations
of public traits (Serialize and Deserialize). Users of Serde aren't really
expected to know the implementation details of those traits. Toasty is a different story. Toasty will generate many public methods and types
that you will use directly. In the "Hello Toasty" example, Toasty generates the User::find_by_email method. Instead of a procedural macro, I used an explicit
code generation step, where Toasty generates code to a file you can open and
read. Toasty will try to keep this generated code as readable as possible to
make discovering generated methods easy. This added discoverability will result
in an easier-to-use library. Toasty is still early in development, and the API will evolve based on your
feedback. At the end of the day, if you hit friction, I want to hear about it
and fix it. SQL and NoSQL Toasty supports both SQL and NoSQL databases. As of today, that means Sqlite and
DyanmoDB, though adding support for other SQL databases should be pretty
straightforward. I also plan to add support for Cassandra soon, but I hope
others will also contribute to implementations for different databases. To be clear, Toasty works with both SQL and NoSQL databases but does not abstract away the target database. An application written with Toasty for a SQL
database will not transparently run on a NoSQL database. Conversely, Toasty does
not abstract away NoSQL databases, and you need to understand how to model your
schema to take advantage of the target database. What I have noticed with
database libraries is that most of each library does the same thing, regardless
of the backend data store: mapping data to structs and issuing basic Get,
Insert, and Update queries. Toasty starts with this standard feature set and exposes database-specific
features on an opt-in basis. It will also help you avoid issuing inefficient
queries for your target database by being selective about the query methods it
generates. Next steps You should try Toasty, try the examples, and play around with it. Today, Toasty
is still in active development and not ready for real-world use. The immediate
next step will be to fill those gaps. I am aiming to get Toasty ready for
real-world use sometime next year (realistically, towards the end of the year). Additionally, trying to support SQL and NoSQL the way Toasty does is novel (as
far as I know). If you know prior art, especially pitfalls that previous
attempts have hit, I would love to hear about it. I also know many of you have
strong opinions on working with databases, ORMs, etc., and I am looking forward
to those discussions. There is a #toasty channel in the Tokio Discord for discussion. Also, feel free to create
issues on the Github repo to propose
features or start a conversation about API design and direction. — Carl Lerche ( @carllerche ) Announcing axum 0.7.0 Get Help: Edit this page Announcing Toasty, an async ORM for Rust Why an ORM? What makes an ORM easy to use? SQL and NoSQL Next steps
======>
https://www.iroh.computer/blog/iroh-0-27-0-Squashing-Bugs-And-Taking-Names
-->>-->>
Blog Index iroh 0.27.0 - Squashing Bugs and Taking Names October 24, 2024 by ramfox Welcome to a new release of iroh, a library for building on direct connections between devices, putting more control in the hands of your users. We’ve fixed some bugs, including a yucky one that only showed up on long running nodes (thank you fishfolk for making us aware!), adjusted some config options that made it too easy to point production code to our staging relays, and streamlined adding Discovery services to the iroh_net::Endpoint . ☎️ Stay connected plz An integral part of our hole punching work includes maintaining a connection to one of our relay servers. If our connection to the relay server ever fails, the next time we go to send information to the relay server, we run the connect process over again to ensure we have a connection. This is great if you are on the sending side of a connection, since if anything fails, you immediately attempt to re-connect to the server the next time you send bytes, but if you are on the receiving end, and something fails…you never attempt to re-connect! We’ve refactored this code path to attempt to reconnect when there is a connection failure, and to not wait for the node to need to send data before attempting to reconnect. 💡 Sensible config options can go a looooong way We had a bug that did not select the proper relay server (production vs staging) when building our binaries. So we decided to simplify the logic that does the selection! This resulted in some minor but breaking changes to the existing API. iroh-net now exposes a new function named force_staging_infra in iroh-net/src/relay.rs which abstracts the selection logic. We’ve also removed the TEST_DNS_NODE_ORIGIN and refactored any tests that require a DNS Discovery service to either spin up their own node to publish to or run against already existing infrastructure. We no longer rely on the test-utils feature or the #[cfg(test)] annotations for determining whether code runs against production or staging infrastructure, but only on the IROH_FORCE_STAGING_RELAYS environment variable being set to a non empty value For end users this should have no real effect , but if you do rely on any of these, setting the IROH_FORCE_STAGING_RELAYS environment variable when running in CI or running tests will result in the same behaviour as before! 🪄 Easier Discovery We’ve streamlined adding the “default” Discovery services, when you use the iroh-net crate directly. Before, in order to enable the different default Discovery services on the iroh_net::Endpoint ,  you needed to do the following: let secret_key = SecretKey :: generate (); let discovery = ConcurrentDiscovery :: from_services ( vec! [ Box :: new ( PkarrPublisher :: n0_dns (secret_key . clone ())), Box :: new ( DnsDiscovery :: n0_dns ()), Box :: new ( LocalSwarmDiscovery :: new (secret_key . public ()) ? ), ]); let ep = iroh_net :: Endpoint :: builder () . secret_key (secret_key) . discovery ( Box :: new (discovery)) . alpns ( vec! [EXAMPLE_ALPN . to_vec ()]) . bind () .await Copy Copied! Now, that’s simplified to: let ep = iroh_net :: Endpoint :: builder () . discovery_n0 () . alpns ( vec! [EXAMPLE_ALPN . to_vec ()]) . bind () .await Copy Copied! But wait, there's more! Many bugs were squashed, and smaller features were added. For all those details, check out the full changelog: https://github.com/n0-computer/iroh/releases/tag/v0.27.0 . If you want to know what is coming up, check out the 0.27.0 , and if you have any wishes, let us know about the issues ! If you need help using iroh or just want to chat, please join us on discord ! And to keep up with all things iroh, check out our Twitter . Iroh is a dial-any-device networking library that just works. Compose from an ecosystem of ready-made protocols to get the features you need, or go fully custom on a clean abstraction over dumb pipes. Iroh is open source, and already running in production on hundreds of thousands of devices. To get started, take a look at our docs , dive directly into the code , or chat with us in our discord channel .
======>
https://old.reddit.com/r/rust/comments/1gb9y9l/new_to_rust_and_need_your_experience/
-->>-->>
Hi Rustaceans, I'm a new member and I wan't you to share your experience with me so I can find my way better in learning Rust and getting more used to thinking in Rust. I have experience in Python, developing DeepLearning models in Pytorch. My purpose is to keep practicing Rust and developing in its environment so I can get used to it so I would be happy if you can help me by giving me directions and any resources.   
   

======>
https://lwn.net/Articles/993337/
-->>-->>
On Rust in enterprise kernels Please consider subscribing to LWN Subscriptions are the lifeblood of LWN.net.  If you appreciate this
content and would like to see more of it, your subscription will
help to ensure that LWN continues to thrive.  Please visit this page to join up and keep LWN on
the net. By Jonathan Corbet October 10, 2024 At the recently concluded Maintainers Summit, it was generally agreed that the Rust experiment would
continue , and that the path was clear for more Rust code to enter the
kernel.  But the high-level view taken at such gatherings cannot always
account for the difficult details that will inevitably arise as the Rust
work proceeds.  A recent discussion on the nouveau mailing list may have
escaped the notice of many, but it highlights some of the problems that
will have to be worked out as important functionality written in Rust heads
toward the mainline. The players Some context to begin: the nouveau driver handles NVIDIA
GPUs.  It was, for years, the product of a determined reverse-engineering
effort in the absence of any help from NVIDIA; it was first merged into the mainline for the 2.6.33
release in 2009.  More recently, NVIDIA has
been changing its tune with regard to free-software support for its
products, and has been helping with nouveau development.  Ben Skeggs, who
has worked on nouveau for years, recently took up employment there. Nouveau has reached a reasonable level of functionality, but the developers
in the kernel's DRM (graphics) subsystem are already looking at replacing
it, at least for newer GPUs.  Specifically, the Nova project has been
launched to create a new driver for NVIDIA GPUs going forward.  Unlike
nouveau, Nova will be written in Rust.  The developers involved feel that
using Rust is the best way to cope with the firmware interface in newer
GPUs, which can change at any time without notice.  Nova is a relatively
young project that may not be ready to enter the mainline for, possibly,
some years. An entirely different part of the kernel is the Virtual Function I/O
(VFIO) subsystem .  In short, VFIO is an interface for the control of
I/O memory-management units (IOMMUs) that can be used to safely give access
to a device to a user-space process.  The IOMMU will ensure that the device
only has access to the memory belonging to the process using it, preventing
the device from (accidentally or deliberately) overwriting other parts of
the system.  VFIO is often used in systems running virtualized guests; each
guest can be given access to the devices it needs while keeping the system
as a whole secure. To complete the cast of characters: in late September, Zhi Wang posted a 23-part patch
set implementing a new "vGPU" functionality for NVIDIA GPUs.  There is
an increasing market for cloud-based systems with GPU access so that we can
all enjoy the benefits of large language models appearing uninvited in
every aspect of our lives.  The vGPU patches make it easy for cloud
providers to give virtual machines access to one or more GPUs and to
arbitrate the use of those GPUs when contention occurs.  NVIDIA clearly
thinks, probably with good reason, that this feature will make its GPUs
more attractive to cloud providers, which are known to purchase a
nontrivial amount of hardware.  Befitting NVIDIA's more upstream-friendly
approach, all of this work is aimed at the mainline kernel. Backporting worries This new subsystem, as one might expect, is based on VFIO, meaning that the
interfaces the kernel already exports for virtualized device control will
work with NVIDIA GPUs.  This decision is uncontroversial.  But the vGPU
work also depends on the nouveau driver for access to the hardware, and
makes significant changes to nouveau to add the support it needs.  This
aspect attracted the attention of the developers behind Nova, who were
concerned about basing this functionality on a driver that they are working
to replace.  The plan is that the chipsets to be managed by vGPU will be
driven by Nova, so it is natural to want to see all of this work done there
instead. Danilo Krummrich asked what
the strategy for vGPU was in the longer term: " Is this more of a proof
of concept? Do you plan to work on Nova in general and vGPU support for
Nova? " Jason Gunthorpe, who has seemingly had a hand in the design of
the new subsystem, answered that it " is
intended to be a real product that customers would use, it is not a proof
of concept ".  He is hoping to see it merged soon, he said.  He finished
by saying: " As a commercial product this will be backported extensively
to many old kernels and that is harder/impossible if it isn't exclusively
in C. " In other words, the new vGPU subsystem will be backported by distributors
(and large-scale users) into "enterprise" kernels that, at their core,
predate the introduction of Rust support into the mainline; Gunthorpe estimated that only 10%
of the kernels running on the target systems are based on 6.0 or newer
releases.  Since the initial Rust support only landed in 6.1, and it is far
from complete even now, any sort of Rust dependency will clearly make
backporting vGPU harder, if indeed it can be done at all.  The result could
be that the vGPU subsystem doesn't find its way into those older kernels at
all, which would be disappointing for many of the people (and companies)
involved.  Thus, Gunthorpe concluded, vGPU must remain based on nouveau for
now, and the Nova project has to somehow live with that. Krummrich acknowledged that
using nouveau for now is a reasonable approach, but worried that the
addition of Nova would lead to a duplication of functionality within the
kernel.  To avoid that, he said, there needed to be an agreement that the
vGPU developers would, in the longer term, commit to helping with Nova
development and the movement of vGPU to a Nova base.  Gunthorpe agreed that Nova might
someday become the base for vGPU but pointed out that, while vGPU is a
working driver that may be merged soon, Nova does not yet exist; " let's
not get too far ahead of ourselves here ". Disagreement and options What followed was an occasionally heated discussion where the participants
did not always seem to be understanding each other.  Gunthorpe suggested that a core
driver (written in C) could be created with just enough functionality
to allow higher-level drivers to communicate with the devices; nouveau and
Nova could both sit on top of that core driver.  DRM maintainer Dave
Airlie, though, insisted that the lowest levels need to be in Rust: The core has to be rust, because NVIDIA has an unstable firmware
	API.  The unstable firmware API isn't some command marshalling,
	it's deep down into the depths of it, like memory sizing
	requirements, base message queue layout and encoding, firmware init
	procedures. These are all changeable at any time with no regard for
	upstream development, so upstream development needs to be insulated
	from these as much as possible. Using rust provides that insulation
	layer. He suggested that there could be ways to plan the transition to make life
as easy as possible on the VFIO side.  Gunthorpe answered that: " We
can't have rust in VFIO right now, we don't have that luxury. This is just
a fact, I can't change it. ".  He also said that his objection to Rust
in VFIO would be somewhat reduced " if the backporting can be shown to be
solved ".  Perhaps, he said, a minimal core driver could be written in
Rust as an alternative to a C version, and that could be used to see how
hard the backporting problem really is. With regard to backporting, Greg Kroah-Hartman advised Gunthorpe
to " never make design decisions based on old ancient commercial kernels
that [don't] have any relevance to upstream kernel development today ".
If companies are interested in having vGPU support in enterprise kernels,
he said, they should just pay for the backporting of the necessary patches
into those kernels.  Gunthorpe replied that he is not
ready to accept any such dependency until the kernel community has decided
that Rust support is no longer experimental.  He also expressed hope that
the problem would just go away: This argument is *way too early*. I'm deeply hoping we never have
	to actually have it, that by the time Nova gets merged Rust will be
	100% ready upstream and there will be no issue. Please? Can that
	happen? The conversation eventually just faded away.  In the short term, there are
no decisions that need to be made; the only way for vGPU to work in current
mainline kernels is to use nouveau, so it would be surprising indeed if it
were not merged in that form.  Even the Nova developers have not argued
against that.  In the longer term, with luck, things will play out as
Gunthorpe hopes and, when Nova is ready to go upstream, the course of
action there will be clear as well. Perhaps life will not be quite that easy, but there is reason for optimism
anyway.  While the positions on both sides appeared to be absolute, all of
the developers involved have shown interest in finding solutions that would
work for everybody.  There is no disagreement over the long-term goals that
the community is working toward, and there is reason to believe that an
agreement will be found on how to get there. Some of the themes that came up in this discussion are likely to recur,
though. Any functionality that is dependent on Rust code is going
to be a challenge to backport to ancient enterprise kernels, so merging
that code may draw opposition, even from developers who are supportive of
Rust in general.  At some point, if the Rust project is going to succeed,
the kernel will have to make that leap.  Expect some further discussions,
though, on just when that point should be reached. Index entries for this article Kernel Development tools/Rust Kernel Device drivers/Nouveau to post comments Fade off Posted Oct 10, 2024 14:01 UTC (Thu)
                               by jgg (subscriber, #55211)
                              [ Link ] (1 responses) FWIW, it didn't fully fade off, it moved off list to conference call discussions on the topic. I'm optimistic a mutual win-win can be agreed. Fade off Posted Oct 10, 2024 14:54 UTC (Thu)
                               by rahulsundaram (subscriber, #21946)
                              [ Link ] > I'm optimistic a mutual win-win can be agreed. What does that look like, in your view? A new policy is born... Posted Oct 10, 2024 16:57 UTC (Thu)
                               by jhoblitt (subscriber, #77733)
                              [ Link ] (4 responses) Does the policy of blocking changes to mainline that would be difficult to backport to a release from the previous decade only apply to features implemented in rust or is it a blanket rule? A new policy is born... Posted Oct 10, 2024 17:51 UTC (Thu)
                               by jgg (subscriber, #55211)
                              [ Link ] (3 responses) There has always been a complex ill defined tension between backporters and the mainline. Keep in mind most members of our community participate in backporting at some level. In general you shouldn't be making coding changes to accomodate backporting. That is severely frowned upon, but does ocassionally happen regardless. But, there is a whole set of other grey areas where things get murky. The gcc toolchain has been kept back for alot of different reasons, including backporting. People tend to think carefully before renaming files or moving code around. I've seen patches pushed back on because of code motion harming backporting. For instance if you send a patch to reorder functions to avoid forward declarations then there is a chance that will be refused as being too trival and too harmful. It depends on the maintainer. I would say the basic informal agreement we seem to have is that people have to do the best technical work upstream but that with enough patches the backporting will succeed. Rust is a big, big step up of the backport requirement and I think nobody with customers and timelines funding their projects wants to be forced to be the first one that pipe cleans this. Nobody knows what this looks like, or what is possible, and there is a real fear that the project would end up useless for the real world if backporting fails. A new policy is born... Posted Oct 10, 2024 18:30 UTC (Thu)
                               by raven667 (subscriber, #5198)
                              [ Link ] This all seems reasonable, don't make hard blocking decisions today because of how things might be in a couple years (not saying about bad about R4L or Nova but the future is always uncertain at some level, plans could change), by the time Nova is ready to replace Nouveau it can have vGPU support with all the necessary infrastructure, now the requirement is known, by the time old hardware has aged out of use at the hosting companies offering vGPU and old Enterprise kernels have aged out of new hardware support, it should be possible to have vGPU on Nova for new systems with new Enterprise kernels, vGPU on Nouveau for old ones, everyone wins.  There is probably a period of time where support for old Enterprise kernels with vGPU on Nouveau overlaps with Nova/vGPU that increases the support costs for the vGPU team, but at that time Nouveau is probably in maintenance mode and not getting new features, hardware support, etc. so the support costs probably won't be unmanagable. A new policy is born... Posted Oct 12, 2024 17:26 UTC (Sat)
                               by marcH (subscriber, #57642)
                              [ Link ] > For instance if you send a patch to reorder functions to avoid forward declarations then there is a chance that will be refused as being too trivial and too harmful. It depends on the maintainer. An example that trivial and harmful should really not depend on the maintainer. Do some maintainers want to get rid of stable kernel branches? A new policy is born... Posted Oct 13, 2024 0:28 UTC (Sun)
                               by marcH (subscriber, #57642)
                              [ Link ] So "upstream first" and "just run the latest" (regressions) was not enough, now it's: "vaporware first". Major exaggeration to get the point across, apologies but the idea stands. Rust on <=6.0 kernels is possible Posted Oct 10, 2024 19:48 UTC (Thu)
                               by geofft (subscriber, #59789)
                              [ Link ] (9 responses) Keep in mind that the code that turned into Rust for Linux was originally developed out-of-tree. I was not at all optimistic about it getting upstream quickly (and I'm very grateful to Miguel and the many others who got it there!); I was instead envisioning that out-of-tree module authors, who wouldn't benefit from LKML reviewing their C, might prefer to write those modules in a language where the compiler can check things for you. To that end, the original code wasn't a branch of linux.git; it used the Kbuild support for out-of-tree modules and it had conditional compilation support for kernel 4.4 onwards . Since we're now in fact in a situation where some hardware company has what is effectively an out-of-tree driver (in that it won't be part of -stable for the versions they want to backport to, it'll be some redistributor adding the patches) and there's good reason for that driver to be in Rust, is the right answer to revive that approach? Should we look at extracting the actual Rust for Linux code in mainline into something that can be backported to old kernels? Or maybe more simply, if vGPU is going to be backported as patches on top of an old enterprise kernel (instead of as an out-of-tree module), why can't the Rust support be backported along with it? Rust on <=6.0 kernels is possible Posted Oct 10, 2024 20:09 UTC (Thu)
                               by ballombe (subscriber, #9523)
                              [ Link ] (5 responses) ...because one would need to include a rust compiler in the backport ? Rust on <=6.0 kernels is possible Posted Oct 11, 2024 0:54 UTC (Fri)
                               by geofft (subscriber, #59789)
                              [ Link ] (4 responses) I think it's not uncommon for enterprise/LTS kernels to need a separate GCC to build the kernel than the normal /usr/bin/gcc, whether it's because the distro is backporting a newer stable branch that needs a newer compiler, or because you need an older kernel to be ABI-compatible, or whatever. In particular this happened in the early Spectre days when you needed a newer version of GCC for retpoline support. So I don't think it's particularly unreasonable for a distro to package up a /usr/lib/rust-to-build-the-kernel/bin/rustc or something that's the right version of Rust for everything the kernel needs, make it clear that there's no support for this version of Rust for any other purpose (and in particular that it might get upgraded during the lifetime of the LTS in a way that /usr/bin/rustc wouldn't), and point their kernel build recipe at it. It's a larger-than-usual demand, sure, but it's not a huge demand (nobody's asking for a bunch of userspace Rust libraries along with the compiler), and if there's motivation from paying customers, doing the packaging seems like a relatively small amount of engineering time for a commercial redistributor, at least compared to implementing both the vGPU feature and the shared Rust core that it and Nova can use. Rust on <=6.0 kernels is possible Posted Oct 11, 2024 11:35 UTC (Fri)
                               by pbonzini (subscriber, #60935)
                              [ Link ] I think it _has_ been uncommon for at least 15 years (and was mostly a thing between the release of GCC 3.0 and the release of GCC 3.2). For retpoline support some distros chose not to backport it but wanted the retpolines, so they obviously could not both have their cake and eat it. However, it has been common to use a separate Rust or even C++ compiler to build packages like Firefox, Libreoffice or Qt. Rust on <=6.0 kernels is possible Posted Oct 11, 2024 12:07 UTC (Fri)
                               by pizza (subscriber, #46)
                              [ Link ] (2 responses) > So I don't think it's particularly unreasonable for a distro to package up a /usr/lib/rust-to-build-the-kernel/bin/rustc or something that's the right version of Rust for everything the kernel needs The way things are currently, you would also have to backport a significant portion (if not all) of the R4L core+specific subsystem code that's changed since the destination release, in addition to the core+subsystem code that R4L code interacts with.  (and by "subsystem" I mean _every_ subsystem that your code interacts with, even indirectly...) Rust on <=6.0 kernels is possible Posted Oct 11, 2024 19:44 UTC (Fri)
                               by smurf (subscriber, #17840)
                              [ Link ] (1 responses) I don't think we're still talking about <6.0 kernels by now. Rust on <=6.0 kernels is possible Posted Oct 11, 2024 23:44 UTC (Fri)
                               by jgg (subscriber, #55211)
                              [ Link ] If only that were true :( Rust on <=6.0 kernels is possible Posted Oct 10, 2024 20:11 UTC (Thu)
                               by tzafrir (subscriber, #11501)
                              [ Link ] I'm not sure what you mean. You mean that backporting is going to be handled in this out-of-tree code? I guess it would be nice, but how practical would it be? If you need to support some specific distro kernels, you are probably well aware that the kernel version in its own is not a good test. If not, then the issue of backporting just moved elsewhere. Along with a separate pain of maintaining an out of tree module. Rust on <=6.0 kernels is possible Posted Oct 11, 2024 0:14 UTC (Fri)
                               by jgg (subscriber, #55211)
                              [ Link ] (1 responses) Long ago there was a popular set of OOT backporting helpers maintainted by the wifi group - they used it to backport bleeding edge wifi drivers to distros back when wifi was more problematic. It was a package of header files and copied functions, etc, etc to try to make it easier to just grab driver source from a new kernel and make it work on much older as an OOT package. We might see a similar communal effort for getting a rust backporting helper for OOT as well eventually. Rust on <=6.0 kernels is possible Posted Oct 11, 2024 7:45 UTC (Fri)
                               by johill (subscriber, #25196)
                              [ Link ] It still somewhat exists, and a lot of us still use it internally in some form or other. Occasionally we update it externally, if a community would re-form around it, we could be more proactive with it... However https://backports.wiki.kernel.org/ is pretty outdated/dead. https://git.kernel.org/pub/scm/linux/kernel/git/backports... Insulating layer? Posted Oct 11, 2024 3:40 UTC (Fri)
                               by jkingweb (subscriber, #113039)
                              [ Link ] (129 responses) > The core has to be rust, because NVIDIA has an unstable firmware API. The unstable firmware API isn't some command marshalling, it's deep down into the depths of it, like memory sizing requirements, base message queue layout and encoding, firmware init procedures. These are all changeable at any time with no regard for upstream development, so upstream development needs to be insulated from these as much as possible. Using rust provides that insulation layer. I'm not much of a programmer, so maybe I'm just hopelessly ignorant, but how does Rust provide that insulation layer? Why cannot C do so as well? I understand that Rust's type system can hide gory implementation details to usually provide a simpler API,  but how does that relate to firmware API changing out from under the driver's feet? The article presents the quotation uncritically, and perhaps rightly so, but I think I might benefit from an explanation. Insulating layer? Posted Oct 11, 2024 5:41 UTC (Fri)
                               by tchernobog (subscriber, #73595)
                              [ Link ] (4 responses) I also would welcome a more detailed explanation on this point. I suspect it has to do with generics and Rust macros for the generation of large swats of serialization or other code in a type-safe manner, but I am not sure. Insulating layer? Posted Oct 11, 2024 7:03 UTC (Fri)
                               by mmechri (subscriber, #95694)
                              [ Link ] (2 responses) I think this was briefly mentioned in a recent article: https://lwn.net/Articles/990736/ It seems like each firmware version may introduce changes that the driver must be able to deal with. As a non-Rust developer, I would also be interested in learning how Rust makes it easier to deal with this. Insulating layer? Posted Oct 11, 2024 9:06 UTC (Fri)
                               by k3ninho (subscriber, #50375)
                              [ Link ] I can't find the citation, but ISTR an Asahi Linux blog post about the macOS packages containing firmware updates, and the new firmware using arbitrary interfaces. This was a problem because an update can cause the same hardware to need different ABI layout to set it up appropriately. I think the text said there's code to solve this problem -- and in a Rust ecosystem, if only scripts and tooling. K3n. Insulating layer? Posted Oct 11, 2024 10:37 UTC (Fri)
                               by farnz (subscriber, #17727)
                              [ Link ] The big thing that Rust makes easier is writing performant code that's structured as "parse external form into suitable internal structures, work with internal structures, serialize into external form". The type system then allows you to make sure that your invariants aren't ever broken inside the internal structures (absent the use of unsafe , which you can catch and worry about during code review). Rust enums (sum type) are powerful for representing choices, because enum variants can be structs, not just integers, while the affine typing rules Rust uses mean that you can guarantee that if someone wants to get an object of a specific type, they've given you ownership of an object of a different type, which allows you to prevent double-action of many forms (double-freeing is the famous one, but any form of double action on a type that shouldn't allow it is something you can prohibit). Insulating layer? Posted Oct 11, 2024 15:11 UTC (Fri)
                               by jgg (subscriber, #55211)
                              [ Link ] The issue is that the FW ABI has a RPC based on a fixed layout struct like interface and NVIDIA re-organizes the struct randomly becauase they don't (yet?) care. So imagine you have to make a function: marshall_foo(int a, int b) To produce the RPC struct to deliver to the FW. Then FW version A has struct foo_A { u32 a; u32 b; }; And FW version B has struct foo_B { u32 c; u64 a; u32 b; }; So, I can't say exactly what part of Rust is making this better. I've assumed it is built around generics to allow source code sharing and still robustly enforce the memory layout. However, any programming language has hard limits, it can't address semantic changes. In the above example you hope that 'c' can remain zero, and maybe have to deal with the reason 'a' got bigger. Notice this is not about complex serializing or dynamic memory, or any of the other things rust is really strong at. Just knowing a giant table of offsets and sizes.. From a C perspective, I'd solve this class of problem with either a code generator or a multi-compilation scheme (similar to generics). iommu_pt and the MM are examples effectively solving a similar problem with non-semantic differences in page table entry memory layout, for instance. Insulating layer? Posted Oct 11, 2024 7:00 UTC (Fri)
                               by NYKevin (subscriber, #129325)
                              [ Link ] I suspect it is some combination of the following: * Rust has visibility restrictions and C (mostly) does not, so forcing C code to never rely on the width or layout of a type is harder (read: de facto impossible in the limit as the size of the codebase grows to infinity). I would tend to assume that this is by far the most important reason, and all of the other reasons are just bonus points. * Safe Rust is not capable of overflowing a buffer without help from (incorrectly-written) unsafe Rust. So if your buffer changes size out from under you, and your code does not handle that correctly, you might end up panicking the kernel, but at least you won't get your device pwned (unless there is something wrong in an unsafe block, but those are hopefully small, infrequent, and audited more carefully than the rest of the codebase). * Rust can represent and statically check more arbitrary requirements (such as "must call the init method before you can call anything else, must not call anything after shutdown," etc.) with typestates, which are not practical in C for multiple reasons (no real ZST support, no visibility restrictions, no move-only types, etc.). (Nitpicking: C does support opaque structs, which is a very basic all-or-nothing visibility restriction. But opaque structs have a plethora of asterisks attached to them that Rust structs do not. Most importantly, you don't know their size at all, so you can't stack allocate them, memcpy them, or do much of anything with them other than pass pointers around. I would also tend to worry about whether it defeats inlining in some cases, but I'm not a compiler expert so I could be mistaken about that.) Insulating layer? Posted Oct 11, 2024 8:19 UTC (Fri)
                               by GhePeU (subscriber, #56133)
                              [ Link ] Asahi Lina, who developed the Rust kernel driver for the Apple GPUs, described the rationale behind that choice in a blog post a couple years ago: https://asahilinux.org/2022/11/tales-of-the-m1-gpu/ . I vaguely remember reading a slightly more technical explanation somewhere but I can't find it now, maybe it was posted on social media or in a mailing list. Anyway I think the Nova developers have the same constraints so their reasoning must be more or less the same too. Insulating layer? Posted Oct 11, 2024 8:32 UTC (Fri)
                               by smurf (subscriber, #17840)
                              [ Link ] That's the same argument for Rust as, well, everywhere else basically. Rust provides a bunch of guarantees that C does not have a concept of, which thus are not checkable at compile time. The practical effect is that a significantly large class of problems just disappears (OK not by itself, you need to heed the compiler warnings and fix them), to the tune that Asahi Lina wrote a complete and mostly-bug-free driver for Apple's M1 basically from scratch in a few weeks (while she actually needed months in real time, she also hat to write all the infrastructure). This kind of feat is basically impossible in C. The people who insist of that part being written in Rust are *right*. It's less work to backport the whole Rust infrastructure to an older kernel, if you really think you have to do that (in a sane world you most emphatically would not), than to write that code in C. Not if you consider the subsequent hunting of bugs. Insulating layer? Posted Oct 11, 2024 13:43 UTC (Fri)
                               by khim (subscriber, #9252)
                              [ Link ] (115 responses) > I'm not much of a programmer, so maybe I'm just hopelessly ignorant, but how does Rust provide that insulation layer? With the use of a typestate pattern . Languages that fully support a typestate can statically encode, in a type system, runtime invariants, but are, usually, very hard to use and reason about for the uninitiated. Rust also started there and if it would have stayed there it would have withered on the vine, most likely. Typestate was removed from it during Rust evolution, but it left behind Rust's borrow checker. Each variable in Rust can be in precisely two states: Initilialized and valid to use Not yet initilialized or dead ( moved out ) thus invalid to use Surprisingly enough that's enough, in practice, to encode very convoluted rules that program that interacts with a firmware (or a physical device) have to follow. > Why cannot C do so as well? None of popular languages can do that because they lack a typestate. The simplest, cleanest example that showcases what they are lacking is growable buffer. In C one would implement it with malloc / realloc / free while in Rust it's represented by Vec . And both languages provide two mutually incompatible sets of operations: Direct access to elements and ranges of elements (called slices in Rust), and Set of function that can grow said buffer and thus can move it in memory If we want to avoid bugs in our programs, then we should ensure that no one have references to elements stashed anywhere when we try to resize the buffer. This sounds like a simple, almost trivial requirement, but Rust is almost unique among popular languages because it actually can represent this restriction in it's API. I think the only other one is Ada (via SPARK ) – and Ada very explicitly and consciously picked that ability from Rust . > The article presents the quotation uncritically, and perhaps rightly so, but I think I might benefit from an explanation. Wouldn't that be a bit silly? We are talking, quite literally, about something that Rust was born to solve, about Rust's raison d'être, about something that shapes the whole language from the bottom to the top and makes it unique. This ability is core to what almost everyone who ever worked with Rust explicitly notes: ease of refactring. Something that even A Critical Retrospective puts under Where Rust Exceeded Expectations banner. Adding explanation about core thing that makes Rust… well… Rust – to every article? It would be like adding explanation about how planes are able to fly to every article that discusses advantages and disadvantages of different modes of transportations. Yes, planes can fly (and pay very high price for that ability), while ships and cars couldn't do that. Heck, the whole reason to add to the kernel Rust is that same ability! It Rust couldn't deliver on that promise then there are no need to even talk about adding it to the kernel! Insulating layer? Posted Oct 11, 2024 13:58 UTC (Fri)
                               by jkingweb (subscriber, #113039)
                              [ Link ] > Adding explanation about core thing that makes Rust… well… Rust – to every article? It would be like adding explanation about how planes are able to fly to every article that discusses advantages and disadvantages of different modes of transportations. I wasn't suggesting the article necessarily lacked anything—that's precisely why I speculated the article may not have erred in omitting more detail. Rather, it was my understanding which was clearly lacking, and apparently I wasn't the only on in this situation. I am vaguely familiar with what advantages Rust has, but the implications of those advantages are not necessarily clear to everyone all the time. Insulating layer? Posted Oct 11, 2024 14:19 UTC (Fri)
                               by MKesper (subscriber, #38539)
                              [ Link ] (112 responses) I think your explanation is very good and touches something that's not so commonly understand as you think. As far as I can see advantages of Rust often get reduced to "memory safety, we could get that by using better tools for C/C++". Insulating layer? Posted Oct 11, 2024 19:41 UTC (Fri)
                               by smurf (subscriber, #17840)
                              [ Link ] (110 responses) > "memory safety, we could get that by using better tools for C/C++". Mwa-har-har. Some C++ people try to tell us that this is possible and/or they're working on it. Clue: it will never happen. More to the point, it cannot happen, both for technical and social reasons. Insulating layer? Posted Oct 12, 2024 5:44 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [ Link ] (109 responses) I think this depends on how much of the problem you want to solve, how badly you want to solve it, and the extent to which you are empowered to point at language constructs and say "don't use that if you want to be safe." And I know that the latter sounds an awful lot like moving the goalposts, but if you have static linting for it (that actually works), then it's not materially different from Rust's unsafe blocks (aside from being uglier and harder for a human to identify). For example, a large class of memory safety issues will magically vanish if you require that all buffers are statically allocated with some fixed size, and malloc et al. are not used. Nobody actually wants to write code like that, so you pretty much only see it in highly failure-sensitive domains like aerospace. But it is technically a "safe subset" of C. But you're going to object that that is obviously not what the C++ folks are talking about. Fine. Let's talk about borrow checking. The modern Rust borrow checker did not emerge fully formed directly from Hoare's brain. It went through a lengthy period of iteration and refinement. It is possible to reinvent less powerful interpretations of borrow checking based on those earlier iterations. For example, you could just have lexically-scoped borrows (the borrow is not allowed to escape a given lexical scope) plus a syntax rule that prohibits moving a borrowed object within the borrow's lexical scope. This is not fully general, and fails to express many things that Rust can handle easily, but when you combine it with C++'s existing smart pointers, it is probably enough for many practical applications (that don't need ultra-low overhead). Perhaps that's too weak. We probably do want the ability to put std::string_view inside of a data structure, and this would ban that as an escaping borrow. There may be ways to strengthen it that are still workable within the context of C++. But we could instead argue that std::string_view is the Wrong Abstraction for safe C++, and they should instead be using some kind of refcounted string view (i.e. each view owns a strong reference to the refcounted string), or perhaps a copy-on-delete string view of some kind (i.e. a string view that is informed when the string is about to be destructed, and allowed to copy the relevant portion of the string into a new heap allocation). Or maybe they'll do something really crazy, such as deprecating std::string altogether and replacing it with a refcounted copy-on-write rope structure. I don't seriously think that will happen, but at least it would (probably) solve the view issue (and it would be very amusing watching people get upset about it on the internet). Insulating layer? Posted Oct 12, 2024 6:47 UTC (Sat)
                               by mb (subscriber, #50428)
                              [ Link ] (38 responses) If you put refcounting everywhere, you are making the language a lot slower and it would still *not* be fully safe. Think about threads. There is no such thing as a "safe subset" of C++ and there never will be. If you create a "safe subset", then it will be a completely different language that people will have to learn. Why bother? You would still end up with a language containing massive amount of obsolete backward compatibility stuff next to your new "safe subset" language. Just switch to a safe language (for new developments). It's much simpler. And it's much safer. Insulating layer? Posted Oct 12, 2024 16:45 UTC (Sat)
                               by NYKevin (subscriber, #129325)
                              [ Link ] I'm not saying I think this is a good idea, and I'm certainly not trying to argue that it will be a serious competitor to Rust in terms of overall safety. I'm saying that the very same political factor you identify (massive legacy codebases) will also create great pressure for C++ to provide a "safe profile" for use in contexts where Compliance™ is mandatory. It is reasonable to assume that the smart people who are working on this will end up producing something that has at least some improvements in safety over the status quo (which is a very low bar for C++). Compare and contrast MISRA C, which has the advantage of starting from a much simpler language, but the disadvantage of not having smart pointers for simple "object X owns object Y" cases. Insulating layer? Posted Oct 14, 2024 9:31 UTC (Mon)
                               by paulj (subscriber, #341)
                              [ Link ] (36 responses) Isn't Rust basically a ref-counting language, for long-lived programmes with non-trivial data relationships? Sure, for a compute-and-complete programme, processing a finite input to give some result, Rust programmes can allocate from the stack, and/or have acyclically-linked heap objects with tight lifetimes. But... for anything long-lived with complex relationships between different data objects, isn't the pattern in Rust to use Rc/Arc? I.e., refcounting? Insulating layer? Posted Oct 14, 2024 10:26 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (14 responses) Your long-lived data is often refcounted (especially in async Rust) because that's simpler to get right than rearranging so that your data doesn't have complicated lifetime relationships. But your short-lived accesses to that data usually borrow it, rather than refcounting it, because their relationship to the bit of data they need is simple. And that's something that's not easy to express in Python - the idea that I borrow this data in a simple fashion, even though the structure as a whole is complicated and refcounted. Insulating layer? Posted Oct 14, 2024 11:00 UTC (Mon)
                               by paulj (subscriber, #341)
                              [ Link ] (13 responses) I have tried playing with Rust. There's a lot to like. There's some stuff I dislike - the macro system especially makes me go "Wut?", Zig's approach of allowing the /same/ language to be used to compute at compile time looks to be much more ergonomic (though, havn't tried Zig yet). In the little I've played with Rust, the lifetime annotations make sense on simpler stuff. But it seems to get very baroque on more complex stuff. And I havn't gotten over a certain "barrier" on how to design the not-simpler kinds of inter-linked relationships that are unavoidable in real-world stuff. E.g. a network protocol, you may have objects and state for "peers", "connections", "TLS contexts", "Streams", "message entity" (plurality of these) with m:n relationships between them. A peer may have multiple connections, messages may have a lifetime related to a /set/ of peers and not just the peer it was received from, etc. You have to use RC for at least some of these - so now you're dealing with /both/ refcounting AND lifetime annotations for the refcounting objects (in other languages you would embed the RC information in the refcounted object, but in Rust the RC object holds the refcounted object), and there are rules for getting data into and out of RCs and cells and what not. And I end up with long screeds of compiler error messages that give me flashbacks to C++ - maybe Rust's are better, but I've been traumatised. The complexity of implementing simple linked lists in Rust, without unsafe, is a bit of a red flag to me. I think the local stack variable lifetimes possibly could be dealt with in simpler ways, perhaps with more limited annotations, or perhaps none at all but just compiler analysis. It might be more limited than Rust, but be enough to deal with typical needs for automatic data in long-lived programmes. What I'd like, for long-lived programmes with non-simple relationships in their data model (e.g., many network protocols) is a language that did ref-counting /really/ well - ergonomically, with low programming complexity, and minimal runtime overhead. Cause refcounting is how all non-trivial, long-lived programmes, that can not tolerate scannning GC, end-up managing state. Maybe I'm just too stupid for Rust. Like I'm too stupid for Haskell. Maybe refcounting is the right answer for my lack of intelligence - but then it seems many other programmers are like me. And it seems maybe even the best Rust programmers too, given how they always end up reaching for (A)RC too. Insulating layer? Posted Oct 14, 2024 11:28 UTC (Mon)
                               by mb (subscriber, #50428)
                              [ Link ] >the macro system especially makes me go "Wut?", Zig's approach of allowing the /same/ language >to be used to compute at compile time looks to be much more ergonomic (though, havn't tried Zig yet). Rust can do that, too. It's called proc-macro Challenges learning how to model data structures in Rust Posted Oct 14, 2024 12:26 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (10 responses) One of the challenges I found in moving from C++ to Rust is that in C++, you model ownership in your head; you sort-of know whether a given pointer represents owning something, shared ownership, or simply referring to something someone else owns. Rust doesn't let you play fast-and-loose with ownership like this; the borrow checker means that you have to be extremely clear about who owns what, and when, using T for "this T is embedded here", Box<T> for "this T is owned, but is stored out of line", and Arc / Rc<T> for "ownership of this T is shared". You can also write a simple linked list in 40 lines of Rust without any unsafe at all - the use of unsafe is because you want a complicated linked list that meets more interesting properties than the simple linked list implementation does. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 9:09 UTC (Thu)
                               by paulj (subscriber, #341)
                              [ Link ] (9 responses) I was referring more to "simple" doubly-linked lists on the complexity side. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 9:39 UTC (Thu)
                               by farnz (subscriber, #17727)
                              [ Link ] (8 responses) Making a doubly-linked list in safe Rust isn't that much more complicated, although it does bring in the need for Rc , since a doubly-linked list has complex ownership (the previous pointer cannot be owning). Challenges learning how to model data structures in Rust Posted Oct 17, 2024 10:10 UTC (Thu)
                               by paulj (subscriber, #341)
                              [ Link ] (2 responses) Yes, course you can do with with RC. But RC is a "fuck it, we'll do it live!" escape hatch to runtime, out from the compile-time lifetime typing system, To stay within the typed lifetime-scope of Rust, without unsafe, is somewhere between extremely complex and impossible, isn't it? (I thought impossible, but then I thought I saw a tutorial claiming it was possible once - but I can't find it back right now). Challenges learning how to model data structures in Rust Posted Oct 17, 2024 14:23 UTC (Thu)
                               by farnz (subscriber, #17727)
                              [ Link ] (1 responses) The difficult bit is the ownership model if you don't use refcounting; you want each node to be individually mutable, but you also want each node to be shared by both the previous and the next nodes. That's inevitably difficult in a "shared, or mutable, but not both" model as Rust imposes, unless you use refcounting. Challenges learning how to model data structures in Rust Posted Oct 18, 2024 14:38 UTC (Fri)
                               by taladar (subscriber, #68407)
                              [ Link ] Something would probably be possible with interior mutability but it likely would need locking or some other internal mechanism to prevent mutation from both ways to access it at the same time. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 11:48 UTC (Thu)
                               by excors (subscriber, #95769)
                              [ Link ] (4 responses) That looks like a poor example - it doesn't implement any operations beyond "new" and "append", so it never even uses its "prev" pointers. The much more thorough exploration at https://rust-unofficial.github.io/too-many-lists/ calls a similar Rc<RefCell<...>> design "A Bad Safe Deque" and notes "It was a nightmare to implement, leaks implementation details, and doesn't support several fundamental operations". Eventually it gets to "A Production Unsafe Deque", which looks pretty complicated. Singly-linked lists are much easier (though still not trivial), the double-linking is the big problem. (It also explains: > Linked lists are terrible data structures. Now of course there's several great use cases for a linked list: [...] But all of these cases are _super rare_ for anyone writing a Rust program. 99% of the time you should just use a Vec (array stack), and 99% of the other 1% of the time you should be using a VecDeque (array deque). These are blatantly superior data structures for most workloads due to less frequent allocation, lower memory overhead, true random access, and cache locality. > > Linked lists are as _niche_ and _vague_ of a data structure as a trie. Few would balk at me claiming a trie is a niche structure that your average programmer could happily never learn in an entire productive career -- and yet linked lists have some bizarre celebrity status. Generally, you probably shouldn't judge a language on the basis of how easily it can implemented linked lists. On the other hand, one of the listed "great use cases" is "You're writing a kernel/embedded thing and want to use an intrusive list", which is probably much more common amongst LWN readers than your average programmer; it's fair to have different priorities when judging languages. But I think it's still important to consider whether your design uses (doubly-)linked lists because they really are the best data structure for your needs, or whether it's just because linked lists are really easy to implement in C (and an equivalent of Vec is hard) so they're your default choice for any kind of sequential container, and you're trying to apply the same mindset to Rust where they're really hard (but Vec is easy) and they should never be the default choice.) Challenges learning how to model data structures in Rust Posted Oct 17, 2024 12:22 UTC (Thu)
                               by khim (subscriber, #9252)
                              [ Link ] > But I think it's still important to consider whether your design uses (doubly-)linked lists because they really are the best data structure for your needs, or whether it's just because linked lists are really easy to implement in C (and an equivalent of Vec is hard) so they're your default choice for any kind of sequential container, and you're trying to apply the same mindset to Rust where they're really hard (but Vec is easy) and they should never be the default choice.) Tragedy of lists (and the reason Rust had to wait till 2015 to be viable) lies with the fact that while double-linked lists are very bad data structure today – that wasn't the case half-century ago when Computer Science was established and first courses that teach people how to program were developed. Back then computers big, but their memories (and thus programs written for these computers) were small, while RAM was fast and CPU slow. Linked lists really shined in such environment! Today computers are tiny, but have memory measured in gagabytes, programs are crazy big and CPUs are hudnreds of times faster than RAM. Today linked lists are almost never the right data structure for anything and even when they actually better than something like Vec the difference is not large. But people are creatures of habits: after they have learned to deal with linked lists once they tend not to ever return back and use them as long as they could. Worse yet: while it's true, today, that Vec is the way to go and linked lists are [mostly] useless… there wasn't any point in time when linked lists stopped being useful and Vec started shining. Transition was very slow and gradual: first Vec was awful and linked lists great, then Vec have become a tiny bit less inefficient and thus become better for some tasks, while linked list have become less efficient and thus worse for these some task, etc. Thus even today, long after linked lists and all tricks associated with them stopped being important in 99% of cases, people try to apply what they know to Rust. And yes, that inevitably sends us back to one funeral at time story. Which is sad, because that means that new generation would have to relearn everything again: while linked lists are no longer useful many other lessons that “old beards” learned are still usefull… but if C/C++ would be replaced with Rust (or any other safe language) via one funeral at time way then all these lessons would be forgotten… and thus repeated. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 12:42 UTC (Thu)
                               by smurf (subscriber, #17840)
                              [ Link ] Well, the kernel contains a veritable heap of linked lists, so we're stuck with them. The key isn't to implement a doubly-linked-list in Safe Rust, which is not exactly a workable idea for obvious lifetime reasons, but to hide them behind an appropriate abstraction. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 14:00 UTC (Thu)
                               by paulj (subscriber, #341)
                              [ Link ] (1 responses) I agree linked-lists are a terrible data-structure. But that isn't really the point here. Doubly linked lists are just a well-understood form of data-structure with cycles. Cycles in data-representations in non-trivial programmes are common, even if those programmes (rightly) eschew linked-lists. The linked-list question speaks to the ability to do common kinds of data modelling. Focusing on the linked-list part misses the point. Challenges learning how to model data structures in Rust Posted Oct 17, 2024 15:57 UTC (Thu)
                               by mb (subscriber, #50428)
                              [ Link ] > Focusing on the linked-list part misses the point. I think that focussing on everything must be implemented in safe Rust is missing the point of Rust. Yes, many things *can* be implemented in safe Rust, but that doesn't mean they should. For implementing such basic principles as containers and lists, it's totally fine to use unsafe code. If you can drop reference counting by implementing it in unsafe code and manually checking the rules, then do it. This is actively encouraged by the Rust community. The only thing that you must do is keep interfaces sound and do all of your internal manual safety checks. Therefore, I don't really understand why reference cycles or linked lists are discussed here. They are not a problem. Use a safe construct such as Rc or use unsafe. It's fine. Insulating layer? Posted Oct 20, 2024 11:15 UTC (Sun)
                               by ssokolow (guest, #94568)
                              [ Link ] I have tried playing with Rust. There's a lot to like. There's some stuff I dislike - the macro system especially makes me go "Wut?", Zig's approach of allowing the /same/ language to be used to compute at compile time looks to be much more ergonomic (though, havn't tried Zig yet). The declarative macro syntax is an interesting beast because it's somehow so effective at disguising that a macro is just a match statement to simulate function overloading with some Jinja/Liquid/Twig/Moustache/Handlebars/etc.-esque templating inside, disguised by a more Bourne-like choice of sigil. ...maybe because it's really the only place in the language where you have to code in a Haskell-like functional-recursive style instead of an imperative style if you want to do the fancier tricks? In the little I've played with Rust, the lifetime annotations make sense on simpler stuff. But it seems to get very baroque on more complex stuff. True. On the plus side, they are still actively working to identify places where they can remove the need for weird tricks. See, for example, the Precise capturing use<..> syntax section in this week's announcement of Rust 1.82.0. Maybe I'm just too stupid for Rust. Like I'm too stupid for Haskell. Maybe refcounting is the right answer for my lack of intelligence - but then it seems many other programmers are like me. And it seems maybe even the best Rust programmers too, given how they always end up reaching for (A)RC too. One of Rust's greatest weaknesses has always been that its "make costs explicit" philosophy has created a language that encourages premature optimization, and people in the community concerned with writing learning materials spend an inordinate amount of time encouraging people to write quick and dirty code (eg. String instead of &str , .clone() everywhere, etc.) first, learn the more advanced stuff later, and always profile before optimizing. (Of course, to be fair, the same thing occurs more subtly at lower levels with people misjudging the cost of a given machine language opcode and how superscalar architecture will affect costs. It just doesn't result in compiler errors.) There's no shame in reaching for a reference-counted pointer. (I'm no C++ programmer, but I've never seen any signs of this much agonizing over std::shared_ptr .) However, for better or for worse, having people averse to it does seem to contribute to the trend that Rust implementations of things have low and stable memory usage. Insulating layer? Posted Oct 14, 2024 10:31 UTC (Mon)
                               by excors (subscriber, #95769)
                              [ Link ] (18 responses) A little bit, but that's not the same as "refcounting everywhere". You might store long-lived complex-ownership state in an Rc/Arc, but I think you typically wouldn't use the Rc much within your code - you'd (automatically) deref it into an uncounted &T reference before passing it down the call stack and operating on it, with the borrow checker ensuring the &T doesn't outlive the Rc<T> it originated from. That means you're rarely touching the reference count (avoiding the performance problems it causes in languages like Python), but you still get the lifetime guarantees (unlike C++ when you mix shared_ptr with basic references and risk use-after-free). Insulating layer? Posted Oct 14, 2024 11:03 UTC (Mon)
                               by paulj (subscriber, #341)
                              [ Link ] (17 responses) That's a nice comment on how RC and lifetimes can complement each other. Thanks. I'm still left a bit uneasy about the complexity on all the rules on getting data in and out of RCs and cells and what not, and implementing non-trivial relationships in data, in Rust. It's not a simple language. Insulating layer? Posted Oct 15, 2024 8:20 UTC (Tue)
                               by taladar (subscriber, #68407)
                              [ Link ] (16 responses) You are confusing the complexity of the problem with the complexity of the language. The problem of ownership is complex, Rust helps you deal with it by making it explicit, other languages just let you make mistakes without providing any help with it at all. Insulating layer? Posted Oct 15, 2024 9:15 UTC (Tue)
                               by paulj (subscriber, #341)
                              [ Link ] (15 responses) The way I code in C is that I reduce the complexity of the ownership problem by constraining myself to what I am allowed to do: - references may be "owning" or "shared" - An "owning" reference may only ever be stored to automatic, local variables. - A "shared" reference is ref-counted, and can be stored to multiple variables (using whatever refcounting machinery/helpers I've put in place) Rust automating these rules and enforcing them, AND allowing me to safely pass a "owning" reference to other functions, taking care of passing the ownership in the process, is nice. In my way of doing things, I will often have to use local helper functions that take "owning" references to do stuff with them. And I have to myself ensure I don't store those pointers somewhere. But, that's what I have to do anyway. Rust enforcing this, unless code is marked 'unsafe' seems nice. But... for me... Rust's lifetime type system allows too much. I can see it would let me do more than what my own heavily constrained system allows, and with guarantees, but it also drags in a lot of complexity. Complexity I already made a conscious decision to avoid. A language with a much more limited set of scoping rules for pointers - more like the system I've already been using for years in unsafe languages - could be simpler and just as effective, possibly. Insulating layer? Posted Oct 16, 2024 7:44 UTC (Wed)
                               by taladar (subscriber, #68407)
                              [ Link ] (14 responses) As long as you are the only one working on your code and you don't make mistakes you might be able to rely on your assumptions but I think the strength of Rust (and similar strict languages) is that they can guarantee that as long as there aren't any compiler bugs all programmers working on your project, including you on a bad day where you do make mistakes, will follow the rules. Insulating layer? Posted Oct 17, 2024 9:19 UTC (Thu)
                               by paulj (subscriber, #341)
                              [ Link ] (13 responses) Yes, I get that. Rust can enforce rules, and that's great. My point is more that my own system of ad-hoc, non-compiler-enforced rules for being able to manage the problem of lifetimes of objects and their references is simpler than Rusts'. In my system I basically try to have just 2 scopes for references - the very local, and then the refcounted. The latter are "safe" use-after-free issues (I don't use usually use refcounting machinery that deals with concurrency, but you could). Rust goes further than that, and provides for arbitrary scopes of lifetimes for references to different objects. Great, lot more powerful. But... it also becomes harder to reason about when you start to make use of that power, it seems to me. I can't keep track of more than a couple of scopes - that's why my ad-hoc safe-guards in lesser languages are so simple, and I think it's part of why I struggle to get my head around Rust's error messages when I try write more interlinked, complex data-representations, in my own toy test/learn coding attempts.  The fact that a lot of code ends up going back to refcounted containers suggests I might not be alone, not sure. What I'm saying is that simple programmers like me might be better off with a "safe" (i.e. enforcing the rules) language with a more constrained, simpler, object lifetime management philosophy. Insulating layer? Posted Oct 17, 2024 10:36 UTC (Thu)
                               by Wol (subscriber, #4433)
                              [ Link ] (12 responses) This sounds a bit like alloca? I wonder. Could you create a "head" object who's lifetime is the same as the function that created it, and then create a whole bunch of "body" objects that share the same lifetime? These objects can now reference each other with much fewer restrictions, because the compiler knows they share the same lifetime. So there's no problem with the head owning the next link down the line etc etc, and each link having a reference to the link above, because they'll all pass out of scope together? Of course that means you can't access that data structure outside the scope of the function that created it, but that would be enough for a lot of purposes? Cheers, Wol Insulating layer? Posted Oct 17, 2024 11:45 UTC (Thu)
                               by smurf (subscriber, #17840)
                              [ Link ] (2 responses) > that would be enough for a lot of purposes? That would also be rather pointless for a lot of other purposes (among them: freeing a bunch of body objects, oops you suddenly have scoping problems anyway), so what's the point? Insulating layer? Posted Oct 17, 2024 12:25 UTC (Thu)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) > so what's the point? It feels pretty simple to me - cf paulj's comment that full blown Rust just seems to blow his mind. Does Rust actually have a memory allocator, or does it have a "new"? That was the point of my mention of alloca - as the caller goes out of scope, any memory allocated by alloca goes out of scope as well. You don't free alloca memory. Could the Rust compiler handle "this memory is now out of scope, just forget about it"? If standard Rust rules say an object cannot own an object with a lifetime longer than itself, just dropping the object can't do any harm, can it? You're effectively setting up a heap, with its lifetime controlled by "head". So you just drop the entire heap, because every object in the heap has had the heap lifetime constraints imposed on it. Cheers, Wol Insulating layer? Posted Oct 20, 2024 11:11 UTC (Sun)
                               by ssokolow (guest, #94568)
                              [ Link ] Does Rust actually have a memory allocator, or does it have a "new"? That's the concern of the data type, not the language. If you want a new , you give the struct at least one private member so it can't be initialized directly and write a public associated function (i.e. a public class method) which constructs and returns an instance (Named new by convention only.) ...but that doesn't automatically mean heap allocation. It's purely a matter of whether there are "correct by construction" invariants that need to be enforced. If you want to heap-allocate, you either use a type which does it internally like Box<T> ( std::unique_ptr in C++) or Vec<T> or you do as they do internally and use the unsafe wrappers around malloc/calloc/realloc/free in std::alloc . Could the Rust compiler handle "this memory is now out of scope, just forget about it"? It does. If you want a destructor, you impl Drop and, if you don't, a stack allocation will just be forgotten until the whole frame is popped and a heap allocation will go away when the owning stack object's Drop is run and frees the memory. Rust's design does a good job of making its memory management appear more sophisticated than it really is. It's really just stack allocation, access control, destructors but no actual language-level support for constructors, and using RAII design patterns to implement everything else in library code on top of manual calls to malloc/realloc/free. The borrow checker plays no role in machine code generation beyond rejecting invalid programs, which is why things like mrustc can exist. Insulating layer? Posted Oct 17, 2024 14:07 UTC (Thu)
                               by paulj (subscriber, #341)
                              [ Link ] (8 responses) Samba has "talloc" which supports having heap objects allocated as a hierarchy. Freeing an object frees all its children in the hierarchy. You could take that kind of tack and allocate stuff on the stack with alloca, sure. Neither hierarchical allocation, nor stack allocation, address the issue of tracking validity of references though, as such. As you're implying, that requires something else - be it an ad-hoc system of rules that enforce guarantees, assuming the programmers' can hold themselves to applying those rules (and... they will fail to every now and then); or whether they are rules in the language and enforced in the compiler. The question for me is: What is the most programmer friendly system of rules to guarantee safety? Rust is one example of that. With a very general lifetime typing system (the most general possible?). That generality brings complexity. Yet, many Rust programmes have to step out of that compile-time lifetime-type system and use runtime ref-counting.  So then the question is, if the lesson from Rust is that many many programmes simply go to runtime ref-counting for much of their scoping, would it be possible to just have a less general, simpler lifetime-typing system? E.g., perhaps it isn't necessary at all to even need to represent lifetime types. Perhaps it would be sufficient to have 2 kinds of references - local and refcounted, with well-defined and safe conversion semantics enforced by the language. Insulating layer? Posted Oct 17, 2024 16:00 UTC (Thu)
                               by khim (subscriber, #9252)
                              [ Link ] > E.g., perhaps it isn't necessary at all to even need to represent lifetime types. Perhaps it would be sufficient to have 2 kinds of references - local and refcounted, with well-defined and safe conversion semantics enforced by the language. That's where Rust have started, more-or-less. The language which used this approach, Cyclone , is listed among many languages that “influenced” Rust. Only it's not practical: language that you are getting as a result doesn't resemble C at all! Or, more precisely: language would look like a C, but it's APIs would be entirely different. Even most trivial functions like strchr are designed around ability to pass reference to local variable somewhere. It's not even possible to pass buffer that you would fill with some values to another function! I'm not entirely sure all the complexity that Rust have (with covariance and contravariance, reborrows and so on) is needed … technically – but it's 100% wanted . Till Rust 2018 introduced Non-lexical lifetimes Rust was famous not for its safety, but for it's needless strictness. I think almost every article till that era included a mandatory part which was telling you tales how your “fight with the borrow checker” is not in vain and how it enables safety, etc. After introduction of NLL , reborrows , HRBT s  and so on people stopped complaning about that… and started complaining about complexity of the whole thing… but it's highly unlikely that people would accept anything less flexible: they want to write code and not fight with a borrow checker! > So then the question is, if the lesson from Rust is that many many programmes simply go to runtime ref-counting for much of their scoping, would it be possible to just have a less general, simpler lifetime-typing system? Sure. Swift does that. It has quite significant performance penalty, but still much faster than many other popular languages. Insulating layer? Posted Oct 17, 2024 17:10 UTC (Thu)
                               by Wol (subscriber, #4433)
                              [ Link ] (5 responses) > Neither hierarchical allocation, nor stack allocation, address the issue of tracking validity of references though, as such. As you're implying, that requires something else - be it an ad-hoc system of rules that enforce guarantees, assuming the programmers' can hold themselves to applying those rules (and... they will fail to every now and then); or whether they are rules in the language and enforced in the compiler. But does it *have* to? If you have a procedure-level heap, or some other data structure with a guaranteed lifetime, you apply exactly the same borrow-rules but on the heap level. If all references to the heap have the same or shorter lifetime than the heap, when the heap dies so will the references. So you create a heap for your linked list as early as possible, and you can freely create references WITHIN that heap as much as you like. They'll die with the heap. It's not meant to be an all-singing-all-dancing structure, it's meant to have a couple of simple rules that make it easy to create moderately complex data structures. You probably wouldn't be able to store pointers in it that pointed outside of it, for example. You would have to be careful storing references to items with shorter lifetimes. But if you want to store something with loads of complex *internal* references, it would be fine precisely because of the "everything dies together" rule. So you'd use the existing rust checking setup, just that it's a lot easier for you as the programmer to keep track of, precisely because "it's an internal self reference, I don't need to care about it". Cheers, Wol Insulating layer? Posted Oct 17, 2024 17:27 UTC (Thu)
                               by daroc (editor, #160859)
                              [ Link ] (1 responses) You may be interested in Vale , a programming language that is trying a bunch of new things around lifetime-based memory safety, including compiler support for "regions", which work almost exactly how you describe. The project also has some interesting ideas about tradeoffs between different memory management strategies. I want to write an article about it at some point. Insulating layer? Posted Oct 18, 2024 16:33 UTC (Fri)
                               by paulj (subscriber, #341)
                              [ Link ] That is very interesting. The generational references sound useful for performant weak-references. Although, you can not use them for very frequently allocated objects (you can't reuse the memory past <generation size>_MAX). Insulating layer? Posted Oct 18, 2024 14:25 UTC (Fri)
                               by taladar (subscriber, #68407)
                              [ Link ] (2 responses) That is essentially just an arena allocator. It solves the problem of forgetting to free something but doesn't solve e.g. accessing something that should not be used anymore after a certain operation. Insulating layer? Posted Oct 18, 2024 16:24 UTC (Fri)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) > but doesn't solve e.g. accessing something that should not be used anymore after a certain operation. And can't you just apply ordinary Rust rules to that? It's not intended as a way of escaping Rust's rules. It's just meant as a way of enabling the *programmer* to forget abut a lot of the rules on the basis that internal references, pointers, objects will all go invalid at the exact same time. So if A points to B and B points to A and they're in this structure you don't worry about cleanup because they both go poof and emit the magic smoke at the same time. If A contains a pointer to C with a shorter (or longer) lifetime, Rust will need to check that A destroys the pointer as C goes out of scope, or alternatively that the entire heap cannot go out of scope until C is destroyed. A simple structure for simple(ish) situations, and more complex structures for where simple doesn't work. And if those complex structures are hard to grasp, it helps if you've realised that the simple structures aren't up to the task (and why). Cheers, Wol Rust has arena allocators Posted Oct 18, 2024 16:35 UTC (Fri)
                               by kleptog (subscriber, #1183)
                              [ Link ] There appear to be several arena allocators for Rust: https://manishearth.github.io/blog/2021/03/15/arenas-in-r... The basic idea is you have a function with allocates an arena and keeps it alive. Within the arena objects can reference each other as much as they want, including cycles. They can also reference other objects, as long as they live longer than the arena. When your function exits, the arena is cleaned up in one go. Incompatible with destructors (though there are tricks for that), but otherwise looks like what you want. I know them from PostgreSQL where they have an arena per query so interrupting the query safely throws away everything. There are plenty of use cases for them. Insulating layer? Posted Oct 18, 2024 14:23 UTC (Fri)
                               by taladar (subscriber, #68407)
                              [ Link ] >  many many programmes simply go to runtime ref-counting for much of their scoping This is not my experience with Rust at all. Refcounting does happen occasionally but the vast, vast majority of cases doesn't use ref-counting at all in Rust. Usually it is no more than a few values (logically speaking, some of them can exist in many copies of course but the one that is referred to by the same name in the code and passed along the same code-paths) even in large programs. Insulating layer? Posted Oct 14, 2024 13:08 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] It's true that some people (even among Rust developers) actively want to make it a ref-counting language … but currently… no, it's not a refcounting language. It's very subtle and very much “something good done for entirely wrong reasons”, but the critical part is that in Rust to increment your counter you need to explicitly call Arc::clone or Rc::clone . And if you pass your smart pointer around you don't really do any expensive operations at all. This affects efficiency very significantly. Swift is trying to reach the same point from the other side and automatically eliminate ref-counting from places where compiler can prove it's not needed. It's an interesting experiment, but I'm sceptical about that approach: that advantage of Rust (every much unplanned, as shown above!) is that pesky .clone that you don't want to see on every line. That's simple psychology: if you see it – you want to eliminate it (when that's easy to do, it's not a crime to call .clone , if you need to!), if it's invisible – you wouldn't even think about whether what you are doing is expensive or not. Insulating layer? Posted Oct 17, 2024 19:16 UTC (Thu)
                               by sunshowers (subscriber, #170655)
                              [ Link ] Yes, it's common to sprinkle in refcounts where necessary. But most of the time you don't need them. So a common pattern is that the components all use refcounts to talk to each other, but use borrows internally. Insulating layer? Posted Oct 12, 2024 7:00 UTC (Sat)
                               by smurf (subscriber, #17840)
                              [ Link ] (67 responses) > the extent to which you are empowered to point at language constructs and say "don't use that if you want to be safe." That's the point right here. Rust has exactly one of these constructs. It's called "unsafe". You can find code that uses it with grep. C++ has myriads of conditions that are documented as Undefined Behavior. Quite a few of them are invisible without deep code analysis that borders on solving the halting problem. They're not going to go away. There's too much code out there that depends on them and too much social weight behind keeping the status quo as it is; a significant subset of the C++ community still thinks that avoiding UB is the programmer's job, not the compiler's. Insulating layer? Posted Oct 12, 2024 20:55 UTC (Sat)
                               by MaZe (subscriber, #53908)
                              [ Link ] (66 responses) Honestly with C/C++ a large part of the problem is the compilers themselves (and/or their developer's mindsets) and their willingness to bend over backwards to not define UB and intentionally not make it do the obvious(ly correct/natural) thing. For example you'd naively expect "foo x = {};" to simply be nicer syntax for "foo x; memset(&x, 0, sizeof(x));" (at least with a lack of non-zero default initialized fields).  But it's not...  There's some ridiculous subtleties with structs/unions and padding and some compilers can quite literally go out of their way to set some bits to 1.  It's obvious/natural that 'foo x = {}' should mean zero initialize the whole damn thing including any and all padding, and then on top of that apply any non-standard constructors/initializations. Basically compilers chase benchmarks instead of trying to make things a little bit more predictable/sane. Insulating layer? Posted Oct 13, 2024 10:23 UTC (Sun)
                               by rschroev (subscriber, #4164)
                              [ Link ] (19 responses) >  It's obvious/natural that 'foo x = {}' should mean zero initialize the whole damn thing including any and all padding To me that is neither obvious nor natural at all. To me it feels much more natural that constructs like that serve to initialize the class/struct members; what happens to the padding is not of my concern. I don't think I've ever cared about the bits in the padding, and I can't immediately think of a reason why I would, but if I would care I would make it explicit and use something which operates not on the members but on the raw memory, i.e. memset or memcpy. Insulating layer? Posted Oct 13, 2024 10:52 UTC (Sun)
                               by johill (subscriber, #25196)
                              [ Link ] (18 responses) It also seems false that the compilers "go out of their way to set bits to 1", if anything it would seem those bits are left untouched (stack memory)? However, this has bitten me in conjunction with -ftrivial-auto-var-init=pattern, where for something like union foo {
  char x;
  int y;
};

...

union foo f = { 0 }; clang created code to initialize x=0 and the other three bytes to the pattern, but ={} actually does initialize it all. The actual case was harder to understand because there was a struct involved, possibly with sub-structs, but ultimately the first member was the union. I feel like the OP is perhaps misrepresenting this quirk, but of course they might have something else in mind. Insulating layer? Posted Oct 13, 2024 13:10 UTC (Sun)
                               by MaZe (subscriber, #53908)
                              [ Link ] (17 responses) Honestly, I don't remember the precise details.  But what you're writing rings multiple bells. Furthermore ={ 0 } and ={} aren't quite the same thing, but ={} isn't supported by older compilers... = { 0 } used to mean zero init the whole thing though... As for why it matters?  Take a look at the bpf kernel system call interface. The argument to the system call is a union of structs for the different system call subcases. The kernel requires everything past a certain point to be zero-filled or it assumes the non-zero values have meaning (ie. future extensions) and it can't understand them and thus returns an error. Another example is when a struct is used as a key to a (bpf) map. Obviously the kernel doesn't know what portion of the key is relevant, so it hashes everything - padding included, so if padding isn't 0, stuff just doesn't work (lookups fail). Yes, the obvious answer is to make *all* padding explicit. Of course, this is also a good idea because userspace might be 32-bit and the BPF .o code is 64-bit, so you also don't have agreement on things like sizeof(long) or its alignment.  You want (or rather need) asserts on all struct sizes. Yes, once these gotchas get you, you know better, and there are workarounds... But it's bloody annoying that you have to think about stuff like this. There's ample other examples of C(++)-compiler-wants-to-get-you behaviours. I just picked one single one... [I guess some sort of 'sane C' dialect that actually says a byte is unsigned 8 bits, arithmetic is twos complement, bitfields work like this, shifts works like this, alignment/padding works like this, here's a native/little/big-endian 16-bit integer, basically get rid of a lot of the undefined behaviour, add some sort of 'defer' keyword, etc... would help a lot] Insulating layer? Posted Oct 13, 2024 18:00 UTC (Sun)
                               by marcH (subscriber, #57642)
                              [ Link ] (16 responses) > But it's bloody annoying that you have to think about stuff like this. It's not just "bloody annoying": it's _buggy_ because there are and always will be many instances where the developer does not think about it. Every single discussion about C++ safety always ends up in the very same "you're just holding it wrong" argument in one form or the other. Well, guess what: yes, there is _always_ someone somewhere "holding it wrong". That's because developers are humans. Even if some humans are perfect, many others are not. When humans don't want to make mistakes, they ask _computers_ to do the tedious bits and catch errors. You know, like... a Rust compiler. And please don't get me started on Coverity and other external checkers: I've seen first hand these to be the pinnacle of "you're holding it wrong". Notably (but not just) because of the many false positives, which invariably lead some developers with an obvious "conflict of interest" to mark real issues in their own code as false positives while more experienced people have simply no time to look and must delegate blindly. Advocates of C++ safety live in some sort of fantasy workplace / ivory tower where none of this happens and where they muse about technical and theoretical problems while being completely disconnected from the trenches where most coding happens. This is a constant failure to realize that usability comes first, technical details second. Same thing with security: the best design on paper is worth nothing at all if it's too complicated to use. The only, last hope for C++ safety is for a large enough number of C++ experts to spend less time on benchmarks and agree on ONE[*] safe subset and implement the corresponding static analyzer and distribute it with LLVM by default. That's a very long shot and even then some users will still find ways to hold that static analysis "wrong" in order to make their deadline. A generous dose of "unsafe" keywords on legacy code will of course be the easiest way (cause some of these keywords will be actually required) but they will find many other ways. I think it got a bit better now but until recently we were still hearing horrors stories based on _SQL injection_! If our industry can't get past a problem that dumb and so easy to spot, catch and kill, then who's going to bet on "safe C++"? [*] ONE subset? No, let's have C++ "profileS", plural! More choice! Insulating layer? Posted Oct 13, 2024 19:48 UTC (Sun)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (15 responses) > [*] ONE subset? No, let's have C++ "profileS", plural! More choice! I sit in on SG23 (the "Safety and Security" study group for ISO C++) and participate in discussions, but have not contributed any papers (mostly I try to help provide accuracy improvements when statements about Rust come up as I have done a fair amount of Rust programming myself, something that is thankfully seeming to increase among other members as well). It is my understanding that multiple profiles is because: - *some* progress has to be able to be made in a reasonable amount of time; - waiting until all of C++ is covered by a single "safe" marker is likely to take until something like C++32 (nevermind all of the things added between now and then); and - even if a single "safe" marker were possible, deployment is nigh impossible if there's not some way to attack problems incrementally rather than "here are 3000 diagnostics for your TU, good luck". Though Sean Baxter (of Circle) et al. are working on the second item there, it is *far* more radical than things like "make sure all array/vector indexing goes through bounds-checked codepaths" that one has a hope of applying incrementally to existing codebases. I believe the plan is to lay down guidelines for profiles so that existing code can be checked against them as well as other SGs coming to SG23 to ask "what can I do to help conform to profile X?" with their work before it is merged. Insulating layer? Posted Oct 14, 2024 10:19 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (14 responses) From the outside, though, what seems to be happening is not "there is a goal of a 'one true safe profile', such that all code can be split into an 'unsafe' part and a 'safe' part, but to make progress, we're coming up with ways for you to say 'I comply only with this subset of the safety profile'", but rather "let 1,000 profiles conflict with each other, such that you cannot combine code written under different profiles safely". This has the worrying corollary that you expect 'safe' C++ to be impossible to safely compose, since there's no guarantee that two libraries will choose the same safety profile. If it was clear that the intent is that there will eventually be one true profile that everyone uses, and that the reason for a lot of profiles existing is that we want to make progress towards the one true profile in stages, then there would be less concern. Insulating layer? Posted Oct 14, 2024 14:30 UTC (Mon)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (13 responses) I don't know who is speculating about "1000 profiles" nevermind them conflicting. AFAIK, there's *maybe* 5 under consideration. Bjarne's papers certainly have a limited number of them at least; anyone making such noise hasn't shown up to ISO. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 16:34 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (12 responses) For all practical purposes, there's no difference between 2 conflicting profiles and 1,000 profiles. The key bit is that you're perceived (by people showing up to ISO) as adding more profiles to "resolve" conflicts between the ultimate goal, rather than working towards a single safe C++ profile, with multiple supersets of "allowed" C++ to let you get from today's "C++ is basically unsafe" to the future's "C++ is safe" in stages. Ultimately, what's scary is that it looks like the idea is that I won't be allowed to write code in safe C++, and mix it with safe C++ from another team, because we might choose different profiles; I can only see two paths to that: Code that is safe according to profile A is definitionally safe according to profile B as well, for all values of A and B. This means that it doesn't matter which profile I use, I get the same outcome - it's just that different profiles will complain about different things. There is one, and only one, safe C++ profile. The remaining profiles are all stepping stones on the route from arbitrary C++ to safe C++, and it's understood that they are just that - stepping stones such that your code isn't safe C++ yet, but it's usefully safer under certain conditions. Otherwise, you end up with the language fragmenting; if you have 3 profiles that conflict, and I write in profile 1, while you write in profile 2, the combination of our code is no longer safe C++. The only way to avoid this is to mandate that we all use the same profile; but then you've spent all that effort writing your profiles, only to see it wasted because all but 1 profile is not used. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 17:34 UTC (Mon)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (11 responses) Profiles are a module-local decision that doesn't affect the decision to use other profiles in any other modules. I believe there is the proposed ability to apply a profile to an import to do some kind of "make sure I don't ship my unsafe bits for this to another module" verification, but I don't believe that there's anything saying one cannot have code under different profiles in the same program (unless C++ is going to also kick out not-under-a-profile C code which seems…unlikely). Where do you forsee any profiles conflicting in such an incompatible way? Sure, it could happen, but there are specific broad profiles proposed and I'm not aware of anything inherently being in conflict. I'd be surprised if anything like that were known during the design and discussion and it not be addressed. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 18:19 UTC (Mon)
                               by smurf (subscriber, #17840)
                              [ Link ] > Profiles are a module-local decision that doesn't affect the decision to use other profiles in any other modules What happens if I code to profile A and need to call some library function that changes the state of some variable to, let's say from shared to unshared (assuming that this is of concern for Profile A; could be anything else), but which isn't annotated with profile A's directives — because it's from library B which is coded according to profile C (or no profile at all) instead? Answer: you spend half your productive time adding profile A's extensions to B's headers, and the other half arguing with your manager why you're even using B (and/or A) when you have to put so much nonproductive effort into it. The result will be that you now have a fork of B. Alternately you spend even more time *really* fixing its profile-A-violations instead of just papering them over (assuming that you have its source code, which is not exactly guaranteed), sending its authors a pull request, and even more time convincing them that coding to Profile A is a good idea in the first place — which will be an uphill battle if the internals of B aren't easily convertible. Contrast this kind of what-me-worry attitude with Rust's, where spending considerable effort to avoid "unsafe" (if at all possible) is something you do (and your manager expects you'll do, as otherwise you'd be coding in C++) because you fully expect that *every* nontrivial or not-carefully-reasoned-out use of it *will* bite you sooner or later. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 18:32 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (9 responses) The versions I've heard about from SG23 members who pay attention to these things are cases where the different profiles assume different properties of "safe C++", such that if I, in module C, import modules A and B that each uses a different profile of "safe C++", the interactions between modules A and B through their safe interfaces, as intermediated by C, result in the safety guarantees made by their respective safety profiles being broken. To put it in slightly more formal terms, each profile needs to be a consistent axiomatic system, such that anything that cannot be automatically proven safe using the profile's axioms is unsafe, and the human is on the hook for ensuring that the resulting code is consistent with the profile's axioms. The problem that multiple profiles introduce is that all pairs of profiles need to be relatively consistent with each other, or any complete program where two modules use different profiles has to be deemed unsafe. I think we agree that "two modules using different profiles means your entire program is unsafe" is a bad outcome. But I'm arguing that fixing the problem of "we can't agree on axioms and theorems in our safe subset" by having multiple sets of axioms and theorems that must be relatively consistent with each other is simply expanding the amount of work you have to do, for no net gain to C++. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 20:34 UTC (Mon)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (8 responses) > the different profiles assume different properties of "safe C++" Yes…they wouldn't be different if not. But I guess I'm having trouble seeing where these conflicts live given that no specific rules have been laid down to even compare yet. The boundaries of where guarantees happen is certainly something that needs to be considered. The situation is going to be messier than Rust, but it's a lot harder to get moving with billions of lines of code you can't rewrite. I think it's more along the lines of "module A says it has bounds safety and module B says it has thread safety". Module C using them together doesn't mean B can't mess up bounds or A gets itself in a thread-related problem by the way they get used together, but the thread bugs shouldn't be in B and the bounds bugs shouldn't be in A. Number of profiles is kinda irrelevant here Posted Oct 14, 2024 21:04 UTC (Mon)
                               by farnz (subscriber, #17727)
                              [ Link ] (7 responses) Unless the profiles are subsets of a "final" target, it's very hard to avoid accidental conflicts based on assumptions; for example, module A says it has bounds safety, but the assumptions in the "bounds safety" profile happen to include "no mutation of data, including via calls into module A, from any module that does not itself have bounds safety", and module B does not have bounds safety, thus causing module A to not have bounds safety because module B calls a callback in module C that calls code in module A in a way that the bounds safety profile didn't expect to ever happen unless you have both the bounds safety and thread safety profiles on module A. As a result, module A claims to have bounds safety, but module A has messed up bounds because it was "bounds safety on the assumption of no threading from modules without bounds safety". Now, if it looked like SG23 were doing this because they knew it would make things a lot harder, and probably put off safe C++ until C++40 or later, I'd not be so concerned; but the mathematical nature here (of axiomatic systems) means that by splitting safety into "profiles", you've got all the work you'd have to do for a single "safe C++", plus all the work involved in ensuring that all the profiles are consistent with each other in any combination - and as the number of combinations of profiles grows, that work grows, too. If you have a "bounds safety" profile and a "thread safety" profile, then you need to ensure that the combination of bounds safety and thread safety is consistent. But you also need to ensure that bounds safety plus not thread safety is consistent, and that thread safety plus not bounds safety is consistent, and so on. Add in a third profile, and now you have to ensure consistency for all three profiles at once, all three cases of 1 profile, and all 3 pairs of profiles, and it just gets worse as the profile count goes up. Number of profiles is kinda irrelevant here Posted Oct 16, 2024 13:43 UTC (Wed)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (6 responses) > but the assumptions in the "bounds safety" profile happen to include "no mutation of data, including via calls into module A, from any module that does not itself have bounds safety" I don't think any profile can guarantee that it can't be subverted by such situations (e.g., even Rust guarantees can be broken by FFI behaviors). The way I forsee it working is that profiles get enough traction to help avoid some kind of "no C++ anymore" regulations by instead allowing "C++ guarded by profiles X, Y, Z" with ratcheting requirements as time goes by. If you need bounds safety, you need bounds safety and the bug needs to be addressed in the right location. Thanks for the discussion, by the way. I'll keep this in mind as I participate in SG23 to help keep an eye on it. Number of profiles is kinda irrelevant here Posted Oct 16, 2024 14:50 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] (5 responses) Wouldn't one of the easiest quick fixes be simply to say that all stuff currently defined as UB must have flags that convert it to Implementation Defined, and like the optimisation levels you can have some standard "bulk" definitions. So it's opt-in (so it won't break existing programs), but would get rid of a huge swathe of programmer logic errors. And it might even help different profiles work together ... Cheers, Wol Number of profiles is kinda irrelevant here Posted Oct 16, 2024 16:09 UTC (Wed)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (3 responses) There is…a lot of UB. If that were done, implementations would now have to document what that behavior is (and I don't think they can say "UB" anymore). What does one even say happens in the case of data races, division by zero, accessing an invalid iterator, access outside the bounds, unaligned atomic operations, etc.? Number of profiles is kinda irrelevant here Posted Oct 16, 2024 17:48 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] (2 responses) Data races? I guess the obvious answer is "what gets stored last gets kept"? Or are you going to tell me that it's not that simple? Division by zero I think is simple - "if the hardware supports it, set the value to IEEE Infinity. Otherwise if the compiler detects it it's compiler error else it's a runtime error. Allow the user to override the hardware and crash". I'd be quite happy with an answer of "The compiler can't detect it, so not our problem". But I'm thinking of the easy wins like "signed integer overflow is UB". At present I gather most compilers optimise your naive programmers' test for it away on the grounds of "if it can't happen then the following code is dead code". Most compilers have a flag that defines it as 2s complement, I believe, though that's not the default. So I'm guessing that I should have worded it better as "All UB that the compiler can detect and act upon needs to have that action documented, and options provided to configure sane behaviour (for the skilled-in-the-arts end-developer's definition of sane). And standard bundles of such options could/should be provided". As they say, the whole is greater than the sum of its parts, and you will get weird interactions between optimisations. But if the "safe profiles" emphasise the end-developer's ability to know and control how the compiler is interpreting UB, we (a) might get more predictable behaviour, (b) we might get more sensible conversations about what is and is not possible, and (c) we might get fewer unpleasant surprises from those weird interactions, because end-developers will disable optimisations they consider not worth the candle. Effectively what the linux kernel is already doing to some extent. But we want to get rid of the masses of options that you need a Ph.D. to understand, and just have a few "here this gives you a sane bundle of options". It's rather telling that most of the options linux specifies are --no-f-this, --no-f-that ... Actually, that "sane bundle of options" sounds like what -O1, 2, 3 should be doing, but I strongly get the impression it isn't. Cheers, Wol Number of profiles is kinda irrelevant here Posted Oct 16, 2024 18:18 UTC (Wed)
                               by smurf (subscriber, #17840)
                              [ Link ] > I'm thinking of the easy wins like "signed integer overflow is UB" This is an "easy win" in the sense that it's a solved problem: both gcc and clang have a '-fwrapv' option. Most UBs are *not* "easy wins". The past discussion here should have told you that already. Number of profiles is kinda irrelevant here Posted Oct 16, 2024 18:43 UTC (Wed)
                               by mathstuf (subscriber, #69389)
                              [ Link ] > Data races? I guess the obvious answer is "what gets stored last gets kept"? Or are you going to tell me that it's not that simple? And if thread 1 wins on the first byte and thread 2 on the second byte, which is "last"? Reading it gives a value that was never written. I believe the JVM handles this by saying that *a* value is returned, but it doesn't have to be a value that was ever written. > Division by zero I think is simple - "if the hardware supports it, set the value to IEEE Infinity. Otherwise if the compiler detects it it's compiler error else it's a runtime error. Allow the user to override the hardware and crash". Sure…assuming IEEE support and things like `-fno-fast-math`. What do you propose for integer division here? >  I'd be quite happy with an answer of "The compiler can't detect it, so not our problem". So…can the compiler optimize based on any assumptions around this behavior? I mean, if the behavior is implementation-defined to be "I dunno", what kinds of as-if code transformations are allowed? > Actually, that "sane bundle of options" sounds like what -O1, 2, 3 should be doing, but I strongly get the impression it isn't. In general, the "optimization level" flags are a grabbag of heuristics and wildly varying behavior across implementations. For example, Intel's toolchains turn on (their equivalent of) `-ffast-math` on `-O1` and above. Number of profiles is kinda irrelevant here Posted Oct 16, 2024 16:58 UTC (Wed)
                               by marcH (subscriber, #57642)
                              [ Link ] I think this explains a bit why this is not possible https://blog.llvm.org/2011/05/what-every-c-programmer-sho... Insulating layer? Posted Oct 13, 2024 11:08 UTC (Sun)
                               by mb (subscriber, #50428)
                              [ Link ] (38 responses) >For example you'd naively expect "foo x = {};" to simply be nicer syntax for "foo x; memset(&x, 0, sizeof(x));" I think this is also not the case in Rust, where =Default::default() is a rough equivalent of C's ={}. It only doesn't ever let you access the padding without unsafe. But it is still uninitialized and UB on read, unless you explicitly memset the whole thing with zeros before. Insulating layer? Posted Oct 13, 2024 19:50 UTC (Sun)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (37 responses) I think touching (reading or writing) padding is always UB (or, at best, indeterminate) because a containing `enum` may use the padding to store the discriminant. Insulating layer? Posted Oct 13, 2024 22:33 UTC (Sun)
                               by khim (subscriber, #9252)
                              [ Link ] (36 responses) It's undeterminate, but not UB. Normally reading the uninitialized value in Rust is UB, but reading padding is specifically excluded : Uninitialized memory is also implicitly invalid for any type that has a restricted set of valid values. In other words, the only cases in which reading uninitialized memory is permitted are inside unions and in “padding” (the gaps between the fields/elements of a type). It's just too easy to write code that reads padding, for one reason or another, thus keeping it UB was considered to be too dangerous. But while reading padding is not an UB, the result is still some random value, and not zero. Insulating layer? Posted Oct 14, 2024 7:54 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (35 responses) > It's just too easy to write code that reads padding, for one reason or another, thus keeping it UB was considered to be too dangerous. Imho, that's the wrong logic entirely. There is no reason it should be UB, therefore it shouldn't be. Accessing the contents pointed to by some random address should be UB - you don't know what's there. It might be i/o, it might be some other program's data, it might not even exist (if the MMU hasn't mapped it). You can't reason about it, therefore it's UB. But padding, uninitialised variables, etc etc are perfectly valid to dereference. You can reason about it, you're going to get random garbage back. So that shouldn't be undefined. But you can't reason about the consequences - you might want to assign it to an enum, do whatever with it that has preconditions you have no clue that this random garbage complies with. Therefore such access should be unsafe. Principle of least surprise - can you reason about it? If so you should be able to do it. Cheers, Wol Insulating layer? Posted Oct 14, 2024 13:08 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] (34 responses) > Imho, that's the wrong logic entirely. There is no reason it should be UB, therefore it shouldn't be. Thanks for showing us, yet again, why C/C++ couldn't be salavaged. Note how you have entirely ignoring everything, except your own opinion (based on how hardware works). Just because there are no reason for it to be UB from your POV doesn't mean that there are not reason for it to be UB from someone's else POV . And, indeed, that infamous be || !be is very much UB in both Rust and C . Yes, it may not make much sense for the “we code for the hardware” guys, it may be somewhat problematic, but, in Rust, it's not easy to hit that corner case (just look on how many contortions I had to do to finally be able to shoot myself in the foot!) and it helps compiler developers thus it was decided that access to “normal” uninitialized variable is UB. Even if some folks think it shouldn't be. > But padding, uninitialised variables, etc etc are perfectly valid to dereference. Nope. That's not how both C and Rust work. And in Rust reading uninitialised variables is UB while reading padding is not. Because that's where the best compromise for all parties involved was found. > Therefore such access should be unsafe. Just how much usafe? What about that pesky be || !be ? Should it guaranteed to return true or is it permitted to return false or crash? Note that both C and Rust allow all three possibilities (although rustc tries to help the programmer and make it crash when it's easily detectable, but such behavior is very much not guaranteed). > Principle of least surprise - can you reason about it? If so you should be able to do it. Except that was tried for many decades and simply doesn't work. Answer to the can you reason about it? is very much depends on the person that you are asking. But to write something in any language at least two persons should give the same answer to that question: the guy who writes the compiler (or interpreter) and the guy who uses said compiler. Simple application of principle of least surprise only works reliably when the sole user of the language is also its developer – and in that case it's very much not needed. Insulating layer? Posted Oct 14, 2024 15:02 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (32 responses) > > But padding, uninitialised variables, etc etc are perfectly valid to dereference. > Nope. That's not how both C and Rust work. And in Rust reading uninitialised variables is UB while reading padding is not. Because that's where the best compromise for all parties involved was found. Are you saying that the memory location is not allocated until the variable is written? Because if the memory location of the variable is only created by writing to it, then fair enough. That however seems inefficient and stupid because you're using indirection, when you expect the compiler to allocate a location. But (and yes maybe this is "code to the hardware" - but I would have thought it was "code to the compiler") I'm not aware of any compilers that wait until a variable is written before allocating the space. When I call a function, isn't it the norm for the COMPILER to allocate a stack frame for all the local variables? (yes I know you can delay declaration, which would delay allocation of stack space.) Which means that the COMPILER allocates a memory location before the variable is accessed for the first time. Which means I may be getting complete garbage if I do a "read before write", but fair enough. Okay, I don't know how modern gcc/clang etc work, but I'm merely describing how EVERY compiler (not many) I've ever had to dig into deep understanding of, work. So no, this is NOT "code to the hardware". It's "code to the mental model of the compiler", and if you insist it's code to the hardware, you need to EXPLAIN WHY. Or are you completely misunderstanding me and thinking I mean "dereference a pointer to memory" - which is not what I said and not what I meant!!! OF COURSE that's UB if you are unfortunate/stupid/incompetent enough to do that - taking the contents of the address of the pointer is perfectly okay - the compiler SHOULD have made sure that the address of the pointer points to valid memory. USING that contents to access memory is UB because using garbage as pointer to memory is obviously a stupid idea. Cheers, Wol Insulating layer? Posted Oct 14, 2024 15:21 UTC (Mon)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (17 responses) IIUC, the way this is thought about is that there are additional values for a memory location beyond the 256 encoded by the bits directly. LLVM's model has `undef` and `poison` at least. The only way to clear them is to write to them. So the memory is allocated, but may be (logically) represented as an language-unrepresentable value. Note that some hardware does have things like this: CHERI actually has 129 bits per pointer, the last not being addressable by the pointer value but is instead managed with dedicated instructions (probably?) to indicate "is a valid pointer". So while in C (and Rust) one could write the equivalent of `T* ptr = (T*)some_u128;`, `ptr` might not actually be usable as a pointer (though when such casts obey the *language* rules, the insertion of instructions to set that flag bit to the right state should be inserted). Insulating layer? Posted Oct 14, 2024 15:40 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (16 responses) But why does that make a difference to your ability to reason about it? It's basically the same problem I have in databases, where Pick has the empty string and SQL has NULL, and you're forever cursing the designer's decision to use those to mean about four different things. Going back to the "to be or not to be", if uninitialised variables are defined as containing the value "undef", about which you cannot reason, then "be || !be" would refuse to compile. But it wouldn't be UB, it would be an illegal operation on an undefined variable. If, however, the language allows you to operate on undefined variables inside an unsafe block, then to_be_or_not_to_be() would be an unsafe function, only callable from other unsafe functions, unless it actively asserted that it itself would not return a value of "undef". (Like SQL allows logical operations on NULL, where any NULL in the expression means the result is NULL.) And if you have a "convert random garbage to boolean" function, that can even handle undef and poison, then to_be_or_not_to_be() would just be like any normal function - guaranteed to return a boolean, just not necessarily a predictable one. Cheers, Wol Insulating layer? Posted Oct 14, 2024 17:28 UTC (Mon)
                               by mathstuf (subscriber, #69389)
                              [ Link ] (15 responses) > But why does that make a difference to your ability to reason about it? There are codepaths which make it impossible to know if something is actually initialized. Diagnostics to that effect are madness. Rust can avoid a lot of these issues because tuples are a language-level thing and functions wanting to return multiple values don't have to fight over which one "wins" the output while the rest live as output parameters. C++ at least has tuples, but they are…cumbersome to say the least. bool be; int status = do_init(&be); // Is `be` initialized or not? `init` is a function call whose implementation is not visible at compile or link time. Given that reading if it is actually uninitialized it is UB, the optimizer is allowed to assume that it is initialized. It may choose to emit a diagnostic that it is doing so, but it doesn't have to (and that may end up in all kinds of unwanted noise). Since the valid representations are "0" or "1", `be || !be` can be as-if implemented as `be == 1 || be == 0` which leads to UB effects of "impossible branches" being taken when it ends up being a bit-value of 2. Insulating layer? Posted Oct 14, 2024 18:01 UTC (Mon)
                               by smurf (subscriber, #17840)
                              [ Link ] (2 responses) > There are codepaths which make it impossible to know if something is actually initialized … in C++. There is no such thing in Rust — unless you're using "unsafe", that is. If you do, it's your responsibility to hide 100% of that unsafe-ness from your caller, at as low a level as possible, where reasoning about the state of a variable is still easy enough to do something about it. Insulating layer? Posted Oct 14, 2024 19:08 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) Exactly my point. to_be_or_not_to_be() knows it's undefined. So either it returns an unsafe boolean, or it has to fix the problem itself. If it knows it's returning undef in a "pure" boolean, it has to be a compiler error. Cheers, Wol Insulating layer? Posted Oct 14, 2024 19:58 UTC (Mon)
                               by smurf (subscriber, #17840)
                              [ Link ] to_be_or_not_to_be() knows it's undefined because its undefined-ness is expressed on exactly one line of C[++]. That argument no longer holds when it's spread over multiple functions, or even compilation units. Insulating layer? Posted Oct 14, 2024 19:17 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (11 responses) > int status = do_init(&be); What does do_init() do? If it merely returns whether or not "be" is initialised, then how can the optimiser assume that it is initialised? That's a massive logic bug in the compiler! If, on the other hand, it forces "be" to be a valid value, then of course the compiler can assume it's initialised. But that would  be obvious from the type system, no? Cheers, Wol Insulating layer? Posted Oct 14, 2024 19:42 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] (10 responses) > If it merely returns whether or not "be" is initialised, then how can the optimiser assume that it is initialised? That's easy: because correct program is not supposed to read uninitialized variable it can conclude that on all branches where it's read it's successfully initilialized. Then it's responsibility of developer to fix code to ensure that > But that would be obvious from the type system, no? Nope. When you call read(2) nothing in the typesystem differs for return values that are larger and smaller than zero. Insulating layer? Posted Oct 14, 2024 20:19 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (9 responses) > > If it merely returns whether or not "be" is initialised, then how can the optimiser assume that it is initialised? > That's easy: because correct program is not supposed to read uninitialized variable it can conclude that on all branches where it's read it's successfully initilialized. Then it's responsibility of developer to fix code to ensure that Circular reasoning !!! Actually, completely screwy reasoning. If do_init() does not alter the value of "be", then the compiler cannot assume that the value of "be" has changed! Let's rephrase that - "Because a SAFE *function* is not supposed to read an uninitialised variable". to_be_or_not_to_be() knows that "be" can be 'undef'. Therefore it either (a) can apply boolean logic to 'undef' and return a true boolean, or (b) it has to return an "unsafe boolean", or (c) it's a compiler error. Whichever route is chosen is irrelevant, the fact is it's just *logic*, not hardware, and it's enforceable by the compiler. In fact, as I understand Rust, normal compiler behaviour is to give the programmer the ability to choose whichever route he wants! All that matters is that the body of to_be_or_not_to_be() is marked as unsafe code, and the return value is either a safe "true boolean", or an unsafe boolean that could be 'undef'. At which point the calling program can take the appropriate action because IT KNOWS. Cheers, Wol Insulating layer? Posted Oct 14, 2024 20:31 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] > In fact, as I understand Rust, normal compiler behaviour is to give the programmer the ability to choose whichever route he wants! Nope. Normal compiler behavior is still the same as in C: language user have to ensure program doesn't have any UBs. The big difference is that for, normal, safe, subset of Rust it's ensured by the compiler. But for unsafe Rust it's still resposibility of the developer to ensure that program doesn't violate any rules WRT UB. > to_be_or_not_to_be() knows that "be" can be 'undef'. Nope. It couldn't be undef . In Rust MaybeUninit<bool> can be undef , but regular bool have to be either true or false . Going from MaybeUninit<bool> to bool when it's undef (and not true or false ) is an instant UB. > At which point the calling program can take the appropriate action because IT KNOWS. How does it know? You couldn't look on MaybeUninit<bool:gt; and ask it whether it's initialized or not. It's still very much a resposibility of Rust user to ensure that program doesn't try to convert MaybeUninit<bool> which contains undef into normal bool . Insulating layer? Posted Oct 14, 2024 20:41 UTC (Mon)
                               by daroc (editor, #160859)
                              [ Link ] (7 responses) I think that using three exclamation marks in a row might be a sign that this back-and-forth is not going anywhere in particular. This is a worthy discussion topic, but I'm not sure the last few comments have added anything new. Insulating layer? Posted Oct 14, 2024 23:43 UTC (Mon)
                               by atnot (subscriber, #124910)
                              [ Link ] (6 responses) > This is a worthy discussion topic, but I'm not sure the last few comments have added anything new. I agree it may have been a worthy topic once upon a time. But when the same two people (khim and wol) have the same near-identical drawn out discussions for dozens of messages a week in every amenable thread to the point of drowning out most other discussion on the site (at least without the filter) and making zero progress on their positions over a span of at least 2 years, perhaps some more action is necessary. Insulating layer? Posted Oct 15, 2024 10:13 UTC (Tue)
                               by Wol (subscriber, #4433)
                              [ Link ] (5 responses) Apologies. I try not to respond to khim any more. Unfortunately, I suspect the language barrier doesn't help. I think sometimes we end up arguing FOR the same thing, but because we don't understand what the other one is saying we end up arguing PAST each other. Cheers, Wol Insulating layer? Posted Oct 15, 2024 11:15 UTC (Tue)
                               by khim (subscriber, #9252)
                              [ Link ] (4 responses) > Unfortunately, I suspect the language barrier doesn't help. It could be a language barrier but I have seen similar discussions going in circles endlessly with a pair of native speakers, too, thus I suspect problem is deeper. My feeling is that it's ralated to difference between how mathematicians apply logic and laymans do it. > I think sometimes we end up arguing FOR the same thing, but because we don't understand what the other one is saying we end up arguing PAST each other. No, that's the issue. The big problem with compiler development (and language development) lies in the fact that compiler couldn't answer any interesting questions about semantics of your program . And that's why we go in circles. Wol arguments usually come in the form of: Compiler can do “the right thing” or “the wrong thing” “The wrong thing” is, well… wrong , it's bad thus compilers have to do “the right thing” And my answer comes in the form of: Sure, we may imagine compilers that do “the right thing” or “the wrong thing” Except usually “the wrong thing” is, possible to implement while “the right thing” is impossible to implement We are stuck with “the wrong thing” this talking about “the right thing” is pointless We may try to mitigate consequence of doing “the wrong thing” using some alternate approaches And Wol ignores the most important #2 step from my answer and goes back to “compilers should do “the right thing”… even if I have no idea how can they do that”. I have no idea why is it so hard for layman to accept that compilers are not omnipotent and compiler developers are not omnipotent either, that there are things that compilers just could never do… but that's the core issue: “we code for the hardware” guys somehow have a mental model of a compiler which is both simulatentously very simple (as discussion about how modern compilers introduce shadow variables for each assignment show) and infinitely powerful (as discussions about what compiler have to detect and report show). I have no idea how compiler can be simultaneously primitive and yet infinitely powerful, that's definitely a barrier right there, but it's not a “native speaker” vs “non-native speaker” barrier. And I continue the discussion to see if that barrier could be broken, somehow – because in that case C/C++ would have a chance! If that barrier could be broken then there's a way to reform the C/C++ “we code for the hardware” community! But so far it looks hopeless. Safe languages would still arrive, they would just have to arrive the sad way, one funeral at time way . And that would be the end of C/C++, because “one funeral at time” approach favors transition to the new language. Insulating layer? Posted Oct 15, 2024 16:18 UTC (Tue)
                               by atnot (subscriber, #124910)
                              [ Link ] I think this is a very tonedeaf way to respond to someone agreeing to finally stop participating in a discussion has been unproductive for years. Even if the powers that be continue to tolerate it, perhaps it's time to practice the (admittedly difficult) skill of letting someone be "wrong on the internet", for the sake of the rest of us. Insulating layer? Posted Oct 15, 2024 16:42 UTC (Tue)
                               by intelfx (subscriber, #130118)
                              [ Link ] (2 responses) One has to have a non-trivial amout of gall to unapologetically continue to paint oneself as a "mathematician" and attack your opponent as a "layman"... immediately after you were asked to stop not just the attacks, but the entire argument. Insulating layer? Posted Oct 15, 2024 21:58 UTC (Tue)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) And to say I'm not a mathematician ... well I don't necessarily understand symbolic logic, and I don't have a COMPUTER degree, but I've got an Honours in Science (Maths, Physics and Chemistry). And I've got a Masters, too (in technology, though I'm much less impressed with the value of that degree than the plain Bachelors). And I've got a very good ituition for "this argument sounds screwy" ... And when I try to argue LOGICally, if people argue straight past my logic and don't try show me what's wrong with it, I end up concluding they have to be attacking the messenger, because they are making no attempt to attack the message. Khim is saying Science advances one funeral at a time, but it'll be his funeral not mine - if you look back over what I've written it's clear I've learnt stuff. Don't always remember it! But as I've said every now and then in the past, you need to try to understand what the other person is arguing. I get the impression khim finds that hard. Cheers, Wol Insulating layer? Posted Oct 15, 2024 22:13 UTC (Tue)
                               by daroc (editor, #160859)
                              [ Link ] Alright, I think that's a fairly clear personal attack. Probably I should have said something after khim's message. Sorry, Wol, for not stepping in at that point. But in either case, this topic should end here. Insulating layer? Posted Oct 14, 2024 19:45 UTC (Mon)
                               by dezgeg (subscriber, #92243)
                              [ Link ] (3 responses) See https://devblogs.microsoft.com/oldnewthing/20040119-00 for example of a real architecture where attempting to use an uninitialized register variable might cause an exception. Insulating layer? Posted Oct 14, 2024 20:32 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (2 responses) Interesting. But I'm not arguing that attempting to dereference garbage is okay. That post explicitly says the programmer chose to return garbage. No problem there. The problem comes when you attempt to USE the garbage as if it was valid. Rust would - I assume - have marked the return value as "could be garbage", and when the caller attempted to dereference it without checking, Rust would have barfed with "compile error - you can't unconditionally dereference possible garbage". The point is, the programmer can reason about it because Rust would force them to track the fact that the return value could be garbage. Cheers, Wol Insulating layer? Posted Oct 14, 2024 20:43 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] > Rust would - I assume - have marked the return value as "could be garbage" Nope. Rust doesn't do that. Rust developer may use MaybeUninit<bool> to signal to the compiler that value may be uninitialized. And then Rust developer (and not compiler!) would decide when to go from MaybeUninit<bool> to bool (which would tell the compiler that at this point value is initialized). If I lie to the compiler (like I did) at that point – that's an instant UB. IOW: Rust does the exact same thing C/C++ does but mitigates the issue by making transition from “could be uninitialized” type to “I believe it's initialized now” type explicit. > Rust would have barfed with "compile error - you can't unconditionally dereference possible garbage". This couldn't be a compiler error, but sure enough, if you violate these rules and compiler can recognize it then there would be a warning. It's warning, not an error, because compiler may recognize this situation it but is not obliged to do that, it's something that it does on “best effort” basis. Insulating layer? Posted Oct 14, 2024 22:03 UTC (Mon)
                               by dezgeg (subscriber, #92243)
                              [ Link ] The linked Itanium example doesn't dereference uninitialized pointers anywhere - it demonstrates that just attempting to store an uninitialized register value might fault. In essence, this could blow up: int global; void f() { int uninitialized; global = uninitialized; } Ie. direct example of an architecture where what you wrote ("But padding, uninitialised variables, etc etc are perfectly valid to dereference. You can reason about it, you're going to get random garbage back.") doesn't apply. Insulating layer? Posted Oct 14, 2024 20:12 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] (9 responses) > Are you saying that the memory location is not allocated until the variable is written? Worse. For decades already most compilers create new location for every store to a variable . GCC started doing it around 20 years ago , LLVM did from the day one. > That however seems inefficient and stupid because you're using indirection, when you expect the compiler to allocate a location. Why would you use indirection? TreeSSA doesn't need that, you just create many copies of variable (one for each store to a variable). > When I call a function, isn't it the norm for the COMPILER to allocate a stack frame for all the local variables? (yes I know you can delay declaration, which would delay allocation of stack space.) No. Consider the following example : struct II {
    int x;
    int y;
};

int foo(struct II);

struct LL {
    long x;
    long y;
};

int bar(struct LL ll) {
    struct II ii = {
        .x = ll.x,
        .y = ll.y
    };
    return foo(ii);
} Here local variable ii ii never allocated at all. > It's "code to the mental model of the compiler", and if you insist it's code to the hardware, you need to EXPLAIN WHY. It's “code to the hardware” because “mental model of the compiler” couldn't explain how your program works without invoking hardware that would be executing your program. And it's only needed when you ignore rules of the language like they are described in the specification and then invent some new, entirely different specification of what you are program is doing and then assert that your POV is not just valid for today, but it would be the same 10, 20, 50 years down the road. That's… quite a presupposition. > USING that contents to access memory is UB because using garbage as pointer to memory is obviously a stupid idea. No, that's not about pointers, but about simple variables. Accessing them, if they are not initialized, is UB because normal program shouldn't do that and assuming that correct program doesn't access uninitialized variable is benefocial for the compiler. Heck, there are whole post dedicated to that issue on Ralf's blog . Insulating layer? Posted Oct 14, 2024 21:02 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (8 responses) > > Are you saying that the memory location is not allocated until the variable is written? > Worse. For decades already most compilers create new location for every store to a variable. GCC started doing it around 20 years ago, LLVM did from the day one. Ummmm ... my first reaction from your description was "Copy on write ??? What ... ???", but it's not even that! The other thing to bear in mind is that this is a quirk of *optimising* compilers, and it completely breaks the mental model of "x = x = x", so it breaks the concept of "least surprise". And I don't buy that's "coding to the hardware". If I have a variable "x", I don't expect the compiler to create lots of "x"s behind my back!!! And while we might be dying off, there's probably plenty of people, like me! for whom this was Ph.D. research for our POST-decessors. But. SERIOUSLY. What's wrong with either (a) pandering to the principle of least surprise and saying that dereferencing an uninitialised variable returns "rand()" (even if it's a different rand every time :-), or it returns 0x0000... The former makes intuitive sense, the latter would actually be useful, and if it's a compiler directive it's down to the programmer! which fits the Rust ethos. If the hardware is my brain, after all, dereferencing an unitialised variable would be a runtime error, not an opportunity for optimisation and code elimination ... Cheers, Wol Insulating layer? Posted Oct 15, 2024 6:41 UTC (Tue)
                               by khim (subscriber, #9252)
                              [ Link ] (6 responses) > And I don't buy that's "coding to the hardware". How do you call it, then? > If I have a variable "x", I don't expect the compiler to create lots of "x"s behind my back!!! Just as you wouldn't expect CPU to create hundred accumulators in place of one that machine code uses? CPUs do that for about 30 years. Just how you wouldn't expect to have one piece of memory to contain different values simultaneously? CPUs started doing that even earlier. I'm not telling your that to shame you for not knowing, I'm explaing to you why assuming that compiler (or hardware) would work in a certain “undocumented yet obvious” way doesn't lead to something that you may trust. There are many things that are implemented both in hardware and compiler via an as-if rule … and you don't need to even know about them if you are using language in accordance to the language specification. The same with hardware. That's why I really like The Tower of Weakenings that Rust seems to embrace. With it normal developers have 90% of code that's “safe” and doesn't need to know anything about how hardware and compilers actually work. But in unsafe world there are also gradations. In Linux kernel there are certain tiny amount of code (related to RCU, e.g.) that touches everything simultaneously: the real hardware, compiler internals, and so on. But if said code is only 0.01% of the whole and the majority of developers work with the other 99.99% of code then precise rules used in that tiny part could be ignored by most developers. > And while we might be dying off, there's probably plenty of people, like me! Weren't we just talking about how major scientific breakthroughs arrive one funeral at time ? Looks like that's how safety if low-level languages would arrive, too. And that's why C/C++ wouldn't get it, in practice: language can be changed to retrofit safety into it, but who would use it in that fashion and why? Insulating layer? Posted Oct 15, 2024 10:09 UTC (Tue)
                               by Wol (subscriber, #4433)
                              [ Link ] (5 responses) > > And I don't buy that's "coding to the hardware". > How do you call it, then? Brain-dead logic. > And that's why C/C++ wouldn't get it, in practice: language can be changed to retrofit safety into it, but who would use it in that fashion and why? So why am I left with the impression that it's PERFECTLY ACCEPTABLE for Rust to detect UB and not do anything about it? I take - was it smurf's - comment that "we can't promise to warn on UB because we can't guarantee to detect it", but if we know it's UB? If a programmer accesses an uninitialised variable there's a whole bunch of reasons why they might have done it. The C(++) response is to go "I don't believe you meant that" and go and do a load of stuff the programmer didn't expect. My understanding of the ethos of Rust is that if the compiler doesn't understand what you mean it's either unsafe, or an error. You seem to be advocating that Rust behave like C(++) and just ignore it. Sorry if I've got that wrong. Imho (in this particular case) Rust should come back at the programmer (like any sensible human being) and ask "What do you mean?". Imho there's three simple scenarios - (1) you expect the data to come from somewhere the compiler doesn't know about, (2) you forgot to explicitly request all variables are zeroed on declaration / read-before-write, or (3) you're expecting random garbage. (If there's more, just add them to the list.) So no you don't just dump stuff into UB, you should force the programmer to explain what they mean. And if they don't, it won't compile. Cheers, Wol Insulating layer? Posted Oct 15, 2024 10:33 UTC (Tue)
                               by farnz (subscriber, #17727)
                              [ Link ] (3 responses) So why am I left with the impression that it's PERFECTLY ACCEPTABLE for Rust to detect UB and not do anything about it? I take - was it smurf's - comment that "we can't promise to warn on UB because we can't guarantee to detect it", but if we know it's UB? Rust simply doesn't have UB outside of unsafe . There's no detection of UB involved at all - all the behaviour of safe Rust is supposed to be fully defined (and if it's not, that's a bug). For example, in the case of the Rust equivalent of bool be; int status = use_be(&be); , the defined behaviour is for the program to fail to compile because be is possibly used before it is initialized. Insulating layer? Posted Oct 15, 2024 21:40 UTC (Tue)
                               by Wol (subscriber, #4433)
                              [ Link ] (2 responses) So accessing the contents of be before you've initialised it is not UB, it's a fatal error. Thanks. I don't necessarily think it's the best definition, but it IS defined and it IS in-character for Rust. Which UB would not be. Cheers, Wol Insulating layer? Posted Oct 16, 2024 9:46 UTC (Wed)
                               by laarmen (subscriber, #63948)
                              [ Link ] (1 responses) Rust makes it *hard* for you to read uninitialized variables, but not impossible: let b: bool = unsafe { MaybeUninit::uninit().assume_init() };  // undefined behavior! ⚠️ I lifted this from the MaybeUninit doc, including the comment. That will compile, but *is* UB. Now, I'm of the opinion that it is perfectly reasonable for Rust to declare this UB, as the alternative makes a lot of assumptions about the underlying implementation, all for a use case that seems dubious to me. Insulating layer? Posted Oct 18, 2024 14:27 UTC (Fri)
                               by taladar (subscriber, #68407)
                              [ Link ] Unsafe blocks in Rust basically mean that you get to use unsafe operations but you are also responsible for upholding safety guarantees in your code inside the block. Methods like assume_init() are meant to be used after you have verified that the value is initialized, otherwise your code is unsound. Insulating layer? Posted Oct 15, 2024 10:42 UTC (Tue)
                               by khim (subscriber, #9252)
                              [ Link ] > You seem to be advocating that Rust behave like C(++) and just ignore it. Sorry if I've got that wrong. I'm not “advocating” anything, I'm just explaining how things work. Not how they “should work”. But how they, inevitably, have to work (and thus how they actually work). If out of ten choices that one like or dislike only one is actually implementable then you are getting that one whether you like it or not. > My understanding of the ethos of Rust is that if the compiler doesn't understand what you mean it's either unsafe, or an error. We are talking the full Rust, not just “safe” Rust here. UB is UB, whether it's in safe code or unsafe code. And yes, you can trigger UB in safe Rust – and it would lead to the exact same outcome as in unsafe Rust. > If a programmer accesses an uninitialised variable there's a whole bunch of reasons why they might have done it. If programmer accesses an uninitialized variable without the use of special construct that is allowed to touch undef , then it's a bug. Period, end of story. If program includes such access then it have to be fixed, there are no any other sensible choice. The only difference of safe Rust and unsafe Rust is decision of whose responsibility is it to fix such bug. If it's “safe” Rust then it's bug in the compiler (currently there are around 100 such bugs ) and compiler developers have to fix it, if it's in unsafe Rust, then developer have to fix it. Compiler may include warning for [potential] bugs in unsafe Rust, but ultimately it's resposibility of developer to fix them. > Imho (in this particular case) Rust should come back at the programmer (like any sensible human being) Impossible. Compilers are mindless (they literally have no mind and couldn't have it) and not sensible (they don't have “a common sense” and attempts to add it inevitable lead to even worse outcome ). That's something “we code for the hardware” people simply just refuse to accept for some unfathomable reason. > (1) you expect the data to come from somewhere the compiler doesn't know about In that case you have to use volatile read or volatile write . > (2) you forgot to explicitly request all variables are zeroed on declaration / read-before-write This is bug and it should be fixed. If you managed to do that in normal, “safe” Rust then it's bug in the compiler and it have to be fixed in compiler, if you did that in unsafe Rust, then it's bug in your code and you have to fix it. > (3) you're expecting random garbage. Currently that's also a bug, although there are discussions about adding such capability to the language (to permit tricks like the one used in the Using Uninitialized Memory for Fun and Profit . Currently Rust's only offer for such access is the use of asm! . > And if they don't, it won't compile. Not possible, sorry. If you wrote the magic unsafe keyword then it's your responsibility to deal with UB now. Compiler may still detect and report suspicious findings, but it couldn't be sure that it detected everything correctly thus such thing couldn't be a compile-time error, only and compile-time warning. Insulating layer? Posted Oct 15, 2024 6:57 UTC (Tue)
                               by smurf (subscriber, #17840)
                              [ Link ] > it completely breaks the mental model of "x = x = x", No it doesn't. If the compiler knows at all times where your particular x lives at any given time, your mental model isn't violated. Your mental model of the apple you're going to have for tomorrow's breakfast doesn't change depending on which side of the table you put it on, does it? Consider code like a=fn1() b=fn2(a) c=fn3(a,b) d=fn4(b,c) return d Now why should the compiler allocate space for four variables when c can easily be stored at a's location? it's not needed any more. d doesn't even need to be stored anywhere, simply clean the stack up and jump to fn4 directly, further bypassing any sort of human-understanding-centered model (and causing much fun when debugging optimized code). Constructing a case where a ends up in multiple locations instead of none whatsoever is left as an exercise to the reader. Insulating layer? Posted Oct 14, 2024 15:22 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] > Just because there are no reason for it to be UB from your POV doesn't mean that there are not reason for it to be UB from someone's else POV. And, indeed, that infamous be || !be is very much UB in both Rust and C. If bool is defined as a value that can ONLY contain 1 or 0, and the location of bool contains something else (for example it's not just a single bit-field), then as I pointed out you are assigning invalid garbage to a field that cannot contain it. Either you have a coercion rule that guarantees that "be" will be understood as a boolean whatever it contains, or yes it should be a compile error or unsafe or whatever. But imho looking at that simple example, the problem boils down to interpreting random garbage as a boolean. If the rules of the language let you do that, then I would expect it to return true - after all, the expression "(garbage) or not (garbage)" must evaluate to true if "garbage" can be interpreted as a boolean. It's all down the language you're using, and whether that language lets you treat any random junk as a boolean. If it does, then those functions make mathematical sense and should not be UB. If the language DOESN'T let you, then it should fail to compile with an "illegal coercion" error. And that has absolutely nothing to do with the hardware, and everything to do with mathematical logic. (Unless, of course, your DRAM behaves like a random number generator unless/until it's written to, such that (garbage) != (garbage) ...) And even there, I'd just say that you can guarantee the function will return a boolean, just not necessarily the boolean you expect ... Cheers, Wol Insulating layer? Posted Oct 13, 2024 11:50 UTC (Sun)
                               by khim (subscriber, #9252)
                              [ Link ] (6 responses) > Honestly with C/C++ a large part of the problem is the compilers themselves (and/or their developer's mindsets) and their willingness to bend over backwards to not define UB and intentionally not make it do the obvious(ly correct/natural) thing. True, but I would put 10% blame on compiler developers and 90% on “we code to the hardware” people. And you show why exteremely well. > There's some ridiculous subtleties with structs/unions and padding and some compilers can quite literally go out of their way to set some bits to 1. Which is not any different from what Rust is doing, isn't it? > It's obvious/natural that 'foo x = {}' should mean zero initialize the whole damn thing including any and all padding, and then on top of that apply any non-standard constructors/initializations. What is “obvious/natural” for one developers is not at all obvious/natural for other developer . And when developers refuse to “play by the rules” and use “who told you one couldn't carry the basketball and run – I tried it I could do that just fine” approach then safety remains a pipe dream. Sure, convoluted and strange rules of C and C++ with, literally, hundreds of UBs, don't help, but these rules could be changes. Except this wouldn't matter at all if people wouldn't accept them. And that is why all attempts to make C or C++ safer would fail: you could change a language, but it's almost impossible to change developers' attitude. The only known way is via Pank's principle : science progresses one funeral at a time … and it looks like language development happens in the exact same way. First we had Mel who refused to adopt assembler, then we had debate about structured programming , today “old school” developers play Mel's tricks with C/C++, loudly complain about compilers and refuse to adopt anything else… we couldn't predict whether Rust or something else would, eventually, replace C/C++ but we know it wouldn't happen peacefully, by people adopting the successor willingly and it wouldn't happen because someone mandate that C/C++ should be dropped… this leaves us with one funeral at a time … approach. That one tried, tested and works. Insulating layer? Posted Oct 13, 2024 12:29 UTC (Sun)
                               by pizza (subscriber, #46)
                              [ Link ] (5 responses) > True, but I would put 10% blame on compiler developers and 90% on “we code to the hardware” people. And you show why exteremely well. ....How dare hardware not perfectly conform to the spherical cow abstractions of higher level languages [1] > And when developers refuse to “play by the rules” and use “who told you one couldn't carry the basketball and run – I tried it I could do that just fine” approach then safety remains a pipe dream. Oh, you mean "if it's not explicitly disallowed then it by definition it must be allowed" attitude of nearly _every_ field of human endeavour?   ("metrics" become the only thing that matter, because that's what you're judged/rewarded by..) [1] "Higher-level" notably includes C _and_ Rust. Insulating layer? Posted Oct 13, 2024 14:12 UTC (Sun)
                               by smurf (subscriber, #17840)
                              [ Link ] (4 responses) > Oh, you mean "if it's not explicitly disallowed then it by definition it must be allowed" attitude of nearly _every_ field of human endeavour? Well the workaround is easy[1], you disallow everything, except under proscribed circumstances. Like Rust's "you can't write *any* data (or free a struct, or …) unless you can prove that there's no concurrent access". 1) for some value of "easy", anyway. Insulating layer? Posted Oct 13, 2024 19:43 UTC (Sun)
                               by Wol (subscriber, #4433)
                              [ Link ] (3 responses) > Well the workaround is easy[1], you disallow everything, except under proscribed circumstances. Or you disallow UB inasmuch as you can! And don't create new UB! If the standard doesn't define it, you have to impose a sensible default (eg, on a 2s complement machine, you assume 2s complement), and allow a flag to change it eg --assume-no-overflow. At which point you get a logical machine that you can reason about without repeatedly getting bitten in the bum. And the compiler writers can optimise up the wazoo for the benchmarks, but they can't introduce nasty shocks and make them the default. Cheers, Wol Insulating layer? Posted Oct 13, 2024 22:33 UTC (Sun)
                               by khim (subscriber, #9252)
                              [ Link ] (2 responses) > Or you disallow UB inasmuch as you can! And don't create new UB! How would that work without buy-in on the compiler users side? The Rust handling of UB (which actually works fine so far) hinges not on one, but two pillars: Language developers try to reduce number of UBs as much as sensible Language users try to avoid triggering the remaining UBs as much as possible But C/C++ community is 100% convinced that doing something about UBs is responsibility of “the other side” . Just read “What Every C Programmer Should Know About Undefined Behavior” (parts one , two , three ) and What every compiler writer should know about programmers Developers demand that compiler developers should ignore the language specifications and accept program with “sensible code” as programs without UB and, as we saw, when asked what is “sensible code” immediately present something that Rust also declares as unspecified. That's very symptomatic: that example shows, again (as if articles above weren't enough), how both sides are entirely uninterested in working together toward common goal in a C/C++ world. Defining padding as containing zeros is extremely non-trivial, because many optimizations rely on ability of the compiler to “separate” a struct (or union) into a set of fields and then “assemble” them back. And because the ability to have predictable padding is very rarely needed the decision, in a Rust world, was made to go with making it unspecified. Note that's is a compromise: usually reading uninitialized value is UB in Rust (to help compiler), but reading padding is not UB. From the reference : Uninitialized memory is also implicitly invalid for any type that has a restricted set of valid values. In other words, the only cases in which reading uninitialized memory is permitted are inside unions and in “padding” (the gaps between the fields/elements of a type). This decision doesn't make any side 100% happy: compiler makers would like to make reading padding UB to simplify their job and compiler users would like to make it zero to simplify their job, but “unspecified but not UB” is good enough for both sides to, grudgingly, accept it. Yet such compromise may not ever save the world where it's always the other side that have to do the job! > At which point you get a logical machine that you can reason about without repeatedly getting bitten in the bum. Nope. Logical machine that high-level language uses to define behavior of a program would always be different from “what the real hardware is does”. That's what separates high-level language from low-level language, after all. You can make it easier to understand or harder to understand, but someone who refuses to accept the fact that virtual machine used by high-level language definition even exists may always find a way to get bitten in the bum . Insulating layer? Posted Oct 14, 2024 13:24 UTC (Mon)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) > How would that work without buy-in on the compiler users side? The Rust handling of UB (which actually works fine so far) hinges not on one, but two pillars: Well, maybe, if we didn't have stupid rules like "don't use signed integer arithmetic because overflow is undefined", the user space developers might buy in. I'm far more sympathetic to the developers when they claim there's too much UB, than I am to compiler developers trying to introduce even more UB. In that particular case, I believe we have -fno-wrap-v or whatever the option is to tell the compiler what to do, but that should be the norm, not the exception, and it should be the default. If the compiler devs change to saying "this is UB. This is what a user-space developer would expect, therefore we'll add a (default) option to do that, and an option that says 'take advantage of UB when compiling'", then I suspect the two sides would come together pretty quickly. Compiler devs know all about UB because they take advantage of it. User devs don't realise it's there until they get bitten. The power is in the compiler guys hands to make the first move. And if it really is *undefinable* data (like reading from a write-only i/o port), then user space deserves all the brickbats they'll inevitably get ... :-) Cheers, Wol Insulating layer? Posted Oct 14, 2024 15:19 UTC (Mon)
                               by khim (subscriber, #9252)
                              [ Link ] > If the compiler devs change to saying "this is UB. This is what a user-space developer would expect, therefore we'll add a (default) option to do that, and an option that says 'take advantage of UB when compiling'", then I suspect the two sides would come together pretty quickly. This was tried, too. And, of course, what the people who tried that have found out that was that every “we code for the hardware” developer have their own idea about what compiler should do (mostly because they code for different hardware). > The power is in the compiler guys hands to make the first move. Why? They have already done the first step and created a specification that lists what is or isn't UB. If developers have a concrete proposals – they could offer changes to it. And yes, some small perceentage of C/C++ community works with developers. But most “we code for the hardware” guys don't even bother to even look and read it… how can compiler developers would know that their changes to that list would be treated any better then what they already gave? > And if it really is *undefinable* data (like reading from a write-only i/o port), then user space deserves all the brickbats they'll inevitably get ... :-) They would probably find a way to complain even in that case… > User devs don't realise it's there until they get bitten. And that's precisely the problem: if language have UBs and users of said language “only realise it's there when they get bitten” then such language couldn't be made safe. Ever. Either developers have to accept and handle UBs pro-actively or language should have no UBs at all. Latter case is limiting because certain low-level things couldn't be expressed in the language without UBs thus, for low-level stuff, safe and reliable language couldn't be made unless developers think about UBs before they are bitten. And the only known way to introduce such a radical non -technical change is to create a new community. And that's the end of story: C/C++ couldn't be made safe not because there are some technical, unsolvable, issues, but because it's community refuses to accept the fact that there are technical, unsolvable, issues (that have to be worked around on social level). Insulating layer? Posted Oct 17, 2024 8:51 UTC (Thu)
                               by peter-b (subscriber, #66996)
                              [ Link ] (1 responses) For example, you could just have lexically-scoped borrows (the borrow is not allowed to escape a given lexical scope) plus a syntax rule that prohibits moving a borrowed object within the borrow's lexical scope. This is not fully general, and fails to express many things that Rust can handle easily, but when you combine it with C++'s existing smart pointers, it is probably enough for many practical applications (that don't need ultra-low overhead). Please read this excellent recent WG21 paper which addresses your suggestions in detail: P3444 "Memory Safety without Lifetime Parameters" . Insulating layer? Posted Oct 17, 2024 16:39 UTC (Thu)
                               by kleptog (subscriber, #1183)
                              [ Link ] I think one of the conclusions in that paper is on the mark: > It’s not surprising that the C++ community hasn’t discovered a better way to approach safe references than the lifetime parameter model. After all, there isn’t a well-funded effort to advance C++ language-level lifetime safety. But there is in the Rust community. Rust has made valuable improvements to its lifetime safety design. Lots of effort goes into making borrow checking more permissive: The integration of mid-level IR and non-lexical lifetimes in 2016 revitalized the toolchain. Polonius[polonius] approaches dataflow analysis from the opposite direction, hoping to shake loose more improvements. Ideas like view types[view-types] and the sentinel pattern[sentinel-pattern] are being investigated. But all this activity has not discovered a mechanism that’s superior to lifetime parameters for specifying constraints. If something had been discovered, it would be integrated into the Rust language and I’d be proposing to adopt that into C++. For now, lifetime parameters are the best solution that the world has to offer. Basically, if there was a simpler way then likely Rust developers would have already found it and implemented it, because they're actually looking for ways to make things simpler. The C++ community is barely engaging with the topic. The likely end result seems fairly obvious to me. Insulating layer? Posted Oct 16, 2024 4:58 UTC (Wed)
                               by jezuch (subscriber, #52988)
                              [ Link ] > "memory safety, we could get that by using better tools for C/C++". Well, we've been hearing that for years (decades?) and it just keeps not happening. I wonder why? 🤔 Insulating layer? Posted Oct 20, 2024 11:15 UTC (Sun)
                               by ssokolow (guest, #94568)
                              [ Link ] They removed typestate shortly after I encountered Rust on Planet Mozilla and my memory of it was that they did so because "nobody was really using it". (i.e. The impression I got was less "usually, very hard to use and reason about for the uninitiated" and more "they didn't prove useful enough to justify being a core language feature as opposed to a design pattern".) Insulating layer? Posted Oct 11, 2024 14:20 UTC (Fri)
                               by asahilina (subscriber, #166071)
                              [ Link ] (4 responses) Rust macros are much, much more powerful than C macros. To implement multiple firmware ABIs ergonomically and maintainably, you need to be able to encode the differences between the ABIs. In terms of structs, this means that some fields might be added, removed, or moved between firmware versions. Not only does the struct definition have to change, but also all the code that uses it. In C, the best you can do is multiply compile the same file with a bunch of #ifdefs declaring the conditions under which certain structs exist or have a different type. This gets verbose quickly, it's ugly to compile, and you also need to wrap every single public top-level identifier in the file so it has a unique name when compiled for each version combination. You can't mix and match code that changes by version and code that doesn't in the same file, which forces code organization that might be different from what logically makes sense. And the more of the driver you cover with this mechanism, the more awkward it gets, which encourages you to add an explicit abstraction layer with dynamic dispatch, which is then slower and reduces the compiler's ability to optimize. Or you end up covering the entire driver in the scheme, which means all the internal interfaces within the driver need the name mangling applied, or you end up wanting to put everything into fewer files so you can use static, or doing something like having all the source files #included into a single compilation unit... and then there's the bug-proneness of this all, because C doesn't force you to initialize all fields of a struct, so if a given firmware build adds a field the compiler won't check that you actually handle it in every code paths that uses that struct. Basically, it's a bunch of bad tradeoffs. We're doing something like this for the DCP (display) driver (written in C) and it's quite messy and bug prone. In Rust, I wrote a rather ugly but functional proc macro [1] that allows you to apply multiple compilation to any top level block in a source file, and then conditionally compile any statement or declaration. The name mangling is automatic for declarations, and to reference them you just append ::ver to the name. Firmware definitions end up looking like [2] and the code that uses them like [3] (search for "ver"). Other parts of the driver can be pulled into the multiple compilation easily without any changes to code organization, and I only use dynamic dispatch (Rust trait objects) where it makes sense to insulate a significant chunk of shared code from the versioning. Then the compiler takes it all and basically creates N variants of the driver for each firmware/GPU combination, optimizing everything together, and sharing the bits that can be shared. If I accidentally mismatch the firmware declarations and their usage (extra field, missing field, wrong type) for any version variant, the compiler will complain. [1] https://github.com/AsahiLinux/linux/blob/asahi-wip/rust/m... [2] https://github.com/AsahiLinux/linux/blob/asahi-wip/driver... [3] https://github.com/AsahiLinux/linux/blob/asahi-wip/driver... That's just the versioning story, but Rust also helps get complex firmware interfaces right in general with its rich type system. Firmware pointers can be typed with the firmware type they point to as a generic parameter, even though from the CPU/Rust's perspective they aren't pointers or references at all, just a u64. I even have lifetime-bound strong firmware pointers that guarantee that the firmware object they point to outlives the firmware object containing the pointer. We don't have this yet, but it's also possible to use macros to verify that a firmware struct declaration has no implicit padding and only uses types that allow all bit patterns, which is important to avoid undefined behavior and portability issues, and allow you to read/write those structs from the firmware without having to use "unsafe". Insulating layer? Posted Oct 11, 2024 14:29 UTC (Fri)
                               by adobriyan (subscriber, #30858)
                              [ Link ] (2 responses) Speaking of macros, can they do: struct S { int a; #ifdef CONFIG_B int b; #endif }; I've seen std::conditional_t implementation on some video (which looks almost exactly like C++ version) which makes me think they can not. Insulating layer? Posted Oct 11, 2024 15:11 UTC (Fri)
                               by farnz (subscriber, #17727)
                              [ Link ] That's built-in, and can be done via macros, or via the existing #[cfg] attribute. Macros in general in Rust get an AST, and output a new AST, so can do pretty much anything (noting that only function-like macros get an arbitrary AST; attribute macros and derive macros must be given valid Rust source code). But using the built-in #[cfg] mechanism, along with feature flags , you'd write that as: struct S {
    a: i32,
    #[cfg(feature = "config_b")]
    b: i32
} This says that the field b only exists if the feature flag "config_b" is set at compile time; there's also preset cfg items , like target_has_atomic , which lets you write #[cfg(target_has_atomic = "64")] to only include a chunk of code on platforms with support for 64 bit atomics. Insulating layer? Posted Oct 11, 2024 15:26 UTC (Fri)
                               by khim (subscriber, #9252)
                              [ Link ] You don't need macros for that because that's handled by separate facility in Rust. > I've seen std::conditional_t implementation on some video (which looks almost exactly like C++ version) which makes me think they can not. You probably misunderstood the precise limitation, I'm afraid. Macros in Rust are incredibly powerful, but they still work with a token trees . You can support configurable code and conditional compilation with them just fine, but they don't have access to the type information . std::conditional_t (and whatever Rust equivalent you saw) work with types, not with define s, and yes, that's something macros couldn't touch (neither in C/C++ nor in Rust) simply because macros work before types come into existence. If you go from C++ to Rust then yes, this limitation feels quite stifling. If you go from C to Rust… nope: C simply doesn't have anything similar to reflection. magic_enum couldn't be replicated in Rust, but then it doesn't work with C, either. Insulating layer? Posted Oct 12, 2024 1:46 UTC (Sat)
                               by rcampos (subscriber, #59737)
                              [ Link ] Thanks for the great and detailed explanation! Why are enterprise kernels so old? Posted Oct 16, 2024 21:13 UTC (Wed)
                               by chexo4 (subscriber, #169500)
                              [ Link ] (1 responses) Hey, why exactly is *anyone* supporting backporting beyond the official LTS kernels? I know there are various companies that offer it as a service, but I feel like the risks of security issues and difficulty auditing would be more than just… updating to a newer kernel Why are enterprise kernels so old? Posted Oct 17, 2024 6:32 UTC (Thu)
                               by smurf (subscriber, #17840)
                              [ Link ] That question is easy to answer as soon as you're in the mobile phone space. Proprietary drivers that are "certified" to work with a particular kernel. No sources. In "enterprise" space there's the lack of exhaustive automated tests, plus managers who convince themselves that a somewhat-random backport to a known kernel is somehow safer than a new "unknown" Linux version. It's basically the same reason why Boeing went to such lengths to pretend that the 737-MAX was just another 737 — instead of a completely different aircraft that requires ex[pt]ensive re-certification. We know the results. Rust-GCC Posted Oct 19, 2024 0:54 UTC (Sat)
                               by jsakkine (subscriber, #80603)
                              [ Link ] (3 responses) Looking at https://github.com/Rust-GCC/Reporting/blob/main/2024/2024... , GCC is making steady progress. IMHO much more important than anything that happens in-tree. When one can take just good old GCC and compile Rust-enabled kernel that should open doors for being non-experimental and being part of arch defconfig-files. The problem sort of starts to solving itself then. Rust-GCC Posted Oct 19, 2024 1:07 UTC (Sat)
                               by intelfx (subscriber, #130118)
                              [ Link ] (2 responses) > When one can take just good old GCC and compile Rust-enabled kernel that should open doors for being non-experimental That shouldn't be a prerequisite. Rust-GCC Posted Oct 19, 2024 7:07 UTC (Sat)
                               by smurf (subscriber, #17840)
                              [ Link ] (1 responses) > > When one can take just good old GCC and compile Rust-enabled kernel that should open doors for being non-experimental > That shouldn't be a prerequisite. I can only assume that a few kernel maintainers more-or-less-strongly disagree. IMHO a strong and demonstrated commitment for the GCC people to "get there", in the not-too-distant future, should go a long way towards assuaging their concerns. NB: There are standards organizations that require at least two, sometimes three, independent implementations before they even start thinking about standardizing something. Rust-GCC Posted Oct 21, 2024 8:35 UTC (Mon)
                               by taladar (subscriber, #68407)
                              [ Link ] And all languages standardized by those organizations are a horrible mess of design by committee compromises between the authors of the various implementations. No thanks.
======>
https://old.reddit.com/r/rust/comments/1gbeeia/rustanalyzer_for_workspaces_with_crates_with/
-->>-->>
After a bit of searching, I haven't found a satisfying solution for a problem I'm having.   

   For context, I'm working on a project contained in a Cargo workspace, which has many facade crates using a common core library. This library has a set of mutually incompatible Cargo features - which is unfortunately somewhat unavoidable in the embedded Rust world. In using the rust-analyzer VScode extension.    

   The issue I'm having is that there doesn't seem to be a way to specify per-crate feature sets in r-a. Which means that I'm stuck either building all crates which all features enabled, which inevitably leads to hundreds of errors in the core crate, or specifying a feature set in the r-a config, which doesn't work either - r-a seems to ignore those for some reason.   

   This is really annoying. Since I spend most of the time working on the core library, what I end up doing is temporarily excluding everything but the core library from the workspace while I'm working in VScode. But I feel like there should be a better solution to this?   

   At the very least, is there a rust-analyzer setting to ignore some packages in a workspace? Or even tell it to analyze a single package, and perhaps ignore the fact that it is in a workspace?   

   Has anyone run into a similar scenario?    
   

======>
https://old.reddit.com/r/rust/comments/1gbg2i5/the_fastest_web_framework_can_now_serve_websocket/
-->>-->>
https://github.com/c410-f3r/wtx   

   wtx    allows, among other things, the development of web applications through a built-in HTTP/2 server framework, a built-in    PostgreSQL    connector and a built-in SQL schema manager.   

   In my defense, the title is not a click-bait because I already provided several benchmarks   ¹   ²   ³   ⁴   ⁵   ⁶    and no-one stated otherwise. If these claims are not true, please let me know, I really want to further improve the project.   

   The    RFC8441    protocol that defines the handshake procedure has been implemented. While HTTP/2 inherently supports full-duplex communication, web browsers typically don't expose this functionality directly to developers and that is why WebSocket tunneling over HTTP/2 is important.   

   
   Servers can efficiently handle multiple concurrent streams within a single TCP connection   
   Client applications can continue using existing WebSocket APIs without modification   
   

   As far as I can tell,    wtx    and    deno    are the only Rust projects that support this feature.   

   Take a look at    https://github.com/c410-f3r/wtx/blob/main/wtx-instances/http2-examples/http2-web-socket.rs    to see a code example. If you have questions, feel free to ask.   
   

======>
https://old.reddit.com/r/rust/comments/1gb6dsf/i_built_an_applet_for_the_new_rust_cosmic_desktop/
-->>-->>
I have been testing the alpha of COSMIC desktop for a few months now, and was missing some small QOL tools such as being able to see how many arch updates I had stacked up. Thought it might be a good opportunity to learn how libcosmic and iced work under the hood, and I was able to spin off the API as a separate crate that could perhaps get use elsewhere.   

   Check it out here!   

   https://github.com/nick42d/cosmic-applet-arch   
   

======>
https://github.com/ArthurBrussee/brush
-->>-->>
Repository files navigation README Apache-2.0 license Brush - universal splats teaser_compressed.mp4 Brush is a 3D reconstruction engine, using Gaussian splatting . It aims to be highly portable, flexible and fast. 3D reconstruction shouldn't require special hardware. Brush can render and train on a wide range of systems: macOS/windows/linux , AMD/Nvidia cards, Android , and in a browser . To achieve this, brush is built using WebGPU compatible tech, that can run practically anywhere! It uses the Burn framework, which has a portable wgpu backend. This project is currently still a proof of concept, and doesn't yet implement any of the extensions to gaussian splatting that have been developed, nor is the performance optimal yet. Try the (experimental) web demo NOTE: This only works on desktop Chrome 129+ currently. Firefox and Safari are hopefully supported soon , but currently even firefox nightly and safari technical preview do not work Features The demo can load pretrained ply splats, and can load datasets to train on. Currently only two formats are supported. A .zip file containing: A transform_train.json and images, like the synthetic nerf scene dataset. An images & sparse folder with COLMAP data While training you can interact with the splats and see their training dynamics live, and compare the current rendering to training / eval views as the training progresses. Web combined_compressed.mp4 Rerun rerun_dash_compressed.mp4 While training, additional data can be visualized with the excellent rerun . To install rerun on your machine, please follow their instructions . Open the ./brush_blueprint.rbl in the viewer for best results. Mobile brush_android_compressed.mp4 Live training on a pixel 7 Why Machine learning for real time rendering has a lot of potential, but at the same time, most popular ML tools don't align well with r. Rendering requires low latency, usually involve dynamic shapes, and it's not pleasant to attempt to ship apps with large PyTorch/Jax/CUDA deps calling out to python in a rendering loop. The usual fix is to write a seperate training and inference application. Brush on the other hand, written in rust using wgpu and burn , can produce simple dependency free binaries, and can run on nearly all devices. Getting started Install rust 1.81+ and run cargo run or cargo run --release . You can run tests with cargo test --all . Brush uses the wonderful rerun for additional visualizations while training.
It currently requires rerun 0.19 however, which isn't released yet. Windows/macOS/Linux Simply cargo run or cargo run --release from the workspace root. Note: Linux has not yet been tested but should work. Windows works well, but does currently only works on Vulkan. Web This project uses trunk to build for the web. Install trunk, and then run trunk serve or trunk serve --release to run a development server. WebGPU is still a new standard, and as such, only the latest versions of Chrome work currently. Firefox nightly should work but unfortunately crashes currently. The public web demo is registered for the subgroups origin trial . To run the web demo for yourself, please enable the "Unsafe WebGPU support" flag in Chrome. Android To build on Android, see the more detailed README instructions at crates/brush-android. iOS Brush should work on iOs but there is currently no project setup to do so. Technical details Brush is split into various crates. A quick overview of the different responsibilities are: brush-render is the main crate that pulls together the kernels into rendering functions. brush-train has code to actually train Gaussians, and handle larger scale optimizations like splitting/cloning gaussians etc. brush-viewer handles the UI and integrating the training loop. brush-android is the binary target for running on android, while brush-desktop is for running both on web, and mac/Windows/Linux. brush-wgsl handles some kernel inspection for generating CPU-side structs and interacing with naga-oil to handle shader imports. brush-dataset handles importing different datasets like COLMAP or synthetic nerf data. brush-prefix-sum and brush-sort are only compute kernels and should be largely independent of Brush (other than brush-wgsl ). rrfd is a small extension of rfd Kernels The kernels are written in a "sparse" style, that is, only work for visible gaussians is done, though the final calculated gradients are dense. Brush uses a GPU radix sort based on FidelityFX (see crates/brush-sort ). The sorting is done in two parts - first splats are sorted only by depth, then sorted by their tile ID, which saves some sorting time compared to sorting both depth and tile ids at the same time. Compatibility with WebGPU does bring some challenges, even with (the excellent) wgpu . WebGPU lacks native atomic floating point additions, and a software CAS loop has to be used. GPU readbacks have to be async on WebGPU. A rendering pass can't do this unless the whole rendering becomes async, which has its own perils, and isn't great for an UI. The reference tile renderer requires reading back the number of "intersections" (each visible tile of a gaussian is one intersection), but this is not feasible. This is worked around by assuming a worst case. To reduce the number of tiles the rasterizer culls away unused tiles by intersecting the gaussian ellipses with the screenspace tiles. The WGSL kernels use naga_oil to manage imports. brush-wgsl additionally does some reflection to generate rust code to send uniform data to a kernel. In the future, it might be possible to port the kernels to Burns new CubeCL language, which is much more ergonomic and would allow generating CUDA / rocM kernels. It might also be possible to integrate with George Kopanos' Slang kernels . Benchmarks Rendering performance is expected to be very competitive with gSplat, while training performance is still a bit slower. You can run some benchmarks using cargo bench . The performance of the splatting forward and backwards kernel are faster than the legacy gSplat kernels as they use some new techniques for better performance, but they haven't been compared yet to the more recent gSplat kernels. End-to-end training performance is also still slower, due to other overheads. For additional profiling, you can use tracy and run with cargo run --release --feature=tracy . Quality Quality is similair, but for now still somewhat lagging behind the original GS implementation. This is likely due to some suboptimal splitting/cloning heuristics. Scene Brush GS paper Bicycle@7K 23.2 23.604 Garden@7k 25.8 26.245 Stump@7k 24.9 25.709 Acknowledgements gSplat , for their reference version of the kernels Peter Hedman & George Kopanas , for the many discussions & pointers. The Burn team , for help & improvements to Burn along the way Raph Levien , for the original version of the GPU radix sort. Disclaimer This is not an official Google product. This repository is a forked public version of the google-research repository
======>
https://github.com/geo-ant/roxygen
-->>-->>
Repository files navigation README MIT license roxygen - documenting function parameters The #[roxygen] attribute allows you to add doc-comments to function
parameters, which is a compile error in current Rust. You can now write use roxygen :: * ; # [ roxygen ] /// sum the rows of an image fn sum_image_rows ( /// the image data in row-major format image_data : & [ f32 ] , /// the number of rows in the image nrows : u32 , /// the number of columns in the image ncols : u32 , /// an out buffer into which the resulting /// sums are placed. Must have space /// for `nrows * ncols` elements sums : & mut [ f32 ] ) -> Result < ( ) , String > { todo ! ( ) } This will produce documentation as if you had written the doc-comment for the function
like so: /// sum the rows of an image /// /// **Arguments**: /// /// * `image_data`: the image data in row-major format /// * `nrows`: the number of rows in the image /// * `ncols`: the number of columns in the image /// * `sums`: an out buffer into which the resulting ///    sums are placed. Must have space ///    for `nrows * ncols` elements fn sum_image_rows ( image_data : & [ f32 ] , nrows : u32 , ncols : u32 , sums : & mut [ f32 ] ) -> Result < ( ) , String > { todo ! ( ) } Placing the Arguments-Section By default, the section documenting the arguments will go at the end
of the top-level function documentation. However, this crate allows to explicitly
place the section by using a custom attribute like so: use roxygen :: * ; # [ roxygen ] /// long documention /// ... # [ arguments_section ] /// # Examples /// ... fn foo ( /// some docs first : i32 , second : f32 ) { } Considerations It's a long standing issue whether and how to add this capability to rustdoc . Firstly, there's no
general consensus on how exactly to document function parameters. However,
I've seen the presented style used a lot, with minor variations.
Secondly, the standard library doesn't need this style of documentation at all. So before you stick this macro on every function,
do consider taking inspiration from how the standard library deals with function parameters, using fewer function parameters, using more descriptive parameters names, using types to communicate intent, sticking function parameters in a struct . All that being said, I've realized that sometimes I still want to document
function parameters. Compile Times Macros will always increase your compile time to some degree, but I don't think
this is a giant issue here for two reasons: firstly, this macro is to be used sparingly .
Secondly, this macro just does some light parsing and shuffling around of
the documentation tokens. It introduces no additional code. Thus, it doesn't
make your actual code more or less complex and should not affect compile
times much, but I haven't measured it... so take it with a grain of sodium-chloride.
======>
https://loro.dev/blog/v1.0
-->>-->>
In Loro 1.0, we implemented a simple LSM (Log-structured merge-tree) (opens in a new tab) engine internally. LSM is a data structure often used to
implement Key-Value Databases, and Loro 1.0 is heavily inspired by its design.
Currently, Loro's storage implementation uses get, set, and range operations of
Key-Value Database as primitives. For example, Loro stores history as a series of
ChangeBlocks, with each ChangeBlock serialized to about 4KB. Each ChangeBlock
uses its first Op Id as the Key, and the serialized binary data of the
ChangeBlock as the Value, stored in the internal LSM engine. In our simple LSM Engine implementation, each Block is compressed during
serialization, and decompression only occurs when the corresponding Value is
actually retrieved. This allows the import speed of the new data format to be up
to a hundred times faster than before, with even lower memory usage. So in Loro
1.0: Data integrity is checked during import Loro internally stores history (History/OpLog) and state (DocState) in
blocks, loading the corresponding blocks as needed The Eg-walker algorithm that Loro is based on allows documents to start
editing without complete CRDTs meta information, thus easily working with
lazy loading behavior Why is lazy loading valuable? Because in many use cases, we don't need to fully
load the document's history and state: For example, when we receive a set of remote updates, but the Loro document
data is still in the database, and we want to know the latest state of the
document, we need to load the LoroDoc snapshot from the database, then import
the remote update set, and then get the latest document state. At this point,
most of the historical information won't be accessed. Sometimes in data synchronization scenarios, peer A needs to send historical
data that peer B doesn't have. It needs to import the snapshot and then
extract the historical information that B doesn't have. In this case, the
document's state doesn't need to be parsed, and the unused part of the history
doesn't need to be parsed either. Users don't need document history when initializing a document; only parsing
the State is necessary at this point When merging remote operations, only the modified containers and some of the
related historical operations need to be visited What happens during import and export in the new version? Let's take a common
scenario as an example: In real-time collaboration sessions or local storage, we recommend developers
first store the operations from users, and then periodically perform compaction.
This compaction involves importing the old snapshot and all scattered updates
into the same LoroDoc, then exporting uniformly through the Snapshot format. In
the new version, this will involve the following: First, the old version of the snapshot is imported The received updates may contain parallel edits, so a part of the related
parallel edit history from the old version needs to be loaded to construct the
CRDT and complete the diff calculation Loro internally loads and parses the data of the corresponding block to get
the corresponding history; at this point, complete document parsing does not
occur After the diff calculation is complete, it needs to be applied to the
corresponding States Loro will internally load and parse the corresponding state, and at this
point, complete document parsing does not occur either Export Unaffected history blocks or state blocks are exported as they are Affected blocks will be serialized to overwrite the original block, then
exported During export, we use a method similar to SSTable internally for the final
export The only data that needs to be parsed in this entire process are: Meta information for each stored block Blocks that need to be read will be decompressed History Blocks / state Blocks that will be affected by Updates ❓ Do we still need to load the entire document blob with these optimizations? We still need to load the entire document blob into memory. However, our current architecture has implemented internal block-based loading and storage, making it easier for us to implement true block-based retrieval and saving from disk in the future. This could make Loro function more like a database. While theoretically feasible, we'll assess if there are practical scenarios that require this capability. For most documents, Loro's current performance is already quite sufficient.

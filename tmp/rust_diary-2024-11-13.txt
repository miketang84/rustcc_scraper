https://github.com/kezhenxu94/ipass
-->>-->>
Repository files navigation README ipass A CLI to interact with Apple macOS Passwords (iCloud KeyChain). Demo Some sample usages of the ipass CLI tool you might be interested in: # login to docker hub ipass pw get docker.com kezhenxu94 | jq ' .entries[0].password ' -r | docker login -u kezhenxu94 --password-stdin About The Project This project basically rewrites the apw project in Rust,
because the apw project doesn't run on my M3 macOS 15.0.1 for no reason. It utilises a built in helper tool in macOS 14 and above to facilitate this
functionality. Installation Binary Go to release page to download the binary
according to you platform, and run xattr -c ./ipass-aarch64-apple-darwin.tar.gz (to avoid "unknown developer" warning).
Then extract the binary from the tarball: tar -zxvf ./ipass-aarch64-apple-darwin.tar.gz Cargo If you have cargo installed, you can easily install the binary using the
command: cargo install --git https://github.com/kezhenxu94/ipass Usage Ensure the daemon is running in the background, via ipass start . To authenticate the daemon: This is required every time the daemon starts i.e on boot ipass auth Query for available passwords for a specific domain: ipass pw list google.com View more commands & help: ipass help Building This project uses Rust for development and compilation.
Make sure you have Rust installed on your system before proceeding. Running the Project To run the project whilst developing: cargo run -- start Building a release version To build a statically compiled binary: cargo build Contributing Contributions are what make the open source community such an amazing place to
learn, inspire, and create. Any contributions you make are greatly
appreciated . If you have a suggestion that would make this better, please fork the repo and
create a pull request. You can also simply open an issue with the tag
"enhancement". Don't forget to give the project a star! Thanks again! Fork the Project Create your Feature Branch ( git checkout -b feature/AmazingFeature ) Commit your Changes ( git commit -m 'feat: add some AmazingFeature' ) Push to the Branch ( git push origin feature/AmazingFeature ) Open a Pull Request License Distributed under the GPL V3.0 License. See LICENSE for more information. Contact kezhenxu94 Project Link: https://github.com/kezhenxu94/ipass Acknowledgments Ben Dews - apw .
======>
https://github.com/davidlattimore/wild
-->>-->>
Repository files navigation README Apache-2.0 license MIT license Wild linker Wild is a linker with the goal of being very fast for iterative development. It's still very much a work-in-progress and definitely shouldn't be used for linking any production
binaries. Q&A Why another linker? Mold is already very fast, however it doesn't do incremental linking and the author has stated that
they don't intend to. Wild doesn't do incremental linking yet, but that is the end-goal. By writing
Wild in Rust, it's hoped that the complexity of incremental linking will be achievable. What's working? The following platforms / architectures are currently supported: x86-64 on Linux The following is working with the caveat that there may be bugs: Output to statically linked, non-relocatable binaries Output to statically linked, position-independent binaries (static-PIE) Output to dynamically linked binaries Output to shared objects (.so files) Rust proc-macros, when linked with Wild work Most of the top downloaded crates on crates.io have been tested with Wild and pass their tests Debug info What isn't yet supported? Lots of stuff. Here are some of the larger things that aren't yet done, roughly sorted by current
priority: Incremental linking Support for architectures other than x86-64 Support for a wider range of linker flags Linker scripts Mac support Windows support LTO How can I verify that Wild was used to link a binary? Install readelf , then run: readelf  -p .comment my-executable Look for a line like: Linker: Wild version 0.1.0 Or if you don't want to install readelf, you can probably get away with: strings my-executable | grep Linker Where did the name come from? It's somewhat of a tradition for linkers to end with the letters "ld". e.g. "GNU ld, "gold", "lld",
"mold". Since the end-goal is for the linker to be incremental, an "I" is added. Let's say the "W"
stands for "Wild", since recursive acronyms are popular in open-source projects. Benchmarks There are lots of features that Wild doesn't yet support, so I'm not sure benchmarking is super
useful at this stage. That said, I have done some very preliminary comparisons. I've tried linking
the binary in my warm build benchmark
repository , which builds an ~80MB, non-PIE,
statically linked binary with symbol tables, eh-frames and no debug info. On my laptop, I get the
following times: Linker Time (ms) Â± Standard deviation (ms) CPU time (ms) File size (MiB) GNU ld 12300 150 12299 80.3 gold 3365 30 3362 83.3 lld 905 5.6 1222 84.8 mold 457 7.2 2834 81.1 wild 363 6.6 1585 80.9 Notes about these results: CPU time is user + system CPU time as reported by hyperfine. Mold by default forks, which lets the user not wait for the mold process that does the work to
shutdown. This is a neat optimisation. In the above benchmarks, the time column is with this
optimisation enabled. The CPU time however is with this optimisation disabled (--no-fork), since
when forking is enabled, we can't easily measure the CPU time. I want to stress that this is only one benchmark. Many unknowns remain: Will the results be significantly different for other benchmarks? How will Wild scale up when linking much larger binaries and/or on systems with many CPU cores? Will implementing the missing features require changes to Wild's design that might slow it down? All we can really conclude from this benchmark is that Wild is currently reasonably efficient at
non-incremental linking and reasonable at taking advantage of a few threads. I don't think that
adding the missing features should change this benchmark significantly. i.e. adding support for
debug info really shouldn't change our speed when linking with no debug info. I can't be sure
however until I implement these missing features. If you decide to benchmark Wild against other linkers, in order to make it a fair comparison, you
should ensure that the other linkers aren't doing work on something that Wild doesn't support. In
particular: Wild defaults to --gc-sections , so for a fair comparison, that should be passed to all the
linkers. Wild defaults to -z now , so best to pass that to all linkers. There might be other flags that speed up the other linkers by letting them avoid some work that
they're currently doing. If you know of such flags, please let me know. Linking Rust code The following is a cargo test command-line that can be used to build and test a crate using Wild.
This has been run successfully on a few popular crates (e.g. ripgrep, serde, tokio, rand, bitflags).
It assumes that the "wild" binary is on your path. It also depends on the Clang compiler being
installed, since GCC doesn't allow using an arbitrary linker. RUSTFLAGS= " -Clinker=clang -Clink-args=--ld-path=wild " cargo test Contributing If you'd like to help out, I'd love to hear from you. It's a good idea to reach out first to avoid
duplication of effort. Also, it'll make it possible for me to provide hints that might make what
you're trying to do easier. Options for communicating: I like, where possible, to talk to people video video chat. You can book a time in my calendar . If time zones make this hard, let me
know via some other means and I'll see if we can find a time that works (I'm in Sydney,
Australia). Open an issue or a discussion here on github. Message me on the rust-lang Zulip Email me at dvdlttmr@gmail.com Sponsorship If you'd like to sponsor this work , that would be very
much appreciated. The more sponsorship I get the longer I can continue to work on this project full
time. License Licensed under either of Apache License, Version 2.0 or MIT license at your option. Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in
Wild by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any
additional terms or conditions.
======>
https://davidlattimore.github.io/
-->>-->>
Posts Video: Wild linker talk at GOSIM China 2024 2024-11-12 Rust dylib rabbit holes 2024-08-27 Testing a linker 2024-07-17 Speeding up rustc by being lazy 2024-06-05 Video: Rust Sydney - A linker in the Wild 2024-04-17 Wild linker - March update 2024-03-18 Speeding up the Rust edit-build-run cycle 2024-02-04 Making Rust supply chain attacks harder with Cackle 2023-10-09
======>
https://github.com/thomasschafer/scooter
-->>-->>
Repository files navigation README MIT license scooter Scooter is an interactive find-and-replace terminal UI app. Search with either a fixed string or a regular expression, enter a replacement, and interactively toggle which instances you want to replace. You can also specify a regex pattern for the file paths you want to search. If the instance you're attempting to replace has changed since the search was performed, e.g. if you've switched branches and that line no longer exists, that particular replacement won't occur: you'll see all such cases at the end. Features Scooter respects both .gitignore and .ignore files. You can add capture groups to the search regex and use them in the replacement string: for instance, if you use (\d) - (\w+) for the search text and ($2) "$1" as the replacement, then 9 - foo would be replaced with (foo) "9" . Installation Cargo Ensure you have cargo installed (see here ), and then run cargo install scooter Building from source Ensure you have cargo installed (see here ), then pull down the repo and run cargo install --path . Usage Run scooter in a terminal to launch Scooter. You can then enter some text to search with and text to replace matches with, toggle on or off fixed strings, and enter a regex pattern that filenames must match. A more extensive set of keymappings will be shown at the bottom of the window: these vary slightly depending on the screen you're on.

======>
https://github.com/nanov/rae-cli
-->>-->>
Repository files navigation README MIT license Real Academia EspaÃ±ola CLI Dictionary (rae-cli) nanov A command-line tool to search and display Spanish word definitions from the Real Academia EspaÃ±ola (RAE) dictionary. Installation macOS - Homebrew brew tap nanov/rae-cli
brew install rae-cli Linux - Community Support Needed Support and testing are needed for building distribution-specific packages. Contributions and feedback on this are welcome. Windows - Community Support Needed Support and testing are needed for packaging on Windows. Feedback on package managers and compatibility is appreciated. Alternatively, download the latest release from releases , unzip, and place the executable in a directory included in your PATH environment variable for easy access. Usage Search for word definitions from the Real Academia EspaÃ±ola. rae-cli < palabra > Arguments <palabra> : The word you want to search for. Options -h , --help : Display help. -V , --version : Display the version number. If the word is found, the tool will display its definition. If suggestions are available, a menu will appear for you to select the desired word.
Looks like I sent that before I finished! Hereâs the rest of the README: Tips For words with lengthy definitions (especially verbs, as full conjugation tables are displayed), combining rae-cli with a pager like less can make it easier to navigate. You can add this alias to your .bashrc or .zshrc file to automatically use less for pagination: function rae_cli () { rae-cli " $@ " | less -fX } alias rae=rae_cli With this setup, typing rae will call rae-cli with less by default, allowing you to scroll through longer entries easily. Copyleft Dimitar Nanov, MIT license

======>
https://github.com/kmdreko/venator
-->>-->>
Repository files navigation README MIT license Venator is a library and GUI application for recording, viewing, and filtering logs and spans from Rust programs instrumented with the tracing crate. It is purpose-built for rapid local development. This is currently in a "beta" state; bugs and quirks are to be expected but functionality should be complete. Bug reports and future feature requests are welcome. Usage In your instrumented program: [ dependencies ] venator = " 0.2.0 " use venator :: Venator ; Venator :: default ( ) . install ( ) ; Installing the Venator app (Rust 1.76 or newer): cargo install venator-app venator Screenshots:

======>
https://old.reddit.com/r/rust/comments/1gqdr41/invite_reviews_and_comments_on_my_very_first_rust/
-->>-->>
Hi all, Iâm very excited to share with you my very first Rust project,    ipass   , a CLI to interact with Apple macOS Passwords (iCloud KeyChain) (sorry non-macOS users). YES itâs my VERY FIRST Rust project literally, I didnât even write a Rust âHello Worldâ program before ipass, and thus I may wrote something looks garbage to senior Rust developers! So Iâm writing to invite you kindly try, review the codes, and comment anything you think of after reviewing the codes, no matter itâs about the project features, bugs, or the codes styles, code implementations, file structures, etc., if you think the project is well done, give it a star please! ð   

   Thank you in advance!!   
   

======>
https://old.reddit.com/r/rust/comments/1gq9ebm/whats_the_best_multidimensional_data_processing/
-->>-->>
I notice there were:   

   - ndarray   

   Is the currently the only and the best one?   

   How's the performance comapre wiht numpy in python?    

   I didn't saw any rust DL framework used this as underlaying data container though   
   

======>
https://gist.github.com/MarcusSorealheis/17307f41b6c632cb751c397280c70ed4
-->>-->>
NativeLink Open Source Bug Bounty Program ð¦ Raw easy_hard.md ð¦ NativeLink Open Source Bug Bounty Program ð¦ ð¤ð¤ ð Welcome to the NativeLink Open Source Bug Bounty Program! We invite engineers to contribute to our project and earn rewards by tackling bugs and implementing features. We want to buck some of the unsustainable paradigms of open source and support more community members. This document outlines the details of our bounty program, including how to participate, the rewards offered, and the criteria for winning a cash bounty. Current Issues This section will change periodically, whenever we approve a submission. You may want to bookmark this section. Current (updated 11/13/2024) Easy Mode : TraceMachina/nativelink#1325 Current (updated 11/13/2024) Hard Mode : TraceMachina/nativelink#1108 Bounty Categories and Rewards We offer two levels of bounties: Easy Mode Bounty Reward: $1,000 for the first bounty won. Bonus: Upon winning a second Easy Mode bounty, the reward doubles to $2,000 . Hard Mode Bounty Reward: $5,000 for the first bounty won. Bonus: Upon winning a second Hard Mode bounty, the reward doubles to $10,000 . Note: The doubling applies separately to each category. For example, if you win one Easy Mode bounty and then one Hard Mode bounty, each will have its initial reward. The doubling occurs when you win a second bounty within the same category. How to Win a Bounty To successfully claim a bounty, your submission must meet the following three criteria: 1. High-Quality Code and Tests Functional Code: Your code must effectively resolve the issue or implement the requested feature. Testing: Include comprehensive tests that cover various scenarios and edge cases. New functionality should have at least 80% test coverage. Code Quality: Follow Rust conventions and idiomatic practices where applicable. Ensure code is well-structured, readable, and maintainable. Use clear and descriptive naming for variables, functions, and modules. 2. Engineering Design Document Explanation of Solution: Provide a detailed design document that explains your approach to solving the problem. Content Guidelines: Architecture Overview: Describe the overall structure and components involved. Design Decisions: Justify your choices and discuss alternatives considered. Implementation Details: Explain how your code works, including algorithms and data structures used. Testing Strategy: Outline how you tested your solution and any tools or frameworks used. References for Strong Design Documents: Writing Effective Design Documents Software Design Documents - A Practical Guide Documenting Software Architectures 3. Timely Responses to Reviews Responsiveness: Aim to respond to code reviews and feedback within 24 hours . Exceptions: We respect religious observances and days off; please communicate any expected delays. Collaboration: Engage constructively with reviewers to refine and improve your submission. How to Participate To join our bounty program, please follow these steps: 1. Star and Fork the Repository Star the Repo: Show your support by starring our GitHub repository. Fork the Repo: Create a fork of the repository to work on your solution. 2. Identity Verification Be Prepared to Verify Your Identity: If your identity is not clear from your GitHub profile, you may need to provide additional verification before payment. This process helps us prevent fraud and ensures fair distribution of rewards. 3. Submission Guidelines Pull Request: Submit your code and design document via a pull request to our repository. Documentation: Include your design document in the pull request or two, preferably in a markdown file. Communication: Use the pull request thread to discuss any questions or clarifications needed. Payment Details Payment Method: We offer payment in every major currency, many other currencies, and USDC (USD Coin) if that is your preferred medium of exchange. Payment Process: Upon successful review and acceptance of your submission, we will coordinate payment. Ensure you provide the necessary details to receive the transfer. Additional Information Open Source Contribution: By participating, you agree that your code will be contributed under the project's open-source license and in adherence with our contributor license agreement. Multiple Bounties: You are welcome to work on multiple bounties, but please focus on one at a time unless otherwise agreed upon. Collaboration: Feel free to collaborate with others, but note that the reward will be split among contributors as agreed upon. Contact and Support Questions: If you have any questions or need assistance, please open an issue on the repository or contact us directly. Stay Updated: Watch the repository to stay informed about new bounties and project updates. We appreciate your interest in contributing to NativeLink. Your expertise and efforts help drive our project forward, and we are excited to see your innovative solutions! ð¦Happy coding!ð¦ Hyper Link to the NativeLink GitHub Repository TL;DR: NativeLink is offering bounties up to $10,000 for Rust developers who contribute high-quality code, provide a solid design document, and engage promptly in code reviews. We want to nurture our community. Star and fork our repo to get started! Sign up for free to join this conversation on GitHub .
    Already have an account? Sign in to comment
======>
https://old.reddit.com/r/rust/comments/1gq0x3t/video_of_wild_linker_talk_at_gosim_2024/
-->>-->>
A few weeks ago I gave a talk at the open source conference, GOSIM about the linker I've been writing in Rust called Wild.    The video of the talk    is now available on youtube.   

   Thank you to the GOSIM organisers for giving me the opportunity to come and speak. It was great to get to meet so many other rustaceans.   

   If you haven't read my previous posts about the Wild linker, they're    on my blog    and the Wild linker itself is    here on github   .   

   One small addendum - I mention in the talk about Linux locking executables while they are being run and this being a slight issue for my intended implementation of hot code reloading. While talking to kernel developer, Gary Guo after the conference, I learned that this locking was recently removed - so that may simplify the implementation slightly.   
   

======>
https://github.com/baehyunsol/ragit
-->>-->>
Repository files navigation README MIT license RAGIT RAGIT (rag-it) is a git-like software that turns your local files into a knowledge-base. The main goal of this project is to make knowledge-bases easy-to-create and easy-to-share. Why another RAG framework? RAGIT is very different from the other RAG frameworks. It adds a title and summary to every chunks. The summaries make AIs very easy to rerank chunks. It uses tfidf scores instead of vector searches. It first asks an AI to generate keywords from a query, then runs tfidf search with the keywords. It supports markdown files with images. It supports multi-turn queries (experimental). You can clone/push knowledge-bases, like git. push command is WIP. More documents Build Chunks Configuration Contribution Evaluation Prompt Engineering Quick Guide
======>
https://sdr-podcast.com/episodes/compile-time-crimes/
-->>-->>
Self-Directed Research Podcast About Contact Episodes Video Audio Show Notes Transcript - Postcard RPC and needed trickery - Problem: de-duplicating lists - Proc/decl macros only work with tokens - What about const fn? - Problem: there's no alloc in const (yet) - Time for a code example - This is the compiler's problem, not my runtime code's problem - const LIST_WITH_DUPES - That's not quite what we want - Write a function that extracts the uniques from the option array - Now we have a whole list of consts - Declarative macro: input as an ident - Limitations and room for improvement - Can we take this further? - The process on how to get a list of lists - Nerd snipe: Huffman coding - Make sure it can never be misunderstood - Internet of things & chips in everything Episode Sponsor Compile Time Crimes November 13, 2024 We shouldn't have to, but it turns out we can James explains how to combine macros and const-fns to work around limitations of what is possible at compile time, and how to do extremely wasteful calculations at compile time to deduplicate lists of things to make embedded systems go brrr View the presentation Video Audio Your browser does not support embedding M4A. Download as M4A Show Notes Episode Sponsor: CodeCrafters Cleanfeed Postcard-RPC and the SDR episode "Talking to Microcontrollers with Postcard-RPC" Disney animation "Paperman" and the trailer on YouTube HashSet Declarative macros or "Macros by example" Procedural macros tokens syn and Amos' take on X Abstract syntax tree (AST) const fn, Oli Scherer (oli-obk) Zig and its comptime , Miri Departure Mono font Ord trait MaybeUninit arrays Enums with "niche" representations rubicon and the SDR episode "Fixing build times with rubicon perfect hashing , Huffman coding , UTF-8 Deflate (aka Flate) , LZ4 compression algorithm Internet of things , MacBook M4 chips AppleTalk Transcript Amos Wenger: Both of you say something? Amanda Majorowicz: Something? James Munns: Hello, I am James. Amos Wenger: Yeah, we have you. I can see the levels in Cleanfeed, which makes me feel safe. I can even like hit the download button as many times as I want. Like, anxiety? Boom. Download. Now you have all the files on your disk. Amanda Majorowicz: That sounds like too many... I don't like it. I get anxiety by that . Amos Wenger: It's fine. I'm the one doing it. Amanda Majorowicz: I'm like: wait. No, no, no. No, no, no. That's too many files. Amos Wenger: Well, okay. I understand that. Yeah, but losing the audio is the higher anxiety. Surely. No? Amanda Majorowicz: Maybe, I don't know. It's intermittent- it's anxiety either way, I guess. Amos Wenger: Ok ok ok, we'll fight about it later. James Munns: We can have our own special anxieties. Amos Wenger: I understand. Amanda Majorowicz: Do you want the little bits of anxiety or just the really big potential anxiety? One of the two. Amos Wenger: All right. Well, James, what do you have for us today? Postcard RPC and needed trickery James Munns: Okay. I've been working on Postcard RPC because I got nerd sniped into how to fix some things that I didn't think needed to be fixed, but that required some trickery. So I've been working on doing tricky things at compile time. This is me documenting some of the silly tricks that I've done because I think you might appreciate them Amos. Amos Wenger: I hope so. James Munns: One of the things that I've been working on is preparing a list of schemas. So if I'm making a device and I want to say: these are all the types that the device can send or receive, sort of like an OpenAPI spec if you're familiar with those, but just a list of, "Hey, when you talk to me, these are all the different kinds of data you can send and receive." And in the past I hadn't been making them unique. I would just kind of do a macro expansion, and if you had 10 endpoints that all sent the same data, then you would just get 10 copies of that schema, but schemas aren't so big, so I wasn't too bothered by it. It wasn't great, but it also wasn't practically a problem. Amos Wenger: Right... do you have a unique identifier for them? Can you hash the schemas? How does that actually work? Because like, doing anything at compile time is complicated enough. But now I'm just thinking, even if you didn't have the compile time constraint, how do you... James Munns: The answer is yes. Yeah, so as a reminder, Postcard RPC- we did an episode on it, and I can't remember how deep I went into this part of it, but- I do have a const function that hashes the schema and the URI together, and produces an 8 byte key at const time for that. So you will have a const schema for a type, so there's a trait Schema that gives you a const SCHEMA, which is a type that is like a recursive struct that describes the data. And then I have a function that can take that type and produce an eight byte hash for that. And then that's usually what I use as my message identifiers on the wire. And that means you also don't need to calculate it at runtime. It just becomes this bag of eight bytes that you append to the front of every message. Problem: de-duplicating lists James Munns: So one of the things I would like to do is deduplicating both that set of key hashes, as well as the set of schemas, because if I am allowing you to download types, like reflection style, of what types are used for this, I need to deduplicate both the IDs. So these are all of my endpoints by ID, as well as these are all of the types that are used by all of those endpoints, and ideally deduplicating them, so that if I have five endpoints that all return the same struct, just in a different context, then I don't repeat that definition five times. Amos Wenger: Question, is it a concern of like, your binaries need to be as tiny as possible because you're doing this on embedded? Is it a constraint of, like it's a bandwidth problem? You want the response to be as small as possible? Why are you even deduplicating them in the first place? Or just someone said, "Oh, that's messy, you should fix it." James Munns: There's actually multiple parts to this here. The main answer is for bandwidth. So right now I use USB, and even though it's sort of the full speed, which is the lower USB 1.1 speeds. Amos Wenger: Right. James Munns: It's still fast enough, and the schemas aren't large enough, it isn't a problem. But I've talked to some people that want to use this over much lower bandwidth links- so either radios, or very old serial ports- saying: Oh, well, you're just gonna transfer five kilobytes of schemas is more problematic than crunching that all down to maybe a couple hundred bytes, ideally, particularly if we have very repetitive or very recursively nested types. So on one hand, I didn't know how to do it before and I managed to figure out how. So it's not like there's a specific pressure on this, but in this case, reducing the amount of packets that I need to send when you say, "Tell me about yourself!" I want to describe myself as succinctly as possible usually for bandwidth reasons. For some implementation reasons, a lot of these end up being statics or static references. So they actually do get kind of deduplicated by the linker already. So this is more about when I've been asked to describe myself, how many packets I need to send to describe myself and how large are those packets. Amos Wenger: For the past two minutes, I've been picturing... there's a short by I want to say Pixar, about a letter that goes through the air. Cause I've been thinking about Postcard over radio frequency. I have that short in mind. I think you should make that part of your branding. I was totally listening to your explanation. Please, please continue James. James Munns: I've kind of crunched out a very basic example of this so it's a little easier to follow if you don't know how all the guts of Postcard RPC work. But let's say I have a list of u32s, so just integers unsigned. The list is 1, 5, 10, 5, 20, 1. So we notice that there are some duplicates on that list, and if I wanted to get rid of those duplicates, I want to end up with an array that is 1, 5, 10, or 20. I don't particularly care about the order, but I don't want there to be any duplicates in this list. So I've removed one of the fives and one of the ones. Amos Wenger: Luckily, HashSet is const, right? Please tell me HashSet is const. Cause that's what I would do at runtime. James Munns: Well, there is an issue with that and we will get to that. Amos Wenger: Okay, okay. Proc/decl macros only work with tokens James Munns: So if I wanted to do this with a procedural or a declarative macro, the issue is that this only works with tokens. So procedural and declarative macros can do some really cool things and if you throw syn at the problem, you can write a proc macro that probably could do all the things you just described and take all those tokens in, convert them into integers, do the hash set of them, and produce a new token tree or whatever of the unique integers, and that would work. And it would work if I gave you the input of this array 1, 5, 10, 5, 20, 1. Amos Wenger: Yes. James Munns: But if I give you another const or a static or something like that, these macros only work with tokens. They can't see the value behind them, which means that it won't work if I just say I have a const LIST_WITH_DUPES because they can't understand that. Amos Wenger: Yeah, all they would see is the identifier. They wouldn't actually see the value that's behind that. And I don't think you can even fake your way around that. Cause syn, S Y N is a parser for Rust. It's an entirely separate parser from rustc's parser which have different functionality. Can we say bitched on the podcast? I bitched about it on X. You can bleep that if you want. I don't care. Anyway, uh, I've bitched about that before: that like, oh, rustc has its own parser and then syn is also a parser, and it's messy my head. In my perfect view of the world, there's only one parser that can do it all. But I was explained that actually, the rustc parser is better at recovering from parse errors and emitting very detailed diagnostics, error messages, yeah. Whereas syn is better at keeping the exact structure of the AST exactly the way written, even when it wouldn't matter for rustc, like new lines and the exact formatting of comments and everything. So it actually has a function, but then even if you have that instead of having a stream of tokens, like you said, like identifiers, numbers, then you still have just this tree of AST and it doesn't do any of the name lookup for you. James Munns: Yeah, and that's the problem, is that that is a token. That is an identifier, whatever you want to qualify it as, and in proc macro or declarative macro world, they work with tokens, they do not work with source code. They don't understand that it is an array, or that it has some length or anything like that. It is just an identifier that it has parsed as an identifier. And like I said, it can't do anything with this because macro world doesn't think about Rust code. It thinks about Rust source, if that makes sense. It's not tied to the semantics. What about const fn? James Munns: The other tool that we have here- and like I mentioned with that is const fn , which is Rust code, does think about Rust code, does have access to Rust code, but it has some restrictions to it. So you mentioned: oh, I would just throw it in a HashSet. Well, at least currently, you can't use heap allocations in const. And even if you could, there's no way to then turn it into a static that would work on my no standard embedded system or anything like that. And I think there is some interest, and I've talked to the const fn folks like Oli, and there is interest in the future of being able to make like a vec in a const fn and then turn it into a slice or a fixed size array, and then use that as just a plain const. I think, some other languages, like maybe Zig, have the ability to do that. Amos Wenger: I was gonna make that parallel. Zig has comptime- evaluation, compile time- and I think you can run any arbitrary Zig code in there because they don't care. But in Rust, const is actually run through Miri? Is that, yeah? And so, you have very limited capabilities. And I think like over time we've unlocked new abilities in const piecemeal. It's been very slow and very frustrating for people who are used to Zig where you can just like fucking run whatever code you want. James Munns: Yeah. I think it is something that will get better, but I'm working with stable Rust today. I'm sort of limited because we either start with a fixed size array, so like a [u32; 6] or a slice of u32 of variable size. So we start with that. And based on my answer, I said we want to end up with either a [u32; 4], so shrunk to just the uniques, or another slice of u32. Problem: there's no alloc in const (yet) James Munns: And there's this problem is that there's no alloc and const, which means I can't just put it in a vec, collect them all, and then dump it out. I need to figure out what type they are, because when you declare a const fn, like any other function in Rust, you have to say the signature of the inputs and the signature of the outputs when you define the function, which means different instances of this sort of need to take and return different pieces, but I found sort of a way to navigate this. Amos Wenger: I'm very curious about that. I have no idea how you got out of this one. Usually it's just like, "Well, const can't do it yet, so fuck it, too bad." James Munns: Yeah, yeah. So, step one: we take our [u32; 6]. It is a const value that we've, let's say, expanded in a macro, where I say: okay, you gave me the six things that you care about, I've done some other things in const world, and I end up with this list of six integers. And I know that it is an array of six integers. So my first step is: we can get the length of a constant array in const. .len() is a const fn, and so if I take a const value and call .len() on it, I can get the size of it. My first step is going to be to convert this [u32; 6] into an option [u32; 6] and a usize. Time for a code example James Munns: I have this const fn, which I'm pretty sure works. I kind of wrote it in pseudocode. So if it doesn't perfectly compile, I'll point you towards the real thing. We can say: we have a function called uniques, which takes a generic argument N, which is a usize, and a slice of u32s. Amos Wenger: Okay. Okay. I have to stop you, James. You know I have to. First of all, are you using a bitmap font? James Munns: Yes! Departure Mono, it's beautiful. Amos Wenger: Okay, I'm glad we clarified that. Second all: I think we need to tell the people,in Rust, they can make functions generic, so they can accept type parameters. A compare function can take two things that implement Ord, then you can compare them, they can be ints or strings or whatever. So you have this idea of type parameters, and like in Java, like in other languages with that kind of influence, it takes type parameters between chevrons? Do you have another name for them? James Munns: I'd say angle brackets, but... Amos Wenger: Angle brackets, exactly. But then in const code, you can also accept not just types, but also values. And so, if you haven't written const code before, and you're looking at the same slide_ I_ am, it will at first look a little weird, because between the angle brackets, there's something const N: usize . like, the function can be parameterized by any of the 4- no, that's 32 bits... James Munns: Yeah. These are const generics. This is just another flavor of generics. You can use them on const fn s, you can use them on non const fn s. You can have types, like a struct that is generic over const n usize. For example, if you wanted a struct that was generic over the size of a buffer, you could have that buffer be u8; N, where N is the const N usize thing like that. We use this a lot in embedded because we have to right size our buffers because we don't have an allocator very often. So yeah, this is const generics. We're parameterizing over a value rather than a type. We'll just take that as a given right now. So we have this N, which is a length. It's actually the length, that number six that we had before. And we take in a slice of u32s and we return an array of option u32 of length N and a usize. So the first thing we check is that the generic parameter N matches the length of our slice. That's just a little robustness of making sure that we handed the right thing in. And then we create a new output array that is just an array of None; N. And because iterator is a trait and we can't use trait methods in const fns, we get to hand roll our own while loops for iteration, which means I start my output index, which is going to track how many things we've placed in that out buffer, and i, which is where I'm kind of scanning my input from. It's back to the lovely C style- actually not even C style because we don't have for loops, we just have while loops at this point of manually managing our iteration. While we're iterating over the input slice, we're going to then iterate over anything in that out that has turned into a Some. So basically I'm just walking through the list. And if I find a new unique value, I kind of push it into this out. But instead of it being like a vec, it's an array of options and I just turn the nones into some as my push primitive here. Amos Wenger: I have so many notes on this, but I'm going to- I'm going to let you finish. This is the compiler's problem, not my runtime code's problem James Munns: Yeah, it's horrifically O(N^2), but again, this is the compiler's problem and not my runtime code's problem, so I'm not going to worry about it. I walk through the input list, pushing items that I find that are unique. If I find that they are not unique, I don't push them into the list because they're already there. And then at the end of this list, I'm left with my full array of six items. I will have pushed four items into it, and because of my outindex variable is 4, I return that whole option list and the number 4. Amos Wenger: Believe it or not, the fact that you're doing a linear lookup every time instead of hashing was not even one of my notes. This is not even the level I'm operating on right now. I'm looking at option u32 and I'm thinking there's no niche for u32. So you're essentially, you're allocating too much, but again, it's all in const, so who cares? James Munns: We'll get there. Amos Wenger: Okay. I like the trick that you don't know the size of the output, so you just fuck it, like every element is an option.You know the maximum number of them, so you just allocate a little too much and then you return, what, the actual size? No, I'm looking at this and going this is maybe uninit- sorry, maybe uninit? James Munns: Yeah, there's some tricks and I tried using this. The answer is that uninit arrays are hard to do with stable const today because at the end you would want to then turn it into an init array. So I did try that, but the answer is: not quite enough things are stable in const with maybe uninit, specifically, maybe uninit arrays. Amos Wenger: Right. James Munns: There might be a way to do it. I just didn't figure it out. Amos Wenger: Those are weird. They don't fit in my need do an episode about them just so that I can refer back to it and then understand them properly. James Munns: Cool. So this is a neat function. It takes some values. const LIST_WITH_DUPES James Munns: Let's look at what it might look like. So I have this array const LIST_WITH_DUPES . It is a slice. It's got six items. I can make another const just called len that is a usize that just takes that first constant and calls .len() on it because len is a const fn. You can take the input of another const, call a const function on it and get a value. So we can then get len. And then I can make this third const called intermediate, which is that tuple of an array of options of len with a usize, and then I can call my const fn, uniques::&lt;LEN&gt; , and then pass it the slice, list with dupes. So then I end up with this array and the length of unique items in it. Amos Wenger: Why can't you just do all of that inside of a const fn ? Is what I'm not understanding. If you pass it a u32 slice, can't you just then call len inside of there? James Munns: Sure, and what's your return type? Amos Wenger: That's... okay. James Munns: So ideally you would have a slice, but then you have to return something, and you can't allocate the array inside of the function and then return a slice of it, because then you haven't bound that to anything. Amos Wenger: Sure. Okay. James Munns: So that's the answer is: because we need to figure out the types in each step. Amos Wenger: Makes sense. Gotcha. That's not quite what we want James Munns: That's not quite what we want. We now have all of our uniques, but that's not how we would want to use them where we have to like iterate through this array. And like you said, options are probably going to double the size of a u32 array because there's no niche and with alignment, it needs to work out. We're essentially doubling our size, actually more than that: we want to have four items in the end, and we now have like, 12 words worth of stuff because we have an option u32 or something like that. Amos Wenger: Yeah, we have identified the uniques, but we haven't shrunken anything. We're actually using more memory. Maybe we should explain what a niche is? It's like for some types, like if you have non zero u32, for example, you know that zero is not a legal value for that. So you can use that to signify none. If you have an option non zero u32, then it's just like a u32, but zero means none and all the other values are some, something. Rust does that a lot. You can do that with like non null pointers. You can do that with enums. An option of an enum is not necessarily larger than the enum. James Munns: The real code for this definitely does that because I have an option reference to the schema when I'm deduplicating it. And because references can never be zero, Amos Wenger: Cannot be null yes. James Munns: So they also get niche'd. So even my option reference is still just the size of a reference. I do take advantage of that there, but not here because this is a simplified example. Write a function that extracts the uniques from the option array James Munns: That's not quite what we want. Let's write that function that extracts the uniques from that option array. Now I have a const fn that is const M: usize . So it's generic over const M usize. And we take a slice in, which is our slice of option u32, and we return an array of u32 of length M. We start by creating our output, which is that array of u32 M. We're going to start iterating where we know M is the size of our output. That's how many unique items we expect to have. So we start iterating from zero until we hit that number, and we unwrap each of the items. And this is another one of those cool things that const is if you unwrap and panic in a const fn, the compile just fails. It is a compile time error rather than a run time error. So we go through and we fill in every value of the output array with Some. And then just as a reasonable check, a robustness check, we then keep iterating to the end of the input array and make sure that all of those are none. So we're kind of verifying that: yes, we got in a slice of six items, you told me there were going to be four unique items in there, I was able to extract all four unique items, and then that additional space was just nothing. Now we have a whole list of consts James Munns: When we put that all together, now we have a whole list of consts. The first one is our input array, the second one is the length of that input array, then we do that intermediate step where we turn it into option, array, and the unique size. We then get the deduplicated length, which is just that usize from the intermediate. And then we can call our extract function with the deduplicated length and the input array of options. And then finally, we can turn that fixed sized array of four items into a slice of u32s. Because, again, in const, you can take a reference to a const value as a const, and the compiler will sort it out for you. Amos Wenger: James, you know how some people have prank shows where they take turns pranking each other? Is that- am I on one of those shows right now? Because I'm looking at the thing and I don't know whether to be impressed or disgusted which if I recall correctly is how you felt about my, "Oh we can cast that lifetime to static because it's just for the duration of a call and it's blocking so within that thread it stays valid," and you were like, "I don't know if that's true..." James Munns: Yeah, I mean, that's what I go for. I love solutions that are just: how could you, but I couldn't think of a different, better way to do that. Amos Wenger: Please tell me that you actually have a declarative macro that actually puts all that in module and hides it away from you. Please tell me that. Declarative macro: input as an ident James Munns: I'm so glad you asked. So here is that, uh, declarative macro, because you can take the input as an ident, and this is a really interesting technique. It makes sense, but you can kind of smash together macros with const fns, and actually sort of bounce between the two, because you can use macros inside of const fn s, and you can use const fn s inside of macros. And even though it seems unreasonable, because you think, "Oh well, what are the stages of compilation?" You can actually sort of bounce back and forth between the two of them and have all of these intermediate consts, which very often then get used as the input to the next stage of something that a macro generates, or have a macro generate multiple const fn's or multiple constant values and sort of bounce between the two until you end up with this. So this is really only sort of one layer where I'm using a declarative macro just because you have to do this multi step process and the user doesn't want any of those intermediate values. They just want the final value out. Now I have my const LIST_WITH_DUPES , which is an array of u32s, and then I have my const LIST_DEDUPED , which is also a slice of u32s, and I just call that dedupe macro, which expands inside of a const block, which means none of those intermediate values are going to end up in my final program, only the calculated subset. We might not even end up with the list with dupes in our program if we don't reference it anywhere. Only the consts that we reference are going to be used in our program. Which means we might just have all of this intermediate garbage at compile time. But, we end up with the perfectly sized array of unique integers at runtime that our microcontroller or whatever can use with no allocations or anything. Amos Wenger: You will not remove from my head the thought... and I know I did rubicon. I presented rubicon on this very podcast! So who am I to say anything? But: if people looked at that, they would be like, "You know what? There's something wrong with Rust and the Rust people and I don't want to touch it." I would understand. It would make sense to me. I couldn't blame them, because it shouldn't exist! Limitations and room for improvement James Munns: I think there's room for improvement, don't get me wrong. I talked to Oli about this, and Oli's first response was, "Oh wow." Which made me feel good, because Oli's the one that wrote a lot of the const pieces, or one of the original architect of a lot of the const stuff. But also said: hey, you know. I'm going to have some time in the near future. And these are exactly the pain points we want to get rid of. Not having to have these intermediate steps... no roadmap on this, but like I said, being able to do allocations in const functions and then returning the array because that would make all of these tricks pointless. You would just have one const fn - Amos Wenger: Of course. James Munns: - that has the signature we want here, which is just slice to slice. Amos Wenger: Just like rubicon should not exist because it's working around limitations of the Rust compiler and you could, you should just really go into the Rust compiler and add the flags that you need. It's so interesting because if you didn't know why it is that way- really, I'm trying to approach this from the perspective of someone who's not using Rust on a daily basis and looking all this and going like, "Dear God, why?" But it's because Rust is doing all this in like super hardcore difficulty because it's not just compiling something to a binary and running it. That's proc macros. Rust does do that in some context. But for const, it's none of that. It's interpreting things and it's not interpreting things the easy way either. It is like doing tracking memory allocations and tagging pointers and doing this weird CHERI like thing because Miri is actually supposed to help you detect undefined behavior and unsafe code. That's where the limitations come from. I'm actually not exactly sure... why do we need all the might of Miri to just do const fn s, but I dunno. Do you? James Munns: Well, I mean,you're going to need to run the code at compile time. And especially if you're cross compiling, you need to work essentially within the Rust abstract machine. So the neat thing about Miri is that it is essentially a Rust abstract machine interpreter, which is why we use it as essentially a sanitizer a lot of the time, because the VM can introspect the world that's going on there. I don't know exactly the decisions that got made there, but it was: well, if you have to abstractly interpret code at compile time, because it might be for a different architecture anyway, then why not have one really good one that can also - Amos Wenger: Yeah. James Munns: -enforce all the invariants of the abstract model at the same time? Amos Wenger: I think it's a case of like: can do more, so it can also do less. In that context, we don't need all the- although I guess it could detect memory unsafety in your const fn code. Like can you have unsafe const fn code? James Munns: It does. You can. Amos Wenger: That's terrible. James Munns: And if you ever do something that would be undefined behavior, your code won't compile. Amos Wenger: I'm revolted. I'm leaving Rust. James Munns: So const fn can have pointers and can have unsafe code and that's why maybe uninit could be possible here. It's just that not all the methods have been made const yet. Amos Wenger: Sure. Can we take this further? James Munns: So... can we take this further? Amos Wenger: No! Enough! James Munns: The answer is yes, because what if we had a list of lists? So we have a slice of slices, and all of my slices can be different sizes, and what I'd like to end up is with one slice that contains all of the items that are used here. So where this maps back to reality is when you have these endpoints- Amos Wenger: I'm sorry. Time out. Where this maps back to reality? 30 minutes into this episode? It was just a great line. James Munns: So where this maps back to reality is because if you have an endpoint that contains multiple types, so it might have the input type and the output type and the input type might be a composite type of a- like a struct or things like that, it might have its own list of types that are involved in the request and response. If I have this array of four endpoints, I'm going to end up with this list of each of their types. And like I said, if I'm trying to figure out the list of unique types in the entire system, then I want one array of all the inputs and all the outputs, because there might be duplicates between my different endpoints. Amos Wenger: So you're also doing flattening... James Munns: I don't have the code written up for this because I wrote these slides about five minutes before we gave the talk, but I'll walk you through the process. Amos Wenger: James... The process on how to get a list of lists James Munns: So first, we write a const fn that gets the total size. It just walks through the arrays and counts the length of each subarray. So we would say the first one's five, the second one's one, the third one's whatever the number it was. And we just get a sum of what is the upper bound of potential unique values that we could have. And then we turn around and use that as our option u32 allocator basically because we go: if there was absolutely no duplication, this list could be at most this many elements large. And then we repeat the same process that we did for a single one. We fill all of the sublists into our allocated chunk, and then we get the total number out of how many uniques there were. Then we're back to the same tricks. We then crunch it back down, we end up with one slice, and we can end up with this same sort of behavior. And again, we can write a little procedural macro that hides all of these intermediate steps that are horrific and necessary. But we can end up from an array of array of u32s to a single array of deduplicated u32s. And like I said, even though it is very wasteful, I'll tell you the algorithm in Postcard RPC also does what's called perfect hashing. When I have all of those keys, I say, "Well, they're all eight byte keys. Could I smash them all down to one byte keys? And would there be any collisions? If yes, could I smash them down to 2 byte keys and not have any collisions?" and if so, then I go, "Ah, I don't need 8 byte keys, I need 2 byte keys," which, when I was talking about what I send over the wire, now I've just shrunk the header of every message from containing an 8 byte key to a 2 byte key, and I can calculate that with no misses at compile time. Which means that you can never screw it up because it's using all the types you're actually using. And it can always get exactly the right number. So this technique is generally called perfect hashing, but this is also something we can do at const time. Nerd snipe: Huffman coding Amos Wenger: So, James, I've already been very annoying to you this episode,I want to thank you for your patience, but I have another nerd snipe, for the next episode, of course, which is you should do Huffman, because- James Munns: Ooh... Amos Wenger: Right? Maybe you have more than 256 different message types or schemas to deduplicate. But some of them come up more often than not. James Munns: That's fair. Amos Wenger: I think the best explanation... Let's put it in terms of UTF-8, they thought. Because that's what people know, instead of Huffman. Okay in UTF-8 uh, if you send "a, b, c" a, b, and c all each take only one byte. But then you have this seventh bit? Eighth bit? What, what am I thinking of? James Munns: Eighth bit. That's the extension bit. Amos Wenger: Eighth bit. Yeah, it's the extension bit. So you can have characters, or whatever you want to call them. Okay, this is not the episode about UTF-8. You can have multi byte sequence. So you can have more than 256 or more than 128 different letters. And you could do the same with the schemas. Like maybe some schemas are used very rarely, and you can afford to have like a two, three, four bytes prefix for those messages. But then the ones you use the most often, and you can actually do that because you know how often they show up in schemas. So let's see you that in const code, James! James Munns: You know how often they show up on schemas, but not how often they are sent over the wire. Amos Wenger: That's true. James Munns: You could have a very common type that is very, very rarely actually sent. Amos Wenger: Exactly. So, you'd have a likely annotation? You do profile guided optimization of schema prefixes? Make sure it can never be misunderstood James Munns: This is more about making sure that it can never be misunderstood. Cause that's one of the big things about Postcard RPC is you could do all of these tricks, manually assign message IDs, manually bump them to go, "Oh, we just rolled over 255 items. Now we need 256 items. So now all of my keys need to bump up to two bytes." You could do that, but I've seen those bugs in production before where you have one system that thinks it's talking one byte keys and another system that's talking two byte keys. And this is also a reason why I chose varints for Postcard RPC you can never misunderstand the length of an integer because they're all varints all the time, which means there's never any room for confusion even if you're compiling on different days. And this is the same thing is I have a deterministic way of hashing and a deterministic way of crunching from 8 to 4 to 2 to 1, which means that also if you have a system that natively is thinking in 1 byte IDs but someone sends it an 8 byte ID, It can crunch that 8 down to 1 and go, "Oh, I know what you're talking about." And because you know that the set of messages that it can accept is representable in one byte, that's always acceptable to do that crunching. And if I'm natively thinking in two bytes and you send me one byte, I go: No, I won't crunch that down because I already proved that I need at least two bytes Amos Wenger: Right, yeah. James Munns: I don't know if this could collide or not. So I'll refuse to expand it or crunch my numbers down to match it. Amos Wenger: Well, I'm just glad I got to name drop Huffman, to be completely honest with you. James Munns: It's very cool. And I think if I wasn't on embedded, I would just do compression because Flate or LZ4 use Huffman encoding as part of the encoding process. And so it is absolutely the right tool to use here, if you're willing to pay for compression. Amos Wenger: You know, I'm glad that we have embedded. You have the internet of things, and people complain about that, call it the internet of something else , that I'm not gonna say on the podcast, even though I could... Amanda Majorowicz: But what? Amos Wenger: It's the internet of shit. James Munns: You'll drop "fuck" about 10 times and not shit? Amos Wenger: You made me say it! Amanda Majorowicz: I'm just curious. James Munns: We're going to have to check the explicit check box on this episode. Internet of things & chips in everything Amos Wenger: Yeah. You have chips in everything. I have chips in my light bulbs at home. But I'm glad, because we have to think about these tricks again. We have to think in terms of constrained resources because there was a gap, like after computers became ridiculously big and powerful and before we started putting chips in light bulbs, where we suddenly didn't have to care anymore, we could be wasteful with everything. And now, sure, we have the Macbook M4... that's like infinitely powerful, and nobody knows what to do with that much compute- yes they do, it's running inference on LLMs- and then we have those tiny chips. James Munns: This is one of my favorite things because we're getting to the point where microcontrollers are as powerful as the computers from the late 80s and early 90s like desktop computers. There's still this mindset and a lot of embedded is that microcontrollers are very tiny. You can't do anything fancy with them. They need to be bare bones. And this is where a lot of that internet of shit stuff has come from in that people were like, "Oh, I can't afford encryption or authentication because I'm just a little guy." And that's not great. A lot of what I've been doing with my network protocol stuff is looking at things like AppleTalk which was done on computers in the early 90s and going: well, if the Mac from 1990 could afford to do it, our microcontrollers can probably afford to do that. And they had networking and printer sharing and file systems and things like that. Sometimes you don't want all those things because you're saving power or whatever, but if you're thinking of "I have to be miserable because I have to shave every byte off of everything," that's not necessarily true. And particularly when we have better compilers like Rust and we can cheat and do a lot of those things at compile time to leverage our ridiculously powerful MacBooks and do all the work ahead of time so that our tiny little 16 kilobyte firmware device doesn't have to worry its pretty little head about it then: yeah, that's exactly the kind of tricks I like doing. Amos Wenger: Yeah, well, you got my vote. Episode Sponsor CodeCrafters is a service for learning programming skills by doing . CodeCrafters offers a curated list of exercises for learning programming languages like Rust or learning skills like building an interpreter. Instead of just following a tutorial, you can instead clone a repo that contains all of the boilerplate already, and make progress by running tests and pushing commits that are checked by the server, allowing you to move on to the next step. If you enjoy learning by doing, sign up today , or use the link in the show notes to start your free trial. If you decide to upgrade, you'll get a discount and a portion of the sale will support this podcast. Find us on YouTube | Bluesky | Mastodon | Twitter | TikTok | Instagram OneVariable UG Â© 2024 | Impressum | Data Privacy const scrollHandler = entries => {
        // Find the first entry which intersecting and ratio > 0.9 to highlight.
        let entry = entries.find(entry => {
            return entry.isIntersecting && entry.intersectionRatio > 0.9;
        });
        if (!entry) return;

        document.querySelectorAll(".toc a").forEach((item) => {
            item.classList.remove("active");
        });

        // let url = new URL(`#${entry.target.id}`);
        let link = document.querySelector(`.toc a[href$="${decodeURIComponent(`#${entry.target.id}`)}"]`)
        if (link) {
            link.classList.add("active");
            link.scrollIntoView({ behavior: "auto", block: "nearest" });
        }
    };
    // Set -100px root margin to improve highlight experience.
    const observer = new IntersectionObserver(scrollHandler, { threshold: 1 });
    let items = document.querySelectorAll('h1,h2,h3,h4,h5,h6');
    items.forEach(item => observer.observe(item));
======>
https://foundation.rust-lang.org/news/rust-foundation-releases-problem-statement-on-c-rust-interoperability/
-->>-->>
Home About News Resources Membership Grants program Contact Careers Rust Foundation Releases Problem Statement on C++/Rust Interoperability New publication outlines a strategic vision for improving language interoperability and calls for community input. by Rust Foundation Team 12 Nov 2024 DOVER, DELAWARE, November 12, 2024 â The Rust Foundation, an independent non-profit dedicated to stewarding and advancing the Rust programming language, released a comprehensive problem statement addressing the challenges and opportunities in C++ and Rust interoperability. This publication marks a significant step toward making cross-language development more accessible and approachable for the wider programming community. The problem statement outlines three key strategic approaches: Improve existing tools and address tactical issues within the Rust Project to reduce interoperability friction and risk in the short term. Build consensus around long-term goals requiring changes to Rust itself and develop the tactical approaches to begin pursuing them. Engage with the C++ community and committee to improve the quality of interoperation for both languages to help realize the mutual goals of safety and performance. About the Initiative # This âInterop Initiativeâ was launched in February 2024 with a generous $1M contribution from Google . The initiative recognizes that both C++ and Rust have essential roles in the future of systems programming. While both languages excel in similar domains, using them together effectively is crucial for organizations seeking to balance safety and performance with maintainability. The core challenge lies in developing a mature, standardized approach to C++ and Rust interoperability, despite Rust's historical focus on C compatibility. Under the guidance of Jon Bauman , who joined as Rust-C++ Interoperability Engineer in June 2024, the Initiative proposes a collaborative, problem-space approach engaging key stakeholders from both language communities. Rather than prescribing specific solutions, this problem statement serves as a foundation for community input and participation in shaping both the strategic direction and tactical implementation of improved C++/Rust interoperability. Read the Full C++/Rust Interoperability Problem Statement Get Involved # We welcome feedback on this problem statement and participation from the community. Here's how you can engage with the Interop Initiative: Join the discussion in the t-lang/interop Zulip channel Email your suggestions to interop@rustfoundation.org Follow the Rust Foundation blog for regular updates Review and comment on the full problem statement Progress updates will be shared through these channels and presented to the Rust Foundation board, which includes members of the Rust Project. About the Rust Foundation # The Rust Foundation is an independent non-profit organization dedicated to stewarding the Rust programming language, nurturing the Rust ecosystem, and supporting the set of maintainers governing and developing the project. Learn more at rustfoundation.org . Tagged: announcement foundation Previous: Announcing the Rust Foundationâs Newest Project Director: Carol Nichols Anti-Bribery and Anti-Corruption Policy | Anti-Trust Policy | Bylaws | Cloud Compute Program | Code of Conduct | Conflict of Interest Policy | Copyright Policy | Intellectual Property Policy | Logo Policy and Media Guide | Privacy Policy | Statement on Global Regulations | Whistleblower Policy
======>
https://old.reddit.com/r/rust/comments/1gqm5rw/interactive_find_and_replace_tui/
-->>-->>
Hi all, I've just released my first Rust project, Scooter, which is a terminal UI app designed for quick find-and-replace while allowing you to select exactly which instances you want to replace. If any files change between selection and replacement, affected instances won't be changed and you'll see these as errors at the end. I'd love any feedback!   

   https://i.redd.it/ery42s7n6q0e1.gif   

   Check it out here:    https://github.com/thomasschafer/scooter        
   

======>
https://old.reddit.com/r/rust/comments/1gqi2nw/cli_for_real_academia_espaÃ±ola_spanish_language/
-->>-->>
I've started learning Spanish a few months ago, and I find the RAE dictionary and website ( something like an equivalent of the Cambridge Dictionary ) extremely useful in the process of learning, once one has made it after the very beginner level. At the same time, in the past few years I get less and less time to do some on-hands work, and when I do it's for a simple POC to demonstrate a point.   

   So I decided to get more familiar with Rust ( C/C++/C#/Js background ) and write a small CLI to search for definitions in RAE, so I will be able to search for words without leaving my terminal.   

   This is the result:   
   https://github.com/nanov/rae-cli   

   Pretty straight forward, but I really got to like some of Rusts principles, and how compile errors are reported.   

   I would appreciate some feedback, and will be even more happy if someone will find it useful.   
   

======>
https://old.reddit.com/r/rust/comments/1gqgd6a/venator_my_log_and_span_viewer_for_tracing_is_now/
-->>-->>
I've been working on this as a passion project for nearly six months and is now MVP feature-complete (and mostly bug free). Here it is in action:   

   https://i.redd.it/fwn5w9g70p0e1.gif   

   https://github.com/kmdreko/venator   

   I built it to be good companion for local development - where basic logs are hard to follow but other solutions are too cumbersome. It is simple to set up: simply set up the    venator    subscriber in your    tracing   -instrumented application and it will export event and span data to the Venator application.   

   I am eager to get any kind of feedback for it: bug reports, UI improvements, feature requests, PRs, etc. Intend to go 1.0 after a round of performance improvements, refactoring, and battle-testing.   

   Hope others find it useful!   
   

======>
https://filtra.io/rust/jobs-report/oct-24
-->>-->>
Rust Jobs Report: October 2024 Welcome to the October 2024 Rust Jobs Report by filtra. Please enjoy our overview of the Rust job market. To add your or another company to the filtra Rust jobs index, please email filtra@filtra.io with the subject line "Rust Index" Want to advertise here? Reach out! filtra@filtra.io Are There Jobs in Rust? 959 October was a huge month for the report. The total number of postings we found jumped by almost 100 to 959! 1000 here we come! How Many Companies Use Rust? 114 While less dramatic, the number of hiring companies also increased month over month! What Companies Use Rust Most? Amazon 239 IBM 58 Apple 56 Microsoft 52 DataDog 40 Cloudflare 39 Tesla 29 Canonical 26 Workato 23 xAI 21 Some company called Amazon is hiring a lot of Rust engineers. Hm, never heard of 'em ; ) What Other Companies Use Rust? Crypto.com 18 Fortanix 14 Disney 14 SpaceX 13 GitHub 13 Fastly 13 Figma 12 Google 11 Worldcoin 8 KeyRock 8 Deliveroo 8 Sentry 8 HelsingAI 7 Discord 7 Data Direct Networks 6 HyperExponential 6 Syndica 6 OpenAI 5 Scaleway 5 Parity 5 Smarkets 5 IOHK 5 Mozilla 5 Volume Finance 5 1Password 5 Deep Fence 5 informal 5 Conduit 5 Mercedes 5 ApolloGraphQL 4 Ellipsis Labs 4 StarkWare 4 Ditto 4 Vivint 4 Matic 4 Matter Labs 4 Akamai 4 Heliax 4 Threema 4 Open Cosmos 3 AllSpice 3 Stockly 3 HealPay 3 Chorus One 3 Svix 3 Axiom 3 Zed Industries 3 Ardan Labs 3 Fullstory 3 Zama 3 InfinyOn 3 ZORA 2 amo 2 Zoo 2 arm 2 Meta 2 OpenTalk 2 Railway 2 Qumulo 2 Activision 2 Dropbox 2 10X Genomics 2 Qovery 2 OneSignal 2 Embark 2 Hugging Face 2 Uniswap Labs 1 AppSignal 1 Signal 1 Atlassian 1 Qwiet AI 1 PayPal 1 Volka 1 Aztec 1 Osmosis Labs 1 Immunant 1 Swift Navigation 1 Two Sigma 1 SurrealDB 1 Tonari 1 Shadow 1 Polybase 1 Vercel 1 Prisma 1 Clever Cloud 1 Rapid7 1 Toyota Connected 1 CancerIQ 1 PingCAP 1 Tabnine 1 LINE 1 shuttle 1 KION 1 Liquid Analytics 1 Star Lab 1 DSCVR 1 Grafbase 1 Materialize 1 automata 1 Scythe Robotics 1 Astropad 1 Cymo 1 FP Complete 1 Ledger 1 The list of other companies using Rust seems to get deeper every month. This month we've got 8 companies with over ten listings! What Industries Use Rust Most? cloud/infrastructure 315 productivity 106 crypto 93 systems/hardware 88 consultancy 63 data science 51 monitoring/reliability 49 iot/robotics/automotive 45 security 38 messaging/notifications 17 With Amazon using Rust so heavily to build out AWS, the largest industry for Rust jobs is cloud/infrastructure. What Other Industries Use Rust? browser 16 aerospace 16 fintech 15 streaming 14 marketplace 11 databases 8 gaming 5 social media 4 health/biotech 3 animation 1 networking 1 The list of other industries may not be that valuable these days. Rust's versatility and increasing adoption mean it can be used and is being used for almost everything. But, the language clearly lends itself to a few applications as you see in the previous list of top industries. Are Rust Jobs Only For Senior Devs? junior 22 mid 531 senior 406 Any company that sticks their neck out to hire juniors will likely scoop up some serious fledgling hackers. We know for a fact there's appetite for a lot more than 22 entry-level positions. get rust jobs on filtra Data for this edition of the Rust Jobs Report is filtra platform data supplemented with public job postings. Feel free to visit the filtra Rust blog to see past reports. Rust Jobs Report: October 2024 Welcome to the October 2024 Rust Jobs Report by filtra. Please enjoy our overview of the Rust job market. To add your or another company to the filtra Rust jobs index, please email filtra@filtra.io with the subject line "Rust Index"
======>
https://maelstrom-software.com/blog/spawning-processes-on-linux/
-->>-->>
Implementing a Container Runtime Part 1: Spawning Processes on Linux 2024-11-13 by Remi Bernotavicius <remi@abort.cc> Spawning child processes using your programming languageâs provided APIs can be very straightforward
in a modern language. For example Rustâs std::process:Command provides an easy
interface: use anyhow::Result; fn std_command_spawn_wait () -> Result <()> { let mut child = std::process::Command::new( "/bin/echo" ) . stdout (std::process::Stdio::null()) . spawn () ? ; let status = child. wait () ? ; assert! (status. success ()); Ok (()) } These APIs make it really easy to get things right. They are the first thing you should reach for.
Sometimes, though, you may find yourself needing to do things that just arenât supported by these
simple APIs. We found ourselves in this position working on Maelstrom, since we run each test in its
own set of Linux namespaces. Maybe you too want to do something with namespaces, or maybe you want
to use a pidfd . If thatâs the case, then you might need to dig deeper and discover what the
underlying APIs are capable of. Of course, once you dig deeper, you might quickly find yourself confused. On Linux there are several
APIs that all spawn a child process. There is fork , vfork , posix_spawn and clone . So which
one do you pick? How are they different? This article tries to answer these questions. Also, at the end I provide a simple
flow chart that I hope makes it easy for you to decide which approach to take. The code examples I provide will be in Rust, which is what Maelstrom is written in, but the
information should apply to all programming languages. The examples will use the maelstrom_linux crate which is our own wrapper around libc and the kernel specific to the Maelstrom project. You
may want to write your own wrappers or use some crate like nix . The fork and exec model The classic way of spawning a child process on Linux and Unix is to use two syscalls together, fork and exec . fork creates a copy of the current process. exec loads and executes a new
program in the current process. Splitting it up into two different syscalls allows the developer to do a bunch of setup in the child
process just by writing regular code. Having this be separated into two calls with each doing one
thing, and the ability to write this setup as plain code, is sometimes referred to as the âeleganceâ
of fork . The following code snippet runs echo with no arguments which acts as a kind of no-op for
benchmarking purposes. use anyhow::Result; use maelstrom_linux::{ self as linux, ExitCode, WaitStatus}; fn fork_execve_wait ( dev_null : & impl linux::AsFd) -> Result <()> { if let Some (child) = linux::fork() ? { // This is executed by the parent let wait_result = linux::wait() ? ; assert_eq! (wait_result.pid, child); assert_eq! (wait_result.status, WaitStatus::Exited(ExitCode::from_u8( 0 ))); Ok (()) } else { // This is executed by the child // Do the housekeeping before we call execve linux::dup2(dev_null, & linux::Fd:: STDOUT ). unwrap (); linux::execve(c "/bin/echo" , & [ None ], & [ None ]). unwrap (); unreachable! () } } I will refer to this setup period in the child before calling exec as the âhousekeepingâ. In this
housekeeping code you can imagine doing a number of different things, most of them being syscalls.
This can include configuring file descriptors, sessions, users, groups, or namespaces for the child.
Also, since exec is a separate function, you can choose to not call it and instead just continue
executing the current program. fork in Multi-Threaded Programs fork seems pretty simple until you start to consider multi-threaded programs. When you call fork in a multi-threaded program only the calling thread is copied from the parent process. This creates
a strange programming environment in the child. In particular locks wonât function properly
since a thread holding a lock can just disappear after the fork! The idea of avoiding certain things
after forking I will refer to as âfork safety.â Fork safety can sometimes be tricky to maintain. Simple things like allocating memory may not work
due to the aforementioned locking problem, and memory allocations are often everywhere. A real
gotcha here are signal handlers, they are copied from the parent so it means that can be called in
the child but they have a high chance of accessing some global memory requiring a lock. One solution to this problem is to employ a programming pattern known as âzygoteâ. In this
pattern we fork a child process (called the zygote) before we become multi-threaded and then tell
our zygote via IPC to spawn any further children we want. +---------+         +---------+          +---------+  +-------+ | parent  |         | zygote  |          | kernel  |  | child | +---------+         +---------+          +---------+  +-------+ |                   |                    |           | | IPC request       |                    |           | |------------------>|                    |           | |                   |                    |           | |                   | fork               |           | |                   |------------------->|           | |                   |                    |           | |                   |                    | spawn     | |                   |                    |---------->| |                   |                    |           | |                   |                    |      exec | |                   |                    |<----------| |                   |                    |           | |                   |                    |      exit | |                   |                    |<----------| |                   |                    |           | |                   |      wait response |           | |                   |<-------------------|           | |                   |                    |           | |      IPC response |                    |           | |<------------------|                    |           | |                   |                    |           | The zygote remains single-threaded so we can write the housekeeping code as before. Having to
communicate with the zygote is a little annoying, but makes things easy to get right at least from a
fork safety perspective. fork is Too Slow! It turns out that fork can be pretty slow in some contexts. A large part of the time is spent
copying the virtual memory mappings of the parent, and the time it takes scales linearly with the
amount of memory mapped. One way around this is the aforementioned âzygoteâ pattern. Since the
âzygoteâ process is spawned early, it shouldnât have much memory mapped. This doesnât completely
avoid the problem though, because copying the virtual memory of a small process is still slow. The graph shows how the time of the fork call increases linearly with the amount of memory mapped.
It would be great to avoid this slow process of copying the virtual memory mappings altogether which
is slow even if your program is not using much memory (like the zygote is.) vfork to the Rescue The vfork syscall is like fork except it doesnât copy the parentâs virtual memory mappings and
instead shares the same memory space. The call is very similar to fork and seems like it could be
a drop-in replacement, but its own man page cautions against this (see below.) The child process thread shares the exact same memory as the parent calling thread (writes in the
child appear in the parent) including the stack. Having two different threads (or in this case
processes) use the same stack at the same time simultaneously like this, doesnât work. So the
calling thread in the parent process is suspended until the child calls exec or _exit . +---------+               +---------+         +-------+ | Parent  |               | Kernel  |         | Child | +---------+               +---------+         +-------+ |                         |                  | | vfork                   |                  | |------------------------>|                  | |                         |                  | |                         | Create child     | |                         |----------------->| |                         |                  | ---------------------\ |                         |                  |-| Returns from vfork | |                         |                  | |--------------------| |                         |                  | --------------------\ |                         |                  |-| Does housekeeping | |                         |                  | |-------------------| |                         |                  | |                         |             exec | |                         |<-----------------| |                         |                  | |      Returns from vfork |                  | |<------------------------|                  | |                         |                  | (We didnât include a code snippet because we canât call this function from Rust unless we use the
unstable ffi_return_twice attribute) vfork has two new sources of potential issues. The first is the fact that we are sharing the same
memory space. Care must be taken to not unintentionally modify memory in the parent in some way that
will cause issues. This drawback is tied directly to the performance improvement we want (avoiding
copying the virtual memory mappings.) The second source of potential issues is the fact that the child ends up executing on the same stack
as the parent. This wonât work in general without some form of support from the compiler. This is
because we âreturn twiceâ from the vfork call (see the diagram above.) The gcc returns_twice attribute
does this, but it may not provide as much support as you might expect. Here is a quote from tldp about this https://tldp.org/HOWTO/Secure-Programs-HOWTO/avoid-vfork.html ââ¦itâs actually fairly tricky for a process to not interfere with its parent, especially in
high-level languages. The ânot interferingâ requirement applies to the actual machine code
generated, and many compilers generate hidden temporaries and other code structures that cause
unintended interference. The result: programs using vfork(2) can easily fail when the code changes
or even when compiler versions change.â So what are you allowed to do in the housekeeping exactly? Letâs check what the man page says about
this. â..the behavior is undefined if the process created by vfork() .. calls any other function before
successfully calling _exit(2) or one of the exec(3) family of functions.â This isnât very helpful either. Clearly calling _exit or exec (but not every form of exec it
turns out) is okay, but what other housekeeping is okay in practice? Itâs not entirely clear, and
researching across the internet leads to many others showing a fair amount of anxiety about this
problem On Linux, the behavior of vfork can be recreated using clone , (which we will cover later) in a
way where we donât have to share a stack. This is always preferable. posix_spawn Another way to try to get the speed up we want would be to use posix_spawn . The latest version of
glibc always does the equivalent of calling vfork in its posix_spawn implementation. posix_spawn is actually what std::process::Command tries to use internally for most cases. use anyhow::Result; use maelstrom_linux::{ self as linux, ExitCode, WaitStatus}; fn posix_spawn_wait ( dev_null : & impl linux::AsFd) -> Result <()> { let mut actions = linux::PosixSpawnFileActions::new(); actions. add_dup2 (dev_null, & linux::Fd:: STDOUT ) ? ; let attrs = linux::PosixSpawnAttrs::new(); let child = linux::posix_spawn(c "/bin/echo" , & actions, & attrs, & [ None ], & [ None ]) ? ; let status = linux::waitpid(child) ? ; assert_eq! (status, WaitStatus::Exited(ExitCode::from_u8( 0 ))); Ok (()) } Calling posix_spawn comes with a whole lot fewer caveats and things to be careful of when compared
to fork and vfork . It comes at the cost of losing the âeleganceâ of fork. You configure the
housekeeping by using a struct. It is a kind of âhousekeeping scriptâ we create which executes after
the fork. The downside of using posix_spawn is that our housekeeping is limited to doing whatever things the
housekeeping script has support for. (See the posix_spawn man page for a complete list of things.) clone the API Underpinning it All The aforementioned fork , and posix_spawn actually call clone under the hood in glibc. Also vfork inside the kernel ends up calling into the kernelâs clone code.  It has the functionality
of the previous APIs and a bunch of other features. It can be more difficult to use though. Although, unlike vfork it allows the parent to allocate a
separate stack for the child process, avoiding some of the issues with vfork . use anyhow::Result; use maelstrom_linux::{ self as linux, ExitCode, WaitStatus}; struct ChildArgs { dev_null : linux::Fd, } /// This function executes in the child extern "C" fn child_func ( arg : *mut std::ffi::c_void) -> i32 { let arg: & ChildArgs = unsafe { &* (arg as *mut ChildArgs) }; linux::dup2( & arg.dev_null, & linux::Fd:: STDOUT ). unwrap (); linux::execve(c "/bin/echo" , & [ None ], & [ None ]). unwrap (); unreachable! () } fn clone_clone_vm_execve_wait ( dev_null : & impl linux::AsFd) -> Result <()> { const CHILD_STACK_SIZE : usize = 1024 ; // 1 KiB of stack should be enough let mut stack = vec! [ 0 u8 ; CHILD_STACK_SIZE ]; // We need to pass the file-descriptor for /dev/null through to the child. let child_args = ChildArgs { dev_null: dev_null. fd (), }; // Clone virtual memory, and give us SIGCHLD when it exits. let args = linux::CloneArgs::default() . flags (linux::CloneFlags:: VM | linux::CloneFlags:: VFORK ) . exit_signal (linux::Signal:: CHLD ); // The function accepts a pointer to the end of the stack. let stack_ptr: *mut u8 = stack. as_mut_ptr (); let child = unsafe { linux::clone( child_func, stack_ptr. wrapping_add ( CHILD_STACK_SIZE ) as *mut _ , & child_args as *const _ as *mut _ , & args, ) } ? ; let wait_result = linux::wait() ? ; assert_eq! (wait_result.pid, child); assert_eq! (wait_result.status, WaitStatus::Exited(ExitCode::from_u8( 0 ))); Ok (()) } The CLONE_VM flag (via linux::CloneFlags::VM ) avoids copying the virtual memory from the parent. The CLONE_VFORK flag suspends the parent until the child calls exec or exits. Passing this flag
allows us to not worry about waiting for the right moment to free the childâs stack memory in the
parent. If we donât pass this flag though, we are able to do other things in the parent in parallel,
but then we need some way to know when we can free the stack memory. One way is to share a pipe or
socket with the child which is CLOEXEC , once the child calls exec (or exits) this pipe or socket
will close. This graph looks much better than the one for fork . It is a pretty constant speed and faster than
the fastest fork !. About the same performance is found with posix_spawn . Finally, letâs compare all the options (minus vfork because we canât call it) for a program without
much memory mapped. ran std::process::Command::{spawn + wait} 10000 times in 5.400743724s (avg. 540.074Âµs / iteration) ran fork + execve + wait 10000 times in 4.068149021s (avg. 406.814Âµs / iteration) ran posix_spawn + wait 10000 times in 2.918496041s (avg. 291.849Âµs / iteration) ran clone(CLONE_VM) + execve + wait 10000 times in 2.907074411s (avg. 290.707Âµs / iteration) ran clone(CLONE_VM | CLONE_VFORK) + execve + wait 10000 times in 2.883697173s (avg. 288.369Âµs / iteration) Rustâs std::process::Command comes in at the slowest even though it should be comparable to posix_spawn , this could be due to differences in the housekeeping or other things the std code
is doing. Conclusion Letâs tie it all together with a flow graph about what to use. start V +--------------+    yes    +---------------------+ | posix_spawn  | <-------- |simple housekeeping? | +--------------+           +---------------------+ |no | V +---------------------+ |performance critical?|--\ +---------------------+  | |no             | |               | yes V               | +------+    yes   +----------------+     | | fork | <--------|single threaded?|     | +------+          +----------------+     | |no             | |               | V               | +------+   yes   +-----------------------+  | |zygote|<--------|foolproof to implement?|  | +------+         +-----------------------+  | |no             | |               | V               | +------+   yes   +------+           | |vfork |<--------|POSIX?|<----------/ +------+         +------+ |no | V +------+ |clone | +------+ Addendum You can check out working code for the snippets and benchmarks here Be sure to check back for the next part of this article series where we dive into the code in
Maelstrom that calls clone .

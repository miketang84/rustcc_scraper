======>
https://github.com/andrewgazelka/hyperion
-->>-->>
Repository files navigation README Apache-2.0 license Hyperion JOIN THE DISCORD hyperion.webm Hyperion aims to have 10k players PvP simultaneously on one Minecraft world to break the Guinness World Record ( 8825 by
EVE Online ). The
architecture is ECS-driven using Flecs Rust . To contribute,
join Hyperion's Discord for the latest updates on development. Our current efforts are focused on making an event roughly similar to something that would be
on Overcast Network (we are not affiliated with them). Category Task Status Notes Lighting Pre-loaded lighting ✅ Done Dynamic lighting updates ❌ Not implemented May be unnecessary for Overcast-like modes Block Mechanics Placing blocks ❌ Not implemented Existed pre-rewrite Block breaking ✅ Done Block drops ❌ Not implemented Existed to some extent pre-rewrite Block physics (doors, liquid, torches, etc.) ❌ Not implemented Existed pre-rewrite World Generation Pre-loaded chunks from Java world saves ✅ Done Uses pre-built maps Rendering Block animation/Frame API ✅ Done Inventory Player inventory ❌ Not implemented Existed to some extent pre-rewrite Block inventory (chests, etc.) ❌ Not implemented Combat PvP (Player vs. Player) ❌ Not implemented Existed pre-rewrite Arrows ❌ Not implemented Existed to some extent pre-rewrite Player health and hunger ❌ Not implemented Health is necessary; hunger less important World Persistence Saving world ❌ Not implemented Most useful in case the event server crashes Physics Entity-block collisions (anti-cheat) ✅ Done Entity-entity collisions ✅ Done Required for arrow-based combat Gameplay Mechanics Day/night cycle ✅ Done Audio Proximity voice chat ✅ Done Not included in open-source repository Modularity Mod/Plugin API 🌊 In progress We want to make events extensions on top of the core game engine Running Debug mode brew install just
just Release mode brew install just
just release

======>
https://github.com/Indra-db/Flecs-Rust
-->>-->>
Repository files navigation README MIT license What is the Flecs Rust API? The Rust API is a wrapper around the Flecs C API. The API is designed to offer Rust developers an intuitive and streamlined interface to harness the full potential of Flecs. It's based on V4 flecs release, blogpost can be found here . What is Flecs ECS? Flecs is a fast and lightweight Entity Component System that lets you build games and simulations with millions of entities ( join the Discord! ). Here are some of the framework's highlights: Fast and portable. Due to Flecs C core, it has major bindings in several languages, including C++, C#, and now Rust! First open source ECS with full support for Entity Relationships ! Fast native support for hierarchies and prefabs Runs in the browser (Rust instructions TBD / WIP) Cache-friendly archetype/SoA storage that can process millions of entities every frame Supports entities with hundreds of components and applications with tens of thousands of archetypes Automatic component registration that works out of the box across shared libraries/DLLs Write free functions with queries or run code automatically in systems Run games on multiple CPU cores with a fast lockless scheduler Flecs is heavily tested, running more than 8000 tests in its core library alone and used in AAA engines. The Rust API itself has 500+ tests and counting. Integrated (WIP Rust) reflection framework with JSON serializer and support for runtime components Powerful query language with support for joins and inheritance Statistics addon for profiling ECS performance A web-based UI for monitoring & controlling your apps ( demo , code ): How to get started? Add the following to your Cargo.toml : [ dependencies ] flecs_ecs = " 0.1.1 " and start hacking away! Make sure to check out the Rust docs (improvements coming soon), Flecs docs , and the 70+ examples in the examples directory. For an example integration of Flecs with the following crates: WGPU for rendering winit for windowing vello for rasterization parley for text check out the demo here Status: Alpha release The project is in the alpha release stage where the core functionality and some addons of Flecs have been ported and are available to use today. While there has been a lot of thought put into the current API, it's still in an experimental phase. The project aims to hit stable when all the soundness/safety issues are resolved and the API has been finalized with all of Flecs functionality. We encourage you to explore, test, and provide feedback, but please be aware of potential bugs and breaking changes as we continue to refine the API and add new features. This library was made publicly available on the release date of Flecs V4 release. Safety One important safety factor that has yet to be addressed is having multiple aliases to the same component. This is a known issue and is being worked on. It will be addressed through a table column lock mechanism. Performance From initial benchmarks and tests, the Rust API is on par with C-level performance, except for where overhead was introduced to make the API safe to use in Rust land (e.g. get performance). However, performance improvements are planned to be made in the future. The progress For detailed feature progress, please visit the issues page. Core library Addons Documentation Most functions are documented and contain a C++ alias. Flecs documentation site contains Rust code. The remaining % is for adding mostly doc test examples and refining the Rust docs page. Test suite (entity, query, observers systems test cases are done) Examples For the current feature set, all examples are done. The Aim The plan is to match feature parity of the C++ API, starting with the core library (done!) while also being fully documented and tested and addressing any safety issues that may arise. The project aims to provide a safe, idiomatic, and efficient Rust API for Flecs, while also being a good citizen in the Rust ecosystem. Contributions If you're excited about this project and would like to contribute, or if you've found any bugs, please feel free to raise an issue or submit a pull request. We'd love to have your involvement! License MIT license, matching Flecs. Example code use flecs_ecs :: prelude :: * ; # [ derive ( Debug , Component ) ] pub struct Position { pub x : f32 , pub y : f32 , } # [ derive ( Debug , Component ) ] pub struct Velocity { pub x : f32 , pub y : f32 , } # [ derive ( Component ) ] pub struct Eats ; # [ derive ( Component ) ] pub struct Apples ; fn main ( ) { // Create a new world let world = World :: new ( ) ; // Register system world . system :: < ( & mut Position , & Velocity ) > ( ) . each ( | ( pos , vel ) | { pos . x += vel . x ; pos . y += vel . y ; } ) ; // Create an entity with name Bob, add Position and food preference let bob = world . entity_named ( "Bob" ) . set ( Position { x : 0.0 , y : 0.0 } ) . set ( Velocity { x : 1.0 , y : 2.0 } ) . add :: < ( Eats , Apples ) > ( ) ; // Show us what you got println ! ( "{}'s got [{:?}]" , bob.name ( ) , bob.archetype ( ) ) ; // Run systems twice. Usually this function is called once per frame world . progress ( ) ; world . progress ( ) ; bob . get :: < & Position > ( |pos| { // See if Bob has moved (he has) println ! ( "{}'s position: {:?}" , bob.name ( ) , pos ) ; } ) ; // Output: //  Bob's got [Position, Velocity, (Identifier,Name), (Eats,Apples)] //  Bob's position: Position { x: 2.0, y: 4.0 } } FAQ What's next? Meta, Json, Script addons. This will allow for reflection, serialization, and scripting capabilities for creating entities and components. See the Flecs documentation for more information. Wasm unknown unknown. The project is currently in the process of supporting wasm32-unknown-unknown target. This is expected to land in some shape or form by the end of August. API refinements, resolving safety issues & documentation. C# scripting support. Integration with Flecs.Net to work seamlessly with Flecs Rust API. More demos and examples. How does it compare to other Rust ECS libraries? Flecs isn't written natively in Rust, it's written in C, but it's a mature and feature-rich ECS library that has been used in AAA games and other commercial software. It's fast, lightweight, and has a lot of features that other ECS libraries don't have. Some of the features that make Flecs stand out are: Everything's an entity. Systems, queries and components are all entities. Focus on builder APIs and DSL macro over the type system: [Builder API] world . system :: < & A > ( ) . with :: < B > ( ) . each ( || { } ) ; [DSL API] system ! ( world, & A , B ) . each ( || { } ) ; Singletons (Resources) are modelled as a component added to it's own entity. world . set ( GameTime { delta_time : 0.0 } ) ; [Builder API] world . system :: < & GameTime > ( ) . term_at ( 0 ) . singleton ( ) . each ( || { } ) ; [DSL API] system ! ( world, & R ( $ ) ) . each ( || { } ) ; Systems/observers are based on queries, and will only run if that query matches. Systems are single-threaded by default and run in order of declaration (See docs for more info on how parallelism and how pipelines work in flecs) Support for building your own custom Pipeline. Relationships are first-class citizens in Flecs, allowing for easy creation of hierarchies. union relationships, exclusive relationships, oneof constraints, relationship traversal, reflexive relationships component inheritance transitivity query variables toggleable components entity disabling builtin hierchies with automatic cleanup prefabs, prefab inheritance, prefab slots, prefab hierarchies flecs script & flecs script templates (hierarchical) entity names archetype-level change detection query sorting query grouping support for unregistration: component, modules (plugins), systems, observers event propagation, event forwarding runtime components runtime reflection with a language agnostic reflection framework a language agnostic core etc Projects using Flecs Rust API This list contains projects that are not under NDA. If you want to showcase your project, feel free to open a PR to add it to the list. [Hyperion]: It switched from using Envio ECS to Flecs, with great performance improvements. It's quest is to break the world record minecraft event of 10000 players. Acknowledgements A big shoutout to Sander Mertens for creating such a wonderful library and the pre-alpha testers who contributed to Flecs Rust API, especially James , Bruce , and Andrew .
======>
https://www.guinnessworldrecords.com/world-records/105603-largest-videogame-pvp-battle
-->>-->>
Who EVE Online, CCP Games, Fury at FWST-8 What 8,825 total number Where Not Applicable When 06 October 2020 The largest multiplayer videogame PvP battle consisted of 8,825 players, and was achieved by EVE Online, developed by CCP Games (Iceland) at a battle called 'Fury at FWST-8', on 6 October 2020. Fury at FWST-8 was PAPI's attempt to establish a beachhead Keepstar in Delve that would allow them to strike deeper into the Imperium's territory with their Supercapital forces. To prevent this, the Imperium fought long and hard to destroy the Keepstar and push PAPI back. As a result, 6,746 ships and one Keepstar Upwell Structure were destroyed, 362 capital-class ships were lost, with the total cost of the battle coming to 1.443 trillion ISK (equating to $18,712 USD). Photo credit: Razorien EVE Records change on a daily basis and are not immediately published online. For a full list of record titles, please use our Record Application Search. (You will need to register / login for access) Comments below may relate to previous holders of this record.
======>
https://github.com/jb2170/better-adb-sync
-->>-->>
Repository files navigation README Apache-2.0 license Better ADB Sync An rsync -like program to sync files between a computer and an Android device Installation Available on PyPI $ pip install BetterADBSync QRD To push from your computer to your phone use $ adbsync push LOCAL ANDROID To pull from your phone to your computer use $ adbsync pull ANDROID LOCAL Full help is available with $ adbsync --help Intro This is a (pretty much from scratch) rewrite of Google's adbsync repo. The reason for the rewrite is to Update the repo to Python 3 codestyle (strings are by default UTF-8, no more b"" and u"", classes don't need to inherit from object, 4 space indentation etc) Add in support for --exclude , --exclude-from , --del , --delete-excluded like rsync has (this required a complete rewrite of the diffing algorithm) Additions --del will delete files and folders on the destination end that are not present on the source end. This does not include exluded files. --delete-excluded will delete excluded files and folders on the destination end. --exclude can be used many times. Each should be a fnmatch pattern relative to the source. These patterns will be ignored unless --delete-excluded is specified. --exclude-from can be used many times. Each should be a filename of a file containing fnmatch patterns relative to the source. Possible future TODOs I am satisfied with my code so far, however a few things could be added if they are ever needed --backup and --backup-dir-local or --backup-dir-android to move outdated / to-delete files to another folder instead of deleting ---BEGIN ORIGINAL README.md--- adb-sync adb-sync is a tool to synchronize files between a PC and an Android device
using the ADB (Android Debug Bridge). Related Projects Before getting used to this, please review this list of projects that are
somehow related to adb-sync and may fulfill your needs better: rsync is a file synchronization tool for local
(including FUSE) file systems or SSH connections. This can be used even with
Android devices if rooted or using an app like SSHelper . adbfs is a FUSE file system that uses adb to
communicate to the device. Requires a rooted device, though. adbfs-rootless is a fork of adbfs
that requires no root on the device. Does not play very well with rsync. go-mtpfs is a FUSE file system to
connect to Android devices via MTP. Due to MTP's restrictions, only a certain
set of file extensions is supported. To store unsupported files, just add
.txt! Requires no USB debugging mode. Setup Android Side First you need to enable USB debugging mode. This allows authorized computers
(on Android before 4.4.3 all computers) to perform possibly dangerous
operations on your device. If you do not accept this risk, do not proceed and
try using go-mtpfs instead! On your Android device: Go to the Settings app. If there is no "Developer Options" menu: Select "About". Tap "Build Number" seven times. Go back. Go to "Developer Options". Enable "USB Debugging". PC Side Install the Android SDK (the
stand-alone Android SDK "for an existing IDE" is sufficient). Alternatively,
some Linux distributions come with a package named like "android-tools-adb"
that contains the required tool. Make sure "adb" is in your PATH. If you use a package from your Linux
distribution, this should already be the case; if you used the SDK, you
probably will have to add an entry to PATH in your ~/.profile file, log out
and log back in. git clone https://github.com/google/adb-sync cd adb-sync Copy or symlink the adb-sync script somewhere in your PATH. For example: cp adb-sync /usr/local/bin/ Usage To get a full help, type: adb-sync --help To synchronize your music files from ~/Music to your device, type one of: adb-sync ~/Music /sdcard
adb-sync ~/Music/ /sdcard/Music To synchronize your music files from ~/Music to your device, deleting files you
removed from your PC, type one of: adb-sync --delete ~/Music /sdcard
adb-sync --delete ~/Music/ /sdcard/Music To copy all downloads from your device to your PC, type: adb-sync --reverse /sdcard/Download/ ~/Downloads ADB Channel This package also contains a separate tool called adb-channel, which is a
convenience wrapper to connect a networking socket on the Android device to
file descriptors on the PC side. It can even launch and shut down the given
application automatically! It is best used as a ProxyCommand for SSH (install SSHelper first) using a configuration like: Host sshelper
Port 2222
ProxyCommand adb-channel tcp:%p com.arachnoid.sshelper/.SSHelperActivity 1 After adding this to ~/.ssh/config , run ssh-copy-id sshelper . Congratulations! You can now use rsync , sshfs etc. to the host name sshelper . Contributing Patches to this project are very welcome. Before sending a patch or pull request, we ask you to fill out one of the
Contributor License Agreements: Google Individual Contributor License Agreement, v1.1 Google Software Grant and Corporate Contributor License Agreement, v1.1 Disclaimer This is not an official Google product. ---END ORIGINAL README.md---
=====>
https://users.rust-lang.org/t/pub-use-breaks-my-compiler/117983
-->>-->>
github.com/rust-lang/rust Adding an `impl for type defined outside of crate` causes every associated method call to be reported as an error 13 opened Jul 16, 2024 udoprog A-diagnostics T-compiler ### Code

```Rust
/// This is not a valid implementation for the current crat … e.
impl dyn core::fmt::Display {}

fn foo(values: &[u32]) -> Option<&u32> {
    values.get(0)
}
```

### Current output

```Shell
As expected, we get an error indicating:


error[E0116]: cannot define inherent `impl` for a type outside of the crate where the type is defined
 --> src/lib.rs:1:1
  |
1 | impl dyn core::fmt::Display {
  | ^^^^^^^^^^^^^^^^^^^^^^^^^^^ impl for type defined outside of crate.
  |
  = note: define and implement a trait or new type instead
```

But we also get the following "bonus" error:

```
error[E0599]: no method named `get` found for reference `&[u32]` in the current scope
 --> src/lib.rs:5:12
  |
5 |     values.get(0)
  |            ^^^
  |
help: there is a method `ge` with a similar name
  |
5 |     values.ge(0)
  |            ~~

Some errors have detailed explanations: E0116, E0599.
For more information about an error, try `rustc --explain E0116`.
error: could not compile `playground` (lib) due to 2 previous errors
```

If for example we add the impl to the [tokio crate](github.com/tokio-rs/tokio), we get:

```
error: could not compile `tokio` (lib) due to 3370 previous errors; 9 warnings emitted
```

### Desired output

Preferably, the `impl for type defined outside of crate` being an illegal implementation should not cause other seemingly unrelated errors.

### Rust Version

```Shell
rustc 1.79.0 (129f3b996 2024-06-10)
binary: rustc
commit-hash: 129f3b9964af4d4a709d1383930ade12dfe7c081
commit-date: 2024-06-10
host: x86_64-unknown-linux-gnu
release: 1.79.0
LLVM version: 18.1.7
```

======>
https://github.com/harr1424/Actix-Route-Rate-Limiter
-->>-->>
Repository files navigation README Actix Route Rate Limiter A library crate that can be used to add rate limiting middleware to Actix Web Application routes. use std :: sync :: Arc ; use actix_web :: { web , App , HttpServer , HttpResponse } ; use chrono :: Duration ; use actix_route_rate_limiter :: { LimiterBuilder , RateLimiter } ; # [ actix_web :: main ] pub async fn main ( ) -> std :: io :: Result < ( ) > { // build a limiter let limiter = LimiterBuilder :: new ( ) . with_duration ( Duration :: seconds ( 20 ) ) // default value is one second . with_num_requests ( 2 ) // default value is one request . build ( ) ; HttpServer :: new ( move || { App :: new ( ) . wrap ( RateLimiter :: new ( Arc :: clone ( & limiter ) ) ) . route ( "/" , web :: get ( ) . to ( HttpResponse :: Ok ) ) } ) . bind ( ( "0.0.0.0" , 12345 ) ) ? . run ( ) . await } This crate can be used to wrap routes with rate limiting logic by defining a duration
and number of requests that will be forwarded during that duration. If a quantity of requests exceeds this amount, the middleware will short circuit the request and instead send an HTTP 429 - Too Many Requests response with headers describing the rate limit: Retry-After :  the rate-limiting duration, begins at first request received and ends after this elapsed time X-RateLimit-Limit : number of requests allowed for the duration X-RateLimit-Remaining : number of requests remaining for current duration X-RateLimit-Reset : number of seconds remaining in the duration Actix Web docs regarding App.wrap()
======>
https://old.reddit.com/r/rust/comments/1fo4482/generics_how_to_design_nested_traits_without/
-->>-->>
Hey Rustaceans! I'm working on a trait design and I'm running into some issues with type specifications. I want to create three traits    A   ,    B   , and    C    where    B    depends on    A   , and    C    depends on    B   . Here's what I have so far:   

   trait A {}

trait B<T: A> {}

trait C<T, U>
where
    T: B<U>,
    U: A,
{}

struct TypeA;
impl A for TypeA {}

struct TypeB<T: A>(std::marker::PhantomData<T>);
impl<T: A> B<T> for TypeB<T> {}

struct TypeC<T, U>(T, std::marker::PhantomData<U>);
impl<T, U> C<T, U> for TypeC<T, U>
where
    T: B<U>,
    U: A,
{}

fn main() {
    let var1: TypeC<TypeB<TypeA>, TypeA> = TypeC(TypeB(std::marker::PhantomData), std::marker::PhantomData);
}
   

   The problem is that when I create a variable of type    C   , I have to specify the types for both    B    and    A   , which feels repetitive:   

   let var1: TypeC<TypeB<TypeA>, TypeA> = TypeC(TypeB(std::marker::PhantomData), std::marker::PhantomData);
   

   I've tried using associated types and type aliases, but I'm still not happy with the result. Ideally, I'd like to be able to create a variable of type    C    without having to specify    A    explicitly, since it's already implied by    B   .   

   Is there a way to design this trait hierarchy to avoid repetitive type specifications? I'd prefer not to use trait objects or the    dyn    keyword if possible.   

   Any insights or alternative approaches would be greatly appreciated. Thanks in advance!   
   

======>
https://fractalfir.github.io/generated_html/rustc_codegen_clr_v0_2_1.html
-->>-->>
I am currently working on a Rust to .NET compiler, rustc_codegen_clr . To get it to work, I need to implement many Rust features using .NET APIs. One of such features is panicking and unwinding. This article is the first one in a series about Rust panics, unwinding, and my implementation of them in .NET. In this part, I will look at unwinding (the compiler side of panicking), and in the next one, I will explain the Linux GNU std implementation of panicking. How a Rust to .NET compiler works: Quick recap Before I talk about the project, I should probably explain what it is. It is a "rust compiler backend" - but what does that mean? You can imagine it as a compiler plugin, which replaces the very last step of compilation (code generation). Instead of using LLVM to generate native code, my project turns the internal Rust representation called MIR, into .NET Common Intermediate Language. CIL is then stored inside .NET assemblies, which allows the .NET runtime to load, and execute the compiled Rust code easily. From the perspective of the Runtime, the compiled Rust looks identical to unsafe C#. So, the Rust code can easily call .NET functions and create .NET objects. In theory, there is nothing you can do in C# that can't be done in Rust too. I am also working on making calling Rust from C# easier. The project allows you to define .NET classes in pure Rust. In the future, the safety of your interop code will be fully checked by the Rust compiler. // Early WIP syntax, subject to change. dotnet_typedef! {
  class MyClass inherits [Some::External::Assebmly]SomeNamespace::SomeClass{ virtual fn ToString (_this:MyClass) -> MString{ "I am a class defined in Rust!" . into_managed ()
    },
  }
} Copy The end goal is to allow near-seamless interop between Rust and C# / F#. Ideally, the users of your Rust .NET library may not even realize that it is written in Rust. Panicking vs unwinding Now that I laid down some basics about how rustc_codegen_clr works, I will probably need to quickly explain what unwinding and panicking are. While the terms panicking and unwinding are related, and sometimes used interchangeably, they are nonetheless 2 different things. Panicking is the name for the feature of the Rust language. When you use the panic! macro, this is the feature you are using. Panicking is very similar to exceptions, and is used in Rust to signal an error in the program logic. So, when you expect an operation could fail, you use the Result type. When an operation should not fail(a failure indicates a bug), you should use panic! s. // Reading and parsing a file can fail, so we return a `Result` to tell the consumers of our API that they should handle it. fn decode_file (path:&Path) -> Result <DecodedFile, DecodeError>{ // ... } // A person's birth date can't be earlier than the current date. // If it is, then either that person is a time traveler, or we have a bug. fn calcualte_age (date_of_birth:Date) -> Duration{ if Date:: now () < date_of_birth{ panic! ( "Found a time traveler!" );
  } // ... } Copy In general, Result s are used where an error is expected, and should be handled by the user of the API, while panics tend to be used when something unexpected happens(eg. an assertion fails, an array is indexed out of bounds). While panicking is not used as commonly as Result s, it is still used in quite a few places(eg. in unit tests). So, this feature must work well. Panicking is currently implemented using unwinding, but that does not have to be the case in the future - you can implement panicking in other ways(eg. a panic can simply terminate the program). Those other implementations have their drawbacks, but there are (rare) situations in which using something other than unwinding makes sense. Unwinding refers to a specific implementation of panicking, where the stack is unwound - traversed upwards, function by function, while the data on the stack is dropped. When I refer to unwinding, I will talk about the specific implementation on the compiler level. Here, things like cleanup blocks or the exact implementation of certain intrinsic matter. When I talk about panicking, I will refer to the implementation present inside the standard library. From the perspective of std, it just calls a few intrinsics and functions, and it does not care how things are implemented under the hood. Unwinds on the compiler level. At the conceptual level, an unwind is relatively easy to understand. An unwind travels up the call stack, popping stack frames of each function along the way, and dropping (disposing of) the stack-allocated objects held by that function. So, if we had some functions like this: fn a (){ let numbers = vec! [ 0 ; 100 ]; b ();
} fn b (){ let hello_string = "Hello World" . to_string (); c ();
} fn c (){ let some_value = Box :: new ( 64 ); panic! ();
} Copy During an unwind(caused by the panic in c ), first the some_value would be dropped, then the unwind would go up the call stack, into b . The hello_string would then be dropped, unwind would continue up to a , and drop the vector numbers it allocated. This process would continue until the unwind encounters a special intrinsic named catch_unwind . The exact signature and behavior of catch_unwind is not relevant for now, so I will discuss it in the next article. Looking at this process, you might ask yourself a question: How does an unwind know what and how to drop? I mean, dropping a Vec<i32> is very different from dropping a Box<i32> , so how do we know how all the objects in that stack frame need to be dropped? How can we tell where a pointer to the string hello_string is on the stack, and how can we know what to do with it? This process of dropping (disposing of) all the data in a stack frame is the responsibility of special pieces of code called "cleanup blocks" Blocks in general Before I explain what a cleanup block is, I should probably explain MIR blocks in general. Each function in the Rust Mid-level Intermediate Representation (MIR) is made up of a series of basic blocks. Those blocks represent the control flow of a program. A block is made up of a bunch of statements, and a terminator. The statements in a block are executed in sequence, and can't diverge or branch in any way. No matter what happens, they will get executed, one by one. The terminator, on the other hand, can change the control flow of a function. It can call other pieces of code, jump to some other block, resume an unwind, or abort the execution of a program. While this may seem a bit odd at first, it is a very convenient representation. It makes analyzing control flow almost trivial - since we know that only terminators can change how the code is executed, we don't have to check each statement to figure out the relationships between blocks. Another nice side-effect of this distinction is that it simplifies unwinding and cleaning up stack frames. Since only a terminator can call other functions,  only terminators can panic. Executing a statement will never panic, since statements can't change the control flow of a program. So, when we want to specify how to deal with a stack frame during an unwind, we only need to do so for each terminator. Now that I explained what a basic block is in the context of MIR, I can show exactly what a cleanup block is. Cleanup blocks As I mentioned before, a cleanup block is not called during “normal” execution. The sole purpose of a cleanup block is dropping (disposing of) all the things in a given stack frame. So, during an unwind, when we want to figure out how to drop a particular frame, we simply call the specified cleanup block. The Rust compiler generates such a cleanup block every time it needs something to get dropped during an unwind. This concept can be a bit hard to grasp, but a small example should make it easier to follow. I will demonstrate how a cleanup block works, by using some Rust-inspired pseudocode. Let's say we have a program like this: fn test (x: u32 ){
bb0:{ let x = 7 ; // If an unwind happens during this function call, `numbers` will not be yet owned by us. // Since we only have ownership of `numbers` once the we allocate the vector(using this function), // dropping it during an unwind is the responsibility of the callee. // So, we don't need a cleanup block. let numbers = vec! [x] -> [ return bn1, unwind continue ];
}
bb1:{ // Up till this point, there was nothing to drop. Now, after the variable `numbers` was allocated, there is something to drop, // so we specify that the cleanup blocks bb4 need to be called if an unwind happens. x = 8 ; do_something (&numbers) -> [ return bn2; unwind bn4];
}
bb2:{
 x = 9 ; // Since calling pass_ownership requires passing the ownership of `numbers`, dropping it is the responsibility of the callee. // So, we don't specify any cleanup blocks. pass_ownership (numbers) -> [ return bn3, unwind continue ];
}
b3:{ return ;
} // The cleanup block generated by the compiler bb4 (cleanup):{ // If this block is called, then we need to dispose of `numbers`. So, we drop it. drop (numbers);
 unwind resume; // Continues the unwind }
} Copy The real MIR looks a bit different(no variable names, explicit types of locals, calls that return nothing assign a () value), but this should give you a rough idea about how everything works. As you can see here, cleanup blocks are not really all that scary. Despite the fancy name, they are eerily similar to finally or using in C#. // If an exception occurs, numbers.Dispose() will be called. using (Vec< int >  numbers = new Vec{x}){
  do_something( ref numbers);
} Copy The main difference between using / finaly and a cleanup block is that it is only called when an unwind happens, while using disposes of the resource when the control flow leaves it. So, it will get called when an exception occurs, but also just during normal execution. Still, despite those differences, unwinds and exceptions map to each other nicely, and exception handlers can be used to implement cleanup blocks. There are, of course, some differences between the two which make this not a 1 to 1 translation. MIR cleanup blocks can jump to other cleanup blocks, and a .NET execution handler can’t jump into another exception handler. Working around that required some tinkering, but it was not a big problem. Now that I talked a little bit about cleanup blocks in general, I thought I might mention another oddity of unwinds in Rust. Drop flags are variables, whose sole purpose is to tell a cleanup block if it should drop something or not. Hang on a minute, did I not just tell you that we only need at most one cleanup block for each terminator, since only terminators can cause an unwind? If we already have separate cleanup blocks for different terminators, why do we need additional flags to tell the cleanup block what to drop? To drop or not to drop: that is the question! Suppose we have some code like this: // Example taken from the Rustonomicon // https://doc.rust-lang.org/nomicon/drop-flags.html let x ; if condition {
    x = Box :: new ( 0 ); // x was uninit; just overwrite. println! ( "{}" , x);
} // x goes out of scope; x might be uninit; // check the flag! Copy As you can see, x may or may not be initialized, depending on the condition. If x is not initialized, then dropping it is UB. If it is initialized, then not dropping it would be a mistake. To solve this, the compiler introduces a second variable, which checks if x is initialized. let x ; // Hiden, compiler-generated drop flags let _is_x_init = false ; if condition{
    x = Box :: new ( 0 );  
    _is_x_init = true ; println! ( "{}" , x);
} if _is_x_init{ drop (x);
} Copy You may say: well, that example is very contrived, and no one writes code like this! You just have to move the definition of x within the body of the if statement, and there would be no problem! if condition{ let x = Box :: new ( 0 ); println! ( "{}" , x); // x is always initialized here, so we can drop it. } Copy Yeah, I would be shocked if someone wrote code exactly like this anywhere. But, this is the simplest possible example of the need for drop flags. More realistic examples are far more complex, and this is the easiest way to showcase why drop flags are needed. Optimizing cleanup blocks is hard. Now that I explained how rust cleanup block works, and how they get turned into exception handlers, I want to highlight a few problems  I encountered while optimizing them. Currently, the Rust code compiled for .NET suffers from massive slowdowns in a few key areas. While I don't want to discuss the exact numbers since I am not 100% confident in them, I can at least highlight some general trends. Rust compiled for .NET is, as expected, always slower than native Rust. However, it is not that much slower. In quite a few benchmarks, it is within 1.5 - 2x of native code - which I feel like is a pretty acceptable result. In the majority of cases, it is no more than 5x slower than native code. This is not great, but it is also not a bad starting point, seeing as my optimizations are ridiculously simple, and not all that effective. Being wrong fast is not impressive, so I am mostly focusing on compilation accuracy and passing tests right now. However, a select few benchmarks jump out. Some of them are pretty much expected (eg. a benchmark showing the vectorization capabilities of LLVM), but other ones may seem a bit more surprising at first. One particularly problematic area is complex iterators, some of which take 70x more time to execute in NET. That is, of course, unacceptable. You may ask yourself: what causes this slowdown? Even though those benchmarks never panic, the cost of the unwinding infrastructure is still there. Just disabling unwind support(which just removes all the exception handlers) speeds up this problematic benchmark by 2x. Of course, being 35x slower than native is still nothing to brag about, but it at least guides us toward the underlying problems. Let us look at one of the problematic functions, decompiled into C# for readability. // NOTE: for the sake of this example, the custom optimizations performed by `rustc_codegen_clr` were disabled. public unsafe static void _ZN4core4iter8adapters3map8map_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hce71e23625930189E(Closure2n22Closure1n11Closure1pi8v * P_0, RustVoid acc, long elt) { //Discarded unreachable code: IL_002b, IL_006f Closure1n11Closure1pi8 * f_; bool flag; long item; try {
    f_ = & P_0 - > f_0;
    flag = true ; void * ptr = ( void * ) 8 u;
    Tuple2i8 tuple2i;
    tuple2i.Item1 = elt;
    tuple2i = tuple2i;
    item = _ZN4core3ops8function5FnMut8call_mut17hd1566fb973e9735aE(ptr, tuple2i.Item1);
  } catch { if (flag) {} throw ;
  } try {
    flag = false ;
    Tuple3vi8 tuple3vi;
    tuple3vi.Item2 = item;
    tuple3vi = tuple3vi;
    RustVoid rustVoid;
    _ZN4iter13for_each_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hfc540be8426f74d2E(f_, rustVoid, tuple3vi.Item2);
  } catch { if (flag) {} throw ;
  }
} Copy You may immediately notice something very odd. All the exception handlers in this function are effectively nops! In some other examples, those catch blocks do assign some values to local variables, but, as soon as the handler finishes execution, those locals will be discarded anyway - since the exception will be rethrown(an unwind will resume). catch { nint num6 = ( nint ) self.b.v; if (num6 != 1 || flag) {} throw ;
} Copy And what about those useless ifs? They are empty, so they can't do anything at all! Those ifs, weird assignments, and empty handlers are ghosts of removed drops. In search for the empty cleanup blocks The example I showed you is nothing more than a faithful recreation of the original optimized MIR passed to me by the Rust compiler front end. All those useless blocks and instructions are there because the compiler explicitly asked me for them. You may think: why would rustc explicitly request a useless block? Can’t it see that it does nothing? I mean, the sole reason for MIRs existence is optimization. If this is its only job, surely it would be better at it? Well, what if I told you that the purpose of MIR optimizations is not to make your code faster, but to make compilation faster? How on earth does an extra step in compilation make it faster? Doing more to do less. Well, as you may know, LLVM is not exactly the fastest thing in the world. It is very good at producing fast code, but it takes its sweet time to do it. Let us say we know that a certain kind of LLVM optimization is quite slow, but we found a faster way to achieve the same thing, by using the unique properties of Rust. Well, if rustc did just that, then LLVM wouldn't need to perform its own costly optimization, and compilation time would improve. This kind of scenario is not very common. We can, however, exploit a different property of Rust to make a lot of optimizations way faster. The nice thing about generics is that they allow for a lot of code to be shared. What if, instead of optimizing the “final” monomorphized versions of a function(like what LLVM does), we could optimize the generic one? Look at this function: fn do_sth <T: Copy >(val:&T){ let tmp = *val; do_sth_inner (&tmp, 8 );
} Copy For each different type, T LLVM will receive a different version of this function from rustc Each of those variants needs to be optimized separately. Let us imagine we call this function with 1000 different Ts. This means LLVM will have to optimize 1000 different versions of this function. This will take time. However, we could try optimizing the generic version of this function. We might not be able to perform all the optimizations, but we should be able to perform quite a few of them. In this example, we could avoid storing the value of val in tmp . Since we only use the address of tmp to pass it to another function, val is an immutable reference to T , and tmp is a bitwise copy of what val points to, this optimization is OK. fn do_sth <T: Copy >(val:&T){ do_sth_inner (val, 8 );
} Copy In one move, we optimized all the possible variants of do_sth . LLVM  would have to do the same thing each time do_sth is used. So, for any function that is used more than once, we should save some compilation time. This is the main reason MIR exists. If we can optimize stuff before handing it to LLVM, we can save time. Disabling MIR optimizations would not slow the final native code all that much, but it would make LLVM have to do more work - which would increase compile times. Limitations of MIR So, we now know that MIR optimization operates on generic functions and that they can leverage Rust-specific information(eg. borrows) to perform certain optimizations faster. However, there are some problems with this approach. rustc can't perform certain kinds of MIR optimizations, since they might not be valid for all possible T s. For example, if the type in our example was Clone , and not Copy , we would not be able to optimize it all that much. fn do_sth <T: Clone >(val:&T){ let tmp = val. clone (); do_sth_inner (&tmp, 8 );
} Copy The clone could have side effects(eg. incrementing an atomic reference counter), and dropping tmp could also do something(eg. decrementing an atomic reference counter). This is not a problem in most cases - since LLVM operates on monomorphized functions(functions with T replaced by some type, like i32 ), it can still perform this optimization, where applicable. You can now probably guess why the iterator code I showcased had empty exception handlers(and cleanup blocks) - those blocks are not always empty. Ghost drop When you look at the original example, you might notice it operates on long s(aka i64 s). Dropping an i64 is a NOP. During compilation, when the cleanup block in question is processed, my backend encounters a drop. When it encounters a drop, I use Instance::resolve_drop_in_place to get information about how to drop a variable. let drop_instance = Instance:: resolve_drop_in_place (ctx. tcx (), ty). polymorphize (ctx. tcx ()); if let InstanceKind :: DropGlue (_, None ) = drop_instance.def { //Empty drop, nothing needs to happen. vec! [CILRoot::GoTo { target: target. as_u32 (), sub_target: 0 }. into ()]
  } else { // Handle the drop } Copy One of the possible ways to drop something is calling the "DropGlue". Drop glue is basically a function that calls the drop implementation. However, when something (like an i64 ) does not need dropping, the drop glue can be None . If it is None , then there is nothing to do during a drop. In such a case, I do nothing(besides jumping to the next block). So, this "empty" drop compiles into nothing - that explains our empty exception handlers. In reality, those handlers look more like this: catch { nint num6 = ( nint ) self.b.v; if (num6 != 1 || flag) { // Ghost drop drop_i64(&item); // Drops the i64 - does nothing. } throw ;
} Copy But the drop_i64 is never generated, since it is not needed. Optimizing exception handlers Optimizing exception handlers is very important for Rust to run well inside the .NET runtime. The biggest problem with them is the amount of byte code those things create. Remember - each Rust block can have its own cleanup bloc, and jumping between exception handlers is not allowed in .NET. This means that I have to store multiple copies of certain blocks. This huge amount of exception handlers is not liked by the .NET JIT. I will admit that I am not an expert when it comes to understanding how RyuJIT works - I only have a very rough idea about how all the pieces fit together. Still, I feel like I know enough to speculate about the cause of the slowdowns I observed. rustc_codegen_clr is not yet all that good at optimizing code, so it generates a lot of CIL. Each unneeded exception handler, unused variable - all that adds up. While the .NET JIT is pretty good, and can optimize most of the inefficiencies away, the increased bytecode size still has an impact. Internally, most JIT's (including RyuJIT) use certain heuristics to estimate if certain optimizations are worth it. So, a JIT may use metrics like the number of exception handlers, locals, and the size of bytecode, to try to guess how large the final compiled function is. Since I emit a lot of code (quite a bit of it useless), the JIT "thinks" my functions are bad optimization and inlining candidates. The exact formula used to calculate the cost of operations like inlining is not all that relevant to this post (you can find it here ), but I still wanted to highlight some things that seem to explain what I observed. Looking at the default inlining policy, it seems like RyuJIT will not inline a function, if it exceeds a certain limit of basic blocks. // Code snipet from: https://github.com/dotnet/runtime/blob/02e95ebff119189ac2d65e4a641980ccb3ea1091/src/coreclr/jit/inlinepolicy.cpp#L577 // CALLEE_IS_FORCE_INLINE overrides CALLEE_DOES_NOT_RETURN if (!m_IsForceInline && m_IsNoReturn && (basicBlockCount == 1 ))
{ SetNever (InlineObservation::CALLEE_DOES_NOT_RETURN);
} else if (!m_IsForceInline && (basicBlockCount > MAX_BASIC_BLOCKS))
{ SetNever (InlineObservation::CALLEE_TOO_MANY_BASIC_BLOCKS);
} Copy This limit seems to currently be 5 . Since I need to duplicate all used cleanup blocks for each handler (jumping between exception handlers is not allowed), most handlers contain multiple blocks. Let us say we have 2 handlers, with 2 blocks each. That alone already adds up almost to the inline limit. When I removed all the exception handlers from an assembly, I removed a lot of those blocks, allowing the JIT to inline more functions. Overall, removing this dead code, and reducing complexity, encourages the JIT to perform more optimizations. You may wonder: why do JIT's use heuristics like this, and why do they only perform optimizations in certain cases? JIT's try to be fast at compiling code. You don't want to have to wait a couple of minutes for your .NET app to start. So, a JIT has a limited time budget. It has to weigh the benefit of any give optimization against the time it would take. You don't want the JIT to spend a lot of time optimizing a complex function, if it can spend the same time optimizing 100 simpler ones. In general, JIT's focus on code that is used often, and does not take a lot of time to optimize. This way, they can give you some pretty decent performance, while also compiling CIL very quickly. If the CIL I generate is simpler, then the JIT can "see" that optimizing it will not take too long. By better preparing a .NET assembly for JIT compilation, I can reduce the amount of needless work a JIT needs to do, allowing it to perform other, highly beneficial optimizations. Of course, you should take this paragraph with a pinch of salt. While I am pretty confident in what I wrote, I am still an amateur when it comes to JIT's. So, I might have misunderstood something or missed some finer detail. Removing empty handlers Now that I expalined why those optimzations are needed, I will show how I implemented them. To remove a useless cleanup block(or exception handlers) I first need a good way of detecting them. Originally, I went with a more complex solution: First, I would simplify each block separately, removing all the side-effect-free statements before a rethrow . // Since assigning a constant to a local has no other side effects, and this bit of CIL is followed by a rethrow instruction, we are free to remove it. ldc . i4 .0 stloc .0
rethrow // Simpler, equivalent version rethrow Copy Next, I would replace unconditional jumps to lone rethrow ops with rethrow s. br bb1
bb1:
rethrow // Simpler, equivalent version rethrow Copy After a few more steps dealing with conditionals, most handlers would get simplified away into a single rethrow instruction. catch [System.Runtime]System.Object{
pop // Ignore the exception Object rethrow
} Copy As a last step, I would simply remove those trialy-NOP handlers. This approach had a lot of advantages - in cases where it was not able to fully optimize something away, it would still simplify it considerably. However, the added complexity made this optimization a bit error-prone. For the sake of simplicity, I have overlooked some of the non-obvious issues. Most of them were caused by some of the tech debt the project has. Fixing them requires some additional refactors, which are just not done yet. Since the code in cleanup blocks only runs during an uwnind(so not during normal excution), debuging it is a bit of a pain. Any issues in cleaup blocks will also be very hard to notice:removing a necessary cleanup block can just lead to a memory leak. So I need to get all the optimzations operating on them to be perfect. In order to avoid those issues, I have decided to go with a much simpler, more conservative approach for now. It is not as good at optimizing stuff away, but it can still improve things a fair bit. I simply go through all the CIL in the handler, checking if it can cause observable side effects. For example, assigning a const value to local has no side effects, while setting a value at some address has side effects. If a handler only consists of assignments to locals, jumps, and a rethrow, then it can't have any side effects, so we can just remove it. After this optimization(and other CIL optimizations implemented by rustc_codegen_clr ), the iterator function I showed before looks like this: public unsafe static void _ZN4core4iter8adapters3map8map_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hce71e23625930189E(Closure2n22Closure1n11Closure1pi8v * P_0, RustVoid acc, long elt) { //Discarded unreachable code: IL_0025, IL_0047 Closure1n11Closure1pi8 * f_ = & P_0 - > f_0; void * ptr = ( void *) 8u ;
  Tuple2i8 tuple2i;
  tuple2i.Item1 = elt;
  Tuple3vi8 tuple3vi; long num = (tuple3vi.Item2 = _ZN4core3ops8function5FnMut8call_mut17hd1566fb973e9735aE(ptr, tuple2i.Item1));
  RustVoid rustVoid;
  _ZN4iter13for_each_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hfc540be8426f74d2E(f_, rustVoid, tuple3vi.Item2);
} Copy Don't get me wrong - there are still a lot of optimizations to be done(the whole function can be optimzed to one line, and easily inlined), but this is far better than what we had before. Conclusion I hope you liked this deep dive into Rust unwinding, drop flags, and MIR optimizations. Originally, this was just supposed to be a "short introduction" to unwinding, after which I would talk about how panics work in std . Well, it turns out unwinds are a bit complex. Since this article is already one of my longest, I will have to split it into 2-3 parts. I also plan to explain the difficulties with Rust code catching arbitrary .NET exceptions and talk a little bit more about safe Rust / .NET interop. So, look forward to that :). The project is 1 year old, GSoC, and special thank you s. Oh, I almost forgot! `rustc_codegen_clr` turned 1 year old on August 28th! I would like to thank You for the amazing reception it received (over 1.4 K stars on Github, and a lot of very nice comments). I(and my project) also participated in Rust GSoC 2024. I originally planned to write an article summarizing GSoC and celebrating the project's birthday, but I was unable to do so due to health reasons. I want to thank my GSoC mentor, Jack Huey, for mentoring me and helping me with some of the organizational stuff. Helping me to stay on track was really important for me since I work better when I have a sense of direction. This may seem small, but receiving even a tiny bit of feedback helps a lot. I also wanted to thank Jakub Beránek(one of the Rust people behind Rust GSoC 2024) for his work organizing and managing Rust GSoC. Good management is almost invisible: everything just goes according to plan. Still, it is something to appreciate. I am, in general, a bit of a messy and forgetful person, so I really appreciate that everything was going smoothly. Since I am talking about the organizational stuff, it would be a crime not to thank the Google GSoC team for making GSoC in the first place. To be quite honest, I assumed that the team behind GSoC had to be pretty sizable since everything worked like a well-oiled machine. Learning how small it is(2.5 people) was a big shock to me, and made me appreciate their work even more. I also want to express my gratitude towards the wider members of the Rust project, who helped me tremendously. It would be hard to list everyone who answered my question by name, but I would like to explicitly mention Bjorn3 , who answered quite a lot of them. One of the greatest things about GSoC was receiving feedback and help from a lot of different people. Overall, GSoC has been a blast, and I met a lot of truly amazing people there. Each project felt like something impactful, and all my interactions with other Rust GSoC contributors were amazing. Meeting all of those people passionate about Rust gave me a lot of confidence in the future of the language. I am currently working on a Rust to .NET compiler, rustc_codegen_clr . To get it to work, I need to implement many Rust features using .NET APIs. One of such features is panicking and unwinding.
======>
https://old.reddit.com/r/rust/comments/1fmznnh/hyperion_10k_player_minecraft_game_engine/
-->>-->>
(open to contributions!)   

   In March 2024   , I stumbled upon the    EVE Online 8825 player PvP World Record   . This seemed beatable, especially given the popularity of Minecraft.   

   Sadly, however, the current vanilla implementation of Minecraft stalls out at around a couple hundred players and is single-threaded.   

   Hence, I’ve spent months making Hyperion —    a highly performant Minecraft game engine built on top of       flecs   . Unlike    many other wonderful Rust Minecraft server initiatives   , our goal is not feature parity with vanilla Minecraft. Instead, we opt for a modular design, allowing us to implement only what is needed for each massive custom event (think like Hypixel).   

   https://i.redd.it/9gi74oeepeqd1.gif   

   With current performance, we estimate we can host ~50k concurrent players. We are in communication with several creators who want to use the project for their YouTube or Livestream content. If this sounds like something you would be interested in being involved in feel free to reach out.   

   GitHub:    https://github.com/andrewgazelka/hyperion   
Discord:    https://discord.gg/WKBuTXeBye   
   

======>
https://old.reddit.com/r/rust/comments/1fnj4st/why_does_some_errortypes_in_the_std_library_not/
-->>-->>
I use an error-handling library that depends on the errors returned from functions implement the `std::error::Error` trait. Of course, there are ways to get around this, but i come across some error-types in the `std` library that doesn't implement the trait - Why is that? Is there a good reason for this?   

   An example would be `std::sync::poison::PoisonError<T>`   
   

======>
https://old.reddit.com/r/rust/comments/1fnwj60/adbsink_even_better_adb_sync_for_syncing/
-->>-->>
adb-sink    is a simple cli tool that i have been using for myself. I thought it worked fine and wanted to share it.   
It was made for recursively syncing directories between your android and computer for backing-up and restoring using    adb    by diffing the file tree composed of the file path, size and the last modified date.   

   It uses the system-wide adb binary for now, but has a work-on-progress adb usb protocol implementation (not sure if i will ever finish it).   

   Basically an even    better-adb-sync   : faster and more correct. As in, unlike better-adb-sync, it properly syncs the file last modified time and does not sometimes miss changed files in the tree.   
   

======>
https://old.reddit.com/r/rust/comments/1fnrn5w/pub_use_breaks_my_compiler_in_refactoring_please/
-->>-->>
I don't know how else to say this, but a refactoring I am performing is absolutely breaking my compiler and I have no idea what is going on. Please bear with me, this is weird and frustrating to me and I have no way forward anymore.   

   What I am doing is this: I have moved some types from my crate A to another crate B Literally copy-pasted them. I also added the crate dependency correctly. So what I am doing to refactor without breaking everything in my crate is to    pub use    the types in the modules that used them.   

   Say I had this code in crate A:   

   
pub struct Foo {
 //fields
}
   
I moved this structure to crate B and changed the code in crate A to:   

   
pub use B::Foo;
   
I am doing this one by one and this works perfectly fine, except for when I hit a particular struct. I don't think the struct itself is important, but here it goes:   

   ```   

   [derive(Debug, PartialEq, Clone, serde::Serialize, serde::Deserialize)]   

   pub struct UnivariateLinearRegressionSolution<F: Scalar + Float + ConstOne> {
    pub slope: F,
    pub intercept: F,
    pub jtj_inv: OMatrix<F, U2, U2>,
    pub sample_variance: F,
}
``   
The type   OMatrix` is a matrix type from nalgebra.   

   I can still check my code using    cargo check   , but as soon as I do    cargo check --tests   , I get hundreds of errors such as    

   shell
error[E0599]: no method named `len` found for reference `&[F]` in the current scope
    --> rdb-lib\src\regression.rs:171:33
     |
171  |         if independent_variable.len() < 2 {
     |                                 ^^^
     |
   

   (Note that this error is not even in test code. This is non-test code that passes cargo check)   

   or    

   shell
error[E0599]: no method named `as_ptr` found for reference `&[f32]` in the current scope
  --> rdb-lib\src\test_util.rs:25:41
   

   There are tons of other failures. It seems it recognizes absolutely no associated functions anymore on any type, mine or foreign.   

   I've tried:   

   
   doing this refactor from the beginning (this is my third time)   
   cargo clean   
   uninstalling and installing the Rust toolchain again   
   using a different Rust toolchain (i am currently on 1.81.0)   
   changing operating systems   
   

   When I continue with the refactoring it gets so bad that downstream dependencies stop building because the compiler claims it cannot find the    alloc    module anymore. Please help and thank you.   

   Note   : I cross-posted this to the user forum as well in    this thread   

   UPDATE    people in the Rust forum had the right idea. There was a forbidden impl block I. My test code that I can remove to get rid of the errors. I have no idea why that impl block would be problematic though that it produces those crazy error messages. It seems to lose access to core in that process.    

   UPDATE2    it’s a    compiler bug   
   

======>
https://old.reddit.com/r/rust/comments/1fnkyam/call_for_papers_rustikon_2025_warsaw_poland/
-->>-->>
Now is your chance to take the stage at the first ever edition of the Rustikon conference in Warsaw, Poland.   

   https://sessionize.com/rustikon-2025   

   There's a couple of talk categories that we are particularly interested in:   

   
   Rust use-cases   
   Performance &  memory efficiency   
   Type safety, functional programming   
   Async, concurrency, distributed systems   
   Web/GUI/Embedded & everyday libraries   
   Data engineering, big data, streaming & databases   
   Rust for machine learning and AI   
   Tooling, developer experience   
   Teaching Rust, building communities   
   

   But if you have another proposition, we'll be happy to review it! Sessions are 30 min., plus 5 min. Q&A. The deadline is until the end of 13 October 2024 (CET). In case of any questions or doubts, just write to us: [   rustikon@rustikon.dev   ](mailto:   rustikon@rustikon.dev   ). More about the conference:    https://www.rustikon.dev   .   
   

======>
https://sabrinajewson.org/blog/truly-hygienic-let
-->>-->>
error[E0530]: let bindings cannot shadow constants
 --> src/main.rs:3:10
  |
3 |         let Ok(x @ _) = read_input() else { return Err(Error) };
  |                ^ cannot be named the same as a constant
...
8 |     const x: &str = "TODO";
  |     ---------------------- the constant `x` is defined here
9 |     oh_my!();
  |     -------- in this macro invocation
  |
======>
https://old.reddit.com/r/rust/comments/1fo08hy/my_first_library_crate_rate_limiting_middleware/
-->>-->>
I recently found this sub and thought I would share my first major Rust project in order to receive feedback and suggestions. I published this to crates in July, and have been using it on my side projects since then.    

   https://github.com/harr1424/Actix-Route-Rate-Limiter   

   https://crates.io/crates/actix_route_rate_limiter        
   

======>
https://slint.dev/blog/slint-1.8-released
-->>-->>
Slint Use Cases Demos Embedded Cross Platform Pricing Blog Docs Resources Success Stories Community Partners Events Theme Get Started Contact Us September 23, 2024 by Slint Team Slint 1.8 Released with New Property Changed Callbacks, Timer, and Swipe Gesture Slint is a native GUI toolkit written in Rust, with APIs in Rust, C++,
            Python, and JavaScript, designed for fast and responsive applications on all platforms. Today, we've released 1.8! This version brings several much requested features. Property change callbacks. Timer and
            SwipeGestureHandler elements. Improvements to the live-preview and vscode extension.
            C++ templates for STM32 embedded boards. Math gains postfix support and atan2. Property Change Callbacks Property bindings are part of how you can achieve a lot with so little code in Slint. Then many elements have
            callbacks
            to help manage side effects. Take TextEdit and the edited call back to let you react to the
            changing text the user is
            being entered. However you quickly find places where your UI is updating and changing, but unlike TextEdit
            there is no
            callback to use. Enter Property Change Callbacks . They can help you make your interface even more
            dynamic without
            having to write extra business side logic. Let's imagine TextEdit didn't have an edited callback. You can create the same behavior as follows: my-edit := TextEdit {
    changed text => { t.text = self .text; }
} t := Text {} This callback is triggered when the text property of the TextEdit changes,
            updating the text in the second Text element accordingly. They also can be used to help you debug your bindings. my-edit := TextEdit {
    changed text => { debug( self .text); }
} And now see a log of every time the text changes.

            With great power, comes great responsibility. In most cases try to rely on declarative property bindings,
            which are more
            efficient.
            So be careful not to introduce unintended performance issues or loops, especially if changes cascade into
            other
            properties.

            For more detailed information about the caveats, check out Olivier's recent blog
                post . Timer Element Chances are at some point your user interface needs to trigger a callback at a regular interval. Now you can
            easily do
            this with the Timer element. Timer { interval: 1s ;
    triggered() => { debug( "Tick" ); }
} Be sure to bind to the Timer's running property to stop the timer when you don't need it
            anymore. Your battery and CPU
            will thank you. SwipeGestureHandler Element For those needing advanced interactions we are pleased to be able
            to take our gesture
            system to the next level with the new SwipeGestureHandler element. As the name suggests it
            allows your UI to
            recognize and respond to interactive swipe gestures. sgh := SwipeGestureHandler { handle-swipe-left: true ; handle-swipe-right: true ; swiped => { /* the swipe is finished */ }
} Rectangle { x: sgh.swiping ? sgh.current-position.x - sgh.pressed-position.x : 0px ;
} Enhanced Live Preview We've again improved the UX of the live-preview and enhanced the property editors. VSCode New Project Template If you have ever wanted to quickly create a new Slint project to try out an idea it's now easier than ever.
            Hit
            CTRL+shift+P (CMD+shift+P on Mac) and "Slint: Create New Project from Template" is a new option. It will
            guide you
            through selecting a template to fit the language of your choice. Easier MCU Development Are you targeting STMicroelectronics' STM32 platform with C++? In this release we've included two brand new
            templates that make it very easy to get started on the STM32H747I-DISCO and the STM32H735G-DK boards: 1. Download the template archive from the table in our documentation . 2. Open the extracted folder in VS Code. 3. Press F5 to cross-compile, flash, and debug in one go.
  
            We're working on adding templates for additional boards in the future. For Espressif's ESP-IDF platform with C++, we've updated our ESP-IDF component to use pre-compiled Slint binaries. This speeds up compilation time and eliminates the need to install Rust. Math Gains Postfix Support A subtle but profound change to the language. Traditional syntax: Math.max(20, Math.abs(value.x)) New postfix syntax: value.x.abs().max(20) The new syntax improves readability by making the transformation steps more explicit. It works well for many
            operations
            but has limitations: Effective for simple transformations (e.g., abs, max)
            Less intuitive for operations like clamp or atan2. pos.y.atan2(pos.x) // Less clear than atan2(pos.y, pos.x) So for now you cannot use postfix for all functions in the Math namespace. We may revisit these cases later,
            so give them a try and let us know your thoughts. Math Gains atan2() While creating the new dial
                example our new designer, Nigel, complained it would have been easier
            with Math.atan2. So we suggested he add it himself. It's his first time writing any Rust, so he insisted we
            mention that in the release notes. Time will tell if this leads
            to fewer complaints or more Rust commits! Finally... To start using Slint 1.8, you can upgrade by following the instructions attached to the GitHub release. With this release, we hope to make Slint even more powerful and user-friendly for developers building
            modern, performant UIs.
            Be sure to check out the full
                ChangeLog for more details.
            As always, we welcome your feedback and contributions.
            In particular, thanks to the following contributors! @crai0 @Enyium @DataTriny @madsmtm @otiv-wannes-vanleemput @wetheredg @sigmaSd @danutsu @asuper0 @dragonrider7225 @tiaoxizhan @algovoid @0x6e @Justyna-JustCode @MiKom @botent @microhobby Happy coding! Comments Slint is a Rust-based toolkit for creating reactive and fluent user interfaces across a range of targets, from embedded devices with limited resources to powerful mobile devices and desktop machines. Supporting Android, Windows, Mac, Linux, and bare-metal systems, Slint features an easy-to-learn domain-specific language (DSL) that compiles into native code, optimizing for the target device's capabilities. It facilitates collaboration between designers and developers on shared projects and supports business logic development in Rust, C++, JavaScript, or Python. About Us Events Careers Investors Imprint & Privacy Explore Embedded Desktop Demos Terms & Conditions Brand Guidelines Markets Automotive Consumer Industrial Medical Success Stories Compare Qt LVGL Flutter Electron Rust GUI Toolkits Contact us Schedule Meeting Email Chat Support GitHub Twitter Mastodon Linkedin Copyright © 2024 SixtyFPS GmbH
======>
https://github.com/ccbrown/iocraft
-->>-->>
Repository files navigation README Apache-2.0 license MIT license iocraft iocraft is a library for crafting beautiful text output and interfaces for the terminal or
logs. It allows you to easily build complex layouts and interactive elements using a
declarative API. Features Define your UI using a clean, highly readable syntax. Organize your UI using flexbox layouts powered by taffy . Output colored and styled UIs to the terminal or ASCII output anywhere else. Create animated or interactive elements with event handling and hooks. Build fullscreen terminal applications with ease. Pass props and context by reference to avoid unnecessary cloning. Getting Started If you're familiar with React, you'll feel right at home with iocraft . It uses all the same
concepts, but is text-focused and made for Rust. Here's your first iocraft program: use iocraft :: prelude :: * ; fn main ( ) { element ! { Box ( border_style: BorderStyle :: Round ,
            border_color: Color :: Blue , ) { Text ( content: "Hello, world!" ) } } . print ( ) ; } Your UI is composed primarily via the element! macro, which allows you to
declare your UI elements in a React/SwiftUI-like syntax. iocraft provides a few built-in components, such as Box , Text , and TextInput , but you can also create your own using the #[component] macro. For example, here's a custom component that uses a hook to display a counter
which increments every 100ms: use iocraft :: prelude :: * ; use std :: time :: Duration ; # [ component ] fn Counter ( mut hooks : Hooks ) -> impl Into < AnyElement < ' static > > { let mut count = hooks . use_state ( || 0 ) ; hooks . use_future ( async move { loop { smol :: Timer :: after ( Duration :: from_millis ( 100 ) ) . await ; count += 1 ; } } ) ; element ! { Text ( color: Color :: Blue , content: format! ( "counter: {}" , count ) ) } } fn main ( ) { smol :: block_on ( element ! ( Counter ) . render_loop ( ) ) . unwrap ( ) ; } More Examples There are many examples on GitHub which
demonstrate various concepts such as tables, progress bars, fullscreen apps,
forms, and more! Shoutouts iocraft was inspired by Dioxus and Ink , which you should also check out,
especially if you're building graphical interfaces or interested in using
JavaScript/TypeScript. License Licensed under either of Apache License, Version 2.0
( LICENSE-APACHE or http://www.apache.org/licenses/LICENSE-2.0 ) MIT license
( LICENSE-MIT or http://opensource.org/licenses/MIT ) at your option. Contribution Unless you explicitly state otherwise, any contribution intentionally submitted
for inclusion in the work by you, as defined in the Apache-2.0 license, shall be
dual licensed as above, without any additional terms or conditions.

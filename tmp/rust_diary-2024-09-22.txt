https://code.visualstudio.com/blogs/2023/06/05/vscode-wasm-wasi
-->>-->>
Blog posts Copilot extensions are all you need VS Code Extensions and WebAssembly - Part Two VS Code Extensions and WebAssembly VS Code Day 2024 Pursuit of wicked smartness in VS Code Shrinking VS Code with name mangling VS Code and WebAssemblies VS Code Day VS Code and Copilot Remote Development Even Better VS Code Sandboxing VS Code Community Discussions Dev Container Features Markdown Language Server The VS Code Server Dev container CLI Moving from Local to Remote Development The problem with tutorials Custom Notebooks vscode.dev Webview UI Toolkit Bracket Pair Colorization Notebooks Workspace Trust Remote Repositories Build 2021 Extension bisect VS Code on Chromebook Development Containers in Education Dev Containers in WSL 2 The Go experience VS Code at Build GitHub Issues Integration Docker in WSL 2 Custom Data Format Improving CI Build Times Inspecting Containers SSH Tips and Tricks WSL 2 Remote SSH Strict null checking Remote Development Language Server Index Format Blogs Copilot extensions are all you need VS Code Extensions and WebAssembly - Part Two VS Code Extensions and WebAssembly VS Code Day 2024 Pursuit of wicked smartness in VS Code Shrinking VS Code with name mangling VS Code and WebAssemblies VS Code Day VS Code and Copilot Remote Development Even Better VS Code Sandboxing VS Code Community Discussions Dev Container Features Markdown Language Server The VS Code Server Dev container CLI Moving from Local to Remote Development The problem with tutorials Custom Notebooks vscode.dev Webview UI Toolkit Bracket Pair Colorization Notebooks Workspace Trust Remote Repositories Build 2021 Extension bisect VS Code on Chromebook Development Containers in Education Dev Containers in WSL 2 The Go experience VS Code at Build GitHub Issues Integration Docker in WSL 2 Custom Data Format Improving CI Build Times Inspecting Containers SSH Tips and Tricks WSL 2 Remote SSH Strict null checking Remote Development Language Server Index Format In this blog post How does it work? How can I run my own WebAssembly code? VS Code's WASI implementation A web shell What comes next? Run WebAssemblies in VS Code for the Web June 5, 2023 by Dirk BÃ¤umer VS Code for the Web ( https://vscode.dev ) has been available for some time now and it has always been our goal to support the full edit / compile / debug cycle in the browser. This is relatively easy for languages like JavaScript and TypeScript since browsers ship with a JavaScript execution engine. It is harder for other languages since we must be able to execute (and therefore debug) the code. For example, to run Python source code in a browser, there needs to be an execution engine that can run the Python interpreter. These language runtimes are usually written in C/C++. WebAssembly is a binary instruction format for a virtual machine. WebAssembly virtual machines ship in modern browsers today and there are tool chains to compile C/C++ to WebAssembly code. To find out what is possible with WebAssemblies today, we decided to take a Python interpreter written C/C++, compile it to WebAssembly, and run it in VS Code for the Web. Luckily, the Python team already started working on compiling CPython to WASM and we happily piggybacked on their effort. The outcome of the exploration can be seen in the short video below: It doesn't really look different than executing Python code in VS Code desktop. So, why is this cool? The Python source code ( app.py and hello.py ) is hosted in a GitHub repository and directly read from GitHub. The Python interpreter has full access to the files in the workspace, but not to any other files. The sample code is multi file. app.py depends on hello.py . The output shows up nicely in VS Code's terminal. You can run a Python REPL and fully interact with it. And of course, it runs on the web. Additionally, the Python interpreter compiled to WebAssembly (WASM) code requires no modification to run in VS Code for the Web. The bits are one for one the same created by the CPython team. How does it work? WebAssembly virtual machines don't come with an SDK (like, for example, Java or .NET ). So out of the box, WebAssembly code can't print to a console or read the content of a file. What the WebAssembly specification defines is how WebAssembly code can call functions in the host running the virtual machine. In the case of VS Code for the Web, the host is the browser. The virtual machine can therefore call JavaScript functions that are executed in the browser. The Python team provides WebAssembly binaries of their interpreter in two flavors: one compiled with emscripten and the other compiled with the WASI SDK . Although they both create WebAssembly code, they have different characteristics regarding the JavaScript functions they provide as a host implementation: emscripten - has a special focus on the Web platform and Node.js . In addition to generating WASM code, it also generates JavaScript code that acts as a host to execute the WASM code in either the browser or Node.js environment. For example, the JavaScript code provides a function to print the content of a C printf statement to the browser's console. WASI SDK - compiles C/C++ code to WASM and assumes a host implementation that conforms to the WASI specification . WASI stands for WebAssembly System Interface . It defines several operating system-like features, including files and file systems, sockets, clocks, and random numbers. Compiling C/C++ code with the WASI SDK will only generate WebAssembly code but will not generate any JavaScript functions. The JavaScript functions necessary to print the content of a C printf statement must be provided by the host. Wasmtime is, for example, a runtime that provides a WASI host implementation that wires WASI to operating system calls. For VS Code, we decided to support WASI. Although our primary focus is to execute WASM code in the browser, we are not actually running it in a pure browser environment. We must run WebAssemblies in VS Code's extension host worker since this is the standard way that VS Code is extended. The extension host worker provides, beside the browser's worker API, the entire VS Code extension API . So instead of wiring a printf call in a C/C++ program to the browser's console, we actually want to wire it to VS Code's Terminal API. Doing this in WASI was easier for us than in emscripten. Our current implementation of VS Code's WASI host is based on the WASI snapshot preview1 and all implementation details described in this blog post refer to that version. How can I run my own WebAssembly code? After we had Python running in VS Code for the Web, we quickly realized that the approach we took allows us to execute any code that can be compiled to WASI. This section therefore demonstrates how to compile a small C program to WASI using the WASI SDK and execute it inside VS Code's extension host. The example assumes that the reader is familiar with VS Code's extension API and knows how to write an extension for VS Code for the Web . The C program we run is a simple "Hello World" program that looks like this: #include <stdio.h> int main ( void ) { printf ( "Hello, World \n " ); return 0 ; } Copy Assuming you have the latest WASI SDK installed and it is on your PATH , the C program can be compiled using the following command: clang hello.c -o ./hello.wasm Copy This generates a hello.wasm file next to the hello.c file. New features are added to VS Code via extensions, and we follow the same model when integrating WebAssemblies into VS Code. We need to define an extension that loads and runs the WASM code. The important parts of the extension's package.json manifest are as follows: { "name" : "..." , ..., "extensionDependencies" : [ "ms-vscode.wasm-wasi-core" ], "contributes" : { "commands" : [ { "command" : "wasm-c-example.run" , "category" : "WASM Example" , "title" : "Run C Hello World" } ] }, "devDependencies" : { "@types/vscode" : "1.77.0" , }, "dependencies" : { "@vscode/wasm-wasi" : "0.11.0-next.0" } } Copy The ms-vscode.wasm-wasi-core extension supplies the WebAssembly execution engine that wires the WASI API up to the VS Code API. The node module @vscode/wasm-wasi provides a facade to load and run WebAssembly code in VS Code. Below is the actual TypeScript code to load and run WebAssembly code: import { Wasm } from '@vscode/wasm-wasi' ; import { commands , ExtensionContext , Uri , window , workspace } from 'vscode' ; export async function activate ( context : ExtensionContext ) { // Load the WASM API const wasm : Wasm = await Wasm . load (); // Register a command that runs the C example commands . registerCommand ( 'wasm-wasi-c-example.run' , async () => { // Create a pseudoterminal to provide stdio to the WASM process. const pty = wasm . createPseudoterminal (); const terminal = window . createTerminal ({ name: 'Run C Example' , pty , isTransient: true }); terminal . show ( true ); try { // Load the WASM module. It is stored alongside the extension's JS code. // So we can use VS Code's file system API to load it. Makes it // independent of whether the code runs in the desktop or the web. const bits = await workspace . fs . readFile ( Uri . joinPath ( context . extensionUri , 'hello.wasm' ) ); const module = await WebAssembly . compile ( bits ); // Create a WASM process. const process = await wasm . createProcess ( 'hello' , module , { stdio: pty . stdio }); // Run the process and wait for its result. const result = await process . run (); if ( result !== 0 ) { await window . showErrorMessage ( `Process hello ended with error: ${ result } ` ); } } catch ( error ) { // Show an error message if something goes wrong. await window . showErrorMessage ( error . message ); } }); } Copy The video below shows the extension running in VS Code for the Web. We used C/C++ code as a source for the WebAssembly and because WASI is a standard, there are other toolchains that support WASI. Examples are: Rust , .NET , or Swift . VS Code's WASI implementation WASI and the VS Code API share concepts like a file system or stdio (for example, a terminal). This enabled us to implement the WASI specification on top of the VS Code API. However, the different execution behavior was a challenge: WebAssembly code execution is synchronous (for example, once a WebAssembly execution started, the JavaScript worker is blocked until the execution finished), whereas most of the API of VS Code and the browser is asynchronous. For instance, reading from a file in WASI is synchronous while the corresponding VS Code API is asynchronous. This characteristic causes two problems for the execution of WebAssembly code inside VS Code extension host worker: We need to prevent the extension host from being blocked while executing WebAssembly code since this would block other extensions from being executed. A mechanism is needed to implement the synchronous WASI API on top of the asynchronous VS Code and browser API. The first case is easy to solve: we run the WebAssembly code in a separate worker thread. The second case is harder to solve since mapping sync code onto async code needs suspending the synchronous executing thread and resuming it when the asynchronously computed result is available. The JavaScript-Promise Integration Proposal for WebAssembly solves this problem on the WASM layer and there is an experimental implementation of the proposal in V8 . However, when we started the effort, the V8 implementation was not available yet. So we chose a different implementation, which uses SharedArrayBuffer and Atomics to map the sync WASI API onto VS Code's async API. The approach works as follows: The WASM worker thread creates a SharedArrayBuffer with the necessary information about the code that should be called on the VS Code side. It posts the shared memory to VS Code's extension host worker and then waits for the extension host worker to finish its work using Atomics.wait . The extension host worker takes the message, calls the appropriate VS Code API, writes results back into the SharedArrayBuffer and then notifies the WASM worker thread to wake up using Atomics.store and Atomics.notify . The WASM worker then reads any result data out of the SharedArrayBuffer and returns it to the WASI callback. The only difficulty with this approach is that SharedArrayBuffer and Atomics require the site to be cross-origin isolated , which, because CORS is very viral, can be an endeavor by itself. This is why it is currently only enabled by default on the Insiders version insiders.vscode.dev and must be enabled using the query parameter ?vscode-coi=on on vscode.dev . Below is a diagram showing the interaction between the WASM worker and the extension host worker in more detail for the C program above that we compiled to WebAssembly. The code in the orange box is WebAssembly code and all the code in green boxes runs in JavaScript. The yellow box represents the SharedArrayBuffer . A web shell Now that we were able to compile C/C++ and Rust code to WebAssembly and execute it in VS Code, we explored whether we could run a shell in VS Code for the Web as well. We investigated compiling one of the Unix shells to WebAssembly. However, some shells rely on operating system features (spawning processes, ...), which are not available in WASI right now. This led us to take a slightly different approach: we implemented a basic shell in TypeScript and tried to compile only the Unix core utils like ls , cat , date , ... to WebAssembly. Since Rust has very good support for WASM and WASI, we gave the uutils/coreutils , a cross-platform reimplementation of the GNU coreutils in Rust, a try. Et voilÃ , we had a first minimal web shell. A shell is very limited if you can't execute custom WebAssemblies or commands. To extend the web shell, other extensions can contribute additional mount points to the file system as well as commands that are invoked when they are typed into the web shell. The indirection via commands decouples the concrete WebAssembly execution from what is typed in the terminal. Using this support in the Python extension from the beginning allows you to execute Python code directly from within the shell by entering python app.py into the prompt or listing the default python 3.11 library, which is usually mounted under /usr/local/lib/python3.11 . What comes next? The WASM execution engine extension and the Web Shell extension are both experimental as a preview and shouldn't be used to implement production ready extensions using WebAssemblies. They have been made publicly available to get early feedback on the technology. If you have any questions or feedback, please open issues in the corresponding vscode-wasm GitHub repository. This repository also contains the source code for the Python example as well as for the WASM execution engine and the Web Shell . What we do know is that we will further explore the following topics: The WASI team is working on a preview2 and preview3 of the specification, which we plan to support as well. The new versions will change the way a WASI host is implemented. However, we are confident that we can keep our API, which is exposed in the WASM execution engine extension, mostly stable. There is also the WASIX effort that extends WASI with additional operating system-like features such as process or futex. We will continue to watch this work. Many language servers for VS Code are implemented in languages different than JavaScript or TypeScript. We plan to explore the possibility of compiling these language servers to wasm32-wasi and running them in VS Code for the Web as well. Improving debugging for Python on the Web. We have started to work on this, so stay tuned. Add support so that extension B can run WebAssembly code contributed by extension A. This will, for example, allow arbitrary extensions to execute Python code by reusing the extension that contributed the Python WebAssembly. Ensuring that other language runtimes that are compiled for wasm32-wasi run on top of VS Code's WebAssembly execution engine. VMware Labs provides Ruby and PHP wasm32-wasi binaries and both do run in VS Code. Thanks, Dirk and the VS Code team Happy Coding! In this blog post, there are 5
                        sections In this blog post How does it work? How can I run my own WebAssembly code? VS Code's WASI implementation A web shell What comes next? Subscribe Ask questions Follow @code Request features Report issues Watch videos Subscribe Ask questions Follow @code Request features Report issues Watch videos
======>
https://github.com/antaalt/shader-validator
-->>-->>
Repository files navigation README MIT license Shader validator This is a vscode extension allowing syntax highlighting, linting & symbol providing for HLSL / GLSL / WGSL shaders. It is using shader-language-server to lint shaders using common validator API. Currently, it support some features and languages: Syntax Highlighting: Provide improved syntax highlighting for code. Diagnostic: Provide errors & warning as user type code. Symbol provider: provide goto, completion, hover & signature. Local symbols: Provide all user created symbols. Intrinsics symbols: Provide all languages provided intrinsics symbols. Language Syntax Highlighting Diagnostics Local symbols Intrinsics symbols GLSL âœ… âœ… âœ… âœ… HLSL âœ… âœ… âœ… ðŸš§ WGSL âœ… âœ… âŒ âŒ Features Syntax highlighting This extension provide syntax highlighting for HLSL, GLSL & WGSL. It also provides some really basic auto completion. Linting You cant lint your code in real time through this extension: WGSL relies on Naga. GLSL relies on Glslang. HLSL relies on DirectX shader compiler on desktop, Glslang on the web (see below). Autocompletion The extension will suggest you symbols from your file and intrinsics as you type. Sigature View available signatures for your function as you type it. Hover View informations relative to a symbol by hovering it. Goto Go to your declaration definition by clicking on it. Extension Settings This extension contributes the following settings: shader-validator.autocomplete : Enable/disable completion suggestion. shader-validator.validateOnType : Enable/disable validate on type. shader-validator.validateOnSave : Enable/disable validate on save. shader-validator.severity : Select minimal log severity for linting. shader-validator.includes : All custom includes for linting. shader-validator.defines : All custom macros and their values for linting. Platform support This extension is supported on every platform, but some limitations are to be expected on some: Windows: full feature set. Mac & Linux: Rely on WASI version of server, same as web, see web support for limitations. Web support This extension run on the web on vscode.dev . It is relying on the WebAssembly Execution engine . Because of this restriction, we can't use dxc on the web as it does not compile to WASI and instead rely on glslang, which is more limited in linting (Only support SM 5.0, same as FXC, while DXC support SM 6.0 and more). Credits This extension is based on a heavily modified version of PolyMeilex vscode-wgsl
======>
https://github.com/zk4x/zyx
-->>-->>
Repository files navigation README Code of conduct Apache-2.0 license MIT license Zyx Zyx is machine learning library written in Rust.
It's main feature is compiled backend. It automatically generates
optimized kernels for CUDA and OpenCL.
Zyx is lazy, waits with execution until it is explicitly asked for results.
All tensors are differentiable. Install cargo add zyx Syntax Zyx uses syntax similar to pytorch. use zyx :: { Tensor , DType } ; let x = Tensor :: randn ( [ 1024 , 1024 ] , DType :: BF16 ) ? ; let y = Tensor :: uniform ( [ 8 , 1024 , 1024 ] , - 1f32 .. 4f32 ) ? ; let b = Tensor :: zeros ( [ 1024 ] , DType :: F16 ) ; let z = & x + & y ; let z = ( x . dot ( & y ) ? + b ) . gelu ( ) ; // Zyx allows for arbitrary differentiation let b_grad = z . backward ( [ & b ] ) [ 0 ] . unwrap ( ) ; // Also higher order derivatives let bb_grad = b_grad . backward ( [ & b ] ) [ 0 ] . unwrap ( ) ; Backends Zyx runs on different devices, current backends are CUDA, OpenCL and wgsl through wgpu.
HIP would be supported too, but HIPRTC is currently broken.
Using COMGR directly as a workaround is in the works..
Zyx automatically tries to utilize all available devices, but you can also manually change it by creating file backend_config.json in folder zyx in home config directory (usually ~/.config/zyx/backend_config.json).
There write [DeviceConfig] struct. Simple neural network cargo add zyx ; cargo add zyx-optim ; cargo add zyx-nn ; use zyx :: { Tensor , DType } ; use zyx_nn :: Linear ; let l0 = Linear :: new ( 3 , 1024 , DType :: F16 ) ; let l1 = Linear :: new ( 1024 , 2 , DType :: F16 ) ; let x = Tensor :: from ( [ 2 , 3 , 1 ] ) . cast ( DType :: F16 ) ; let target = Tensor :: from ( [ 2 , 4 ] ) ; // Zyx also provides some optimizers like SGD and Adam let mut optim = zyx_optim :: SGD { learning_rate : 0.01 , momentum : 0.9 , nesterov : true , .. Default :: default ( ) } ; let train_steps = 100 ; for _ in 0 ..train_steps { let y = l0 . forward ( & x ) . relu ( ) ; let y = l1 . forward ( & y ) . sigmoid ( ) ; let loss = y . mse_loss ( & target ) ? : let grads = loss . backward ( l0 . into_iter ( ) . chain ( l1 . into_iter ( ) ) ) ; optim . update ( l0 . into_iter ( ) . chain ( l1 . into_iter ( ) ) , grads ) ; } l0 . into_iter ( ) . chain ( l1 . into_iter ( ) ) . save ( "my_net.safetensors" ) ; For more details, there is a book . Lazyness Tensors do not get realized automatically. Realization happens only when user accesses tensors, or explicitly using Tensor::realize function. Tensor :: realize ( [ & x , & y ] ) . unwrap ( ) ; If you do not know when to realize tensors, just do it after updating them with optimizer. sgd . update ( & mut model , grads ) ; Tensor :: realize ( & model ) . unwrap ( ) ; This function might get obsolete in the future once detection of repeating graph patterns is implemented. Goals Correctness Hardware support Performance Rust version Zyx currently only supports latest rust stable version. Zyx also requires std,
as it accesses files (like cuda, hip and opencl runtimes), env vars (mostly for debugging)
and also some other stuff that requires filesystem and threads. Features rand - enables support for functions that enable random number generation half - enables support for f16 and bf16 dtypes complex - enables support for cf32 and cf64 dtypes Warning Zyx breaks many principles of clean code. Clean code was tried in older versions of zyx.
Abstractions, wrappers, dyn (virtual tables), generics and lifetimes made the code hard
to reason about. Zyx now uses enums for everything and almost zero generics (only in functions, such as impl IntoShape to make API more flexible).
If you dislike ugly code, please do not use zyx. Zyx uses some unsafe code, mostly due to FFI access. If you find unsafe code offensive,
please do not use zyx. Zyx brings it's own runtime. It is a single global struct behind mutex.
Tensors are indices into graph stored in this runtime. If runtime wasn't
global variable, all tensors would have to keep lifetime to it. It was
tried and it poisoned the whole codebase with lifetimes. If you find global variables
offensive, please do not use zyx. Zyx uses some code duplication. If you hate code that is not DRY, please do not use zyx. Code of conduct Zyx has code of conduct that we humbly borrowed from sqlite. Contributing Please check out CONTRIBUTING.md Thanks For contributing to Zyx, finding bugs and using it in your ML models. License Zyx is free software licensed under the terms of both the MIT license and the Apache License, Version 2.0.
======>
https://github.com/andrewgazelka/hyperion
-->>-->>
Repository files navigation README Apache-2.0 license Hyperion JOIN THE DISCORD hyperion.webm Hyperion aims to have 10k players PvP simultaneously on one Minecraft world to break the Guinness World Record ( 8825 by
EVE Online ). The
architecture is ECS-driven using Flecs Rust . To contribute,
join Hyperion's Discord for the latest updates on development. Our current efforts are focused on making an event roughly similar to something that would be
on Overcast Network (we are not affiliated with them). Category Task Status Notes Lighting Pre-loaded lighting âœ… Done Dynamic lighting updates âŒ Not implemented May be unnecessary for Overcast-like modes Block Mechanics Placing blocks âŒ Not implemented Existed pre-rewrite Block breaking âœ… Done Block drops âŒ Not implemented Existed to some extent pre-rewrite Block physics (doors, liquid, torches, etc.) âŒ Not implemented Existed pre-rewrite World Generation Pre-loaded chunks from Java world saves âœ… Done Uses pre-built maps Rendering Block animation/Frame API âœ… Done Inventory Player inventory âŒ Not implemented Existed to some extent pre-rewrite Block inventory (chests, etc.) âŒ Not implemented Combat PvP (Player vs. Player) âŒ Not implemented Existed pre-rewrite Arrows âŒ Not implemented Existed to some extent pre-rewrite Player health and hunger âŒ Not implemented Health is necessary; hunger less important World Persistence Saving world âŒ Not implemented Most useful in case the event server crashes Physics Entity-block collisions (anti-cheat) âœ… Done Entity-entity collisions âœ… Done Required for arrow-based combat Gameplay Mechanics Day/night cycle âœ… Done Audio Proximity voice chat âœ… Done Not included in open-source repository Modularity Mod/Plugin API ðŸŒŠ In progress We want to make events extensions on top of the core game engine Running Debug mode brew install just
just Release mode brew install just
just release

======>
https://github.com/Indra-db/Flecs-Rust
-->>-->>
Repository files navigation README MIT license What is the Flecs Rust API? The Rust API is a wrapper around the Flecs C API. The API is designed to offer Rust developers an intuitive and streamlined interface to harness the full potential of Flecs. It's based on V4 flecs release, blogpost can be found here . What is Flecs ECS? Flecs is a fast and lightweight Entity Component System that lets you build games and simulations with millions of entities ( join the Discord! ). Here are some of the framework's highlights: Fast and portable. Due to Flecs C core, it has major bindings in several languages, including C++, C#, and now Rust! First open source ECS with full support for Entity Relationships ! Fast native support for hierarchies and prefabs Runs in the browser (Rust instructions TBD / WIP) Cache-friendly archetype/SoA storage that can process millions of entities every frame Supports entities with hundreds of components and applications with tens of thousands of archetypes Automatic component registration that works out of the box across shared libraries/DLLs Write free functions with queries or run code automatically in systems Run games on multiple CPU cores with a fast lockless scheduler Flecs is heavily tested, running more than 8000 tests in its core library alone and used in AAA engines. The Rust API itself has 500+ tests and counting. Integrated (WIP Rust) reflection framework with JSON serializer and support for runtime components Powerful query language with support for joins and inheritance Statistics addon for profiling ECS performance A web-based UI for monitoring & controlling your apps ( demo , code ): How to get started? Add the following to your Cargo.toml : [ dependencies ] flecs_ecs = " 0.1.1 " and start hacking away! Make sure to check out the Rust docs (improvements coming soon), Flecs docs , and the 70+ examples in the examples directory. For an example integration of Flecs with the following crates: WGPU for rendering winit for windowing vello for rasterization parley for text check out the demo here Status: Alpha release The project is in the alpha release stage where the core functionality and some addons of Flecs have been ported and are available to use today. While there has been a lot of thought put into the current API, it's still in an experimental phase. The project aims to hit stable when all the soundness/safety issues are resolved and the API has been finalized with all of Flecs functionality. We encourage you to explore, test, and provide feedback, but please be aware of potential bugs and breaking changes as we continue to refine the API and add new features. This library was made publicly available on the release date of Flecs V4 release. Safety One important safety factor that has yet to be addressed is having multiple aliases to the same component. This is a known issue and is being worked on. It will be addressed through a table column lock mechanism. Performance From initial benchmarks and tests, the Rust API is on par with C-level performance, except for where overhead was introduced to make the API safe to use in Rust land (e.g. get performance). However, performance improvements are planned to be made in the future. The progress For detailed feature progress, please visit the issues page. Core library Addons (Meta + Json + Script are a WIP, expected to be released by end of August, experimental phase sooner) Documentation Most functions are documented and contain a C++ alias. Flecs documentation site contains Rust code. The remaining % is for adding mostly doc test examples and refining the Rust docs page. Test suite (entity, query, observers systems test cases are done) Examples For the current feature set, all examples are done. The Aim The plan is to match feature parity of the C++ API, starting with the core library (done!) while also being fully documented and tested and addressing any safety issues that may arise. The project aims to provide a safe, idiomatic, and efficient Rust API for Flecs, while also being a good citizen in the Rust ecosystem. Contributions If you're excited about this project and would like to contribute, or if you've found any bugs, please feel free to raise an issue or submit a pull request. We'd love to have your involvement! License MIT license, matching Flecs. Example code use flecs_ecs :: prelude :: * ; # [ derive ( Debug , Component ) ] pub struct Position { pub x : f32 , pub y : f32 , } # [ derive ( Debug , Component ) ] pub struct Velocity { pub x : f32 , pub y : f32 , } # [ derive ( Component ) ] pub struct Eats ; # [ derive ( Component ) ] pub struct Apples ; fn main ( ) { // Create a new world let world = World :: new ( ) ; // Register system world . system :: < ( & mut Position , & Velocity ) > ( ) . each ( | ( pos , vel ) | { pos . x += vel . x ; pos . y += vel . y ; } ) ; // Create an entity with name Bob, add Position and food preference let bob = world . entity_named ( "Bob" ) . set ( Position { x : 0.0 , y : 0.0 } ) . set ( Velocity { x : 1.0 , y : 2.0 } ) . add :: < ( Eats , Apples ) > ( ) ; // Show us what you got println ! ( "{}'s got [{:?}]" , bob.name ( ) , bob.archetype ( ) ) ; // Run systems twice. Usually this function is called once per frame world . progress ( ) ; world . progress ( ) ; bob . get :: < & Position > ( |pos| { // See if Bob has moved (he has) println ! ( "{}'s position: {:?}" , bob.name ( ) , pos ) ; } ) ; // Output: //  Bob's got [Position, Velocity, (Identifier,Name), (Eats,Apples)] //  Bob's position: Position { x: 2.0, y: 4.0 } } FAQ What's next? Meta, Json, Script addons. This will allow for reflection, serialization, and scripting capabilities for creating entities and components. See the Flecs documentation for more information. Wasm unknown unknown. The project is currently in the process of supporting wasm32-unknown-unknown target. This is expected to land in some shape or form by the end of August. API refinements, resolving safety issues & documentation. C# scripting support. Integration with Flecs.Net to work seamlessly with Flecs Rust API. More demos and examples. How does it compare to other Rust ECS libraries? Flecs isn't written natively in Rust, it's written in C, but it's a mature and feature-rich ECS library that has been used in AAA games and other commercial software. It's fast, lightweight, and has a lot of features that other ECS libraries don't have. Some of the features that make Flecs stand out are: Everything's an entity. Systems, queries and components are all entities. Focus on builder APIs and DSL macro over the type system: [Builder API] world . system :: < & A > ( ) . with :: < B > ( ) . each ( || { } ) ; [DSL API] system ! ( world, & A , B ) . each ( || { } ) ; Singletons (Resources) are modelled as a component added to it's own entity. world . set ( GameTime { delta_time : 0.0 } ) ; [Builder API] world . system :: < & GameTime > ( ) . term_at ( 0 ) . singleton ( ) . each ( || { } ) ; [DSL API] system ! ( world, & R ( $ ) ) . each ( || { } ) ; Systems/observers are based on queries, and will only run if that query matches. Systems are single-threaded by default and run in order of declaration (See docs for more info on how parallelism and how pipelines work in flecs) Support for building your own custom Pipeline. Relationships are first-class citizens in Flecs, allowing for easy creation of hierarchies. union relationships, exclusive relationships, oneof constraints, relationship traversal, reflexive relationships component inheritance transitivity query variables toggleable components entity disabling builtin hierchies with automatic cleanup prefabs, prefab inheritance, prefab slots, prefab hierarchies flecs script & flecs script templates (hierarchical) entity names archetype-level change detection query sorting query grouping support for unregistration: component, modules (plugins), systems, observers event propagation, event forwarding runtime components runtime reflection with a language agnostic reflection framework a language agnostic core etc Projects using Flecs Rust API This list contains projects that are not under NDA. If you want to showcase your project, feel free to open a PR to add it to the list. [Hyperion]: It switched from using Envio ECS to Flecs, with great performance improvements. It's quest is to break the world record minecraft event of 10000 players. Acknowledgements A big shoutout to Sander Mertens for creating such a wonderful library and the pre-alpha testers who contributed to Flecs Rust API, especially James , Bruce , and Andrew .
======>
https://www.guinnessworldrecords.com/world-records/105603-largest-videogame-pvp-battle
-->>-->>
Who EVE Online, CCP Games, Fury at FWST-8 What 8,825 total number Where Not Applicable When 06 October 2020 The largest multiplayer videogame PvP battle consisted of 8,825 players, and was achieved by EVE Online, developed by CCP Games (Iceland) at a battle called 'Fury at FWST-8', on 6 October 2020. Fury at FWST-8 was PAPI's attempt to establish a beachhead Keepstar in Delve that would allow them to strike deeper into the Imperium's territory with their Supercapital forces. To prevent this, the Imperium fought long and hard to destroy the Keepstar and push PAPI back. As a result, 6,746 ships and one Keepstar Upwell Structure were destroyed, 362 capital-class ships were lost, with the total cost of the battle coming to 1.443 trillion ISK (equating to $18,712 USD). Photo credit: Razorien EVE Records change on a daily basis and are not immediately published online. For a full list of record titles, please use our Record Application Search. (You will need to register / login for access) Comments below may relate to previous holders of this record.
======>
https://old.reddit.com/r/rust/comments/1fmwgux/using_rust_for_working_with_arduino_uno_r3/
-->>-->>
Firstly, I love rust.
Secondly, I want to work on a project that involve camera based tracking using facial recognition to prioritise tracking some people over others and integrate the whole thing with a web app that allows configuration of faces to prioritise.
The whole facial recognition and web app part is what I am good at, but the thing that I am not at all good at is the circuitry and making the actual hardware for it.
I have found an kit my friend gave me an year ago, it is a spin off Arduino kit but my friend used it for learning and it worked well with the Arduino IDE and is in really good condition.   

   I love rust and I really want to use rust for coding on Arduino rather than using the Arduino IDE.   

   I am rather confused at the provess of learning and configuration that I have to go through to get this to work.
I did found a video that explains the benifits of using rust for embedded programming, but I got nothing about how to actually learn or work with the thing.
I want to be able to integrate various components into the system and for that alot of learning is required.
If anyone can help me with some resources for learning and the learning process I should follow to actually properly learn this, it would be aloy helpful.   

   Any help is appreciated!   
   

======>
https://old.reddit.com/r/GraphicsProgramming/comments/1fms2ps/shadervalidator_a_shader_language_server_for_hlsl/
-->>-->>
Hello there,   

   Its been some months that I have released a first version of a vscode extension shader-validator, and i think its time for some sharing with everything I have added since then. This is an extension based on the    LSP protocol    which support some basics features such as :
- Diagnostics: relying on validator API (glslang for glsl, dxc for hlsl, naga for wgsl)
- Symbols: goto, hover, signature, completion providers aswell
- Syntax highlighting: Better syntax highlighting than the one in vscode   

   Its also working on the web version of VS code    vscode.dev    !   

   You can get it from    marketplace    or    OpenVSX    !   

   Feel free to give me some feedbacks, repo is    here    for curious.   

   Under the hood   

   The extension is relying on language server protocol, so you have a language server written in Rust that is interacting with the extension, that could even be used for any other IDE (demanding some extension to support it) as its following the LSP protocol.   

   To support web version of vscode, the server can be compiled to    WASI    and run into VS code using some newly added features for    WASI   . Due to some limitation regarding dxc compilation that do not compile to WASI, there is also classic executable of server with DXC support, else hlsl fallback to glslang which also support HLSL but with less features (up to sm5.0).   

   Roadmap   

   
   Add all intrinsics for HLSL   
   Improved support for WGSL (using naga-oil for Bevy instead of pure naga ?)   
   Improved symbol provider (possibly using tree-sitter)   
   
   

======>
https://old.reddit.com/r/rust/comments/1fmu9dy/searched_vs_hardcoded_code_in_ml_libraries/
-->>-->>
https://crates.io/crates/zyx   

   https://github.com/zk4x/zyx   

   Hello, I am the creator of zyx, ML library written in rust. This is a release annoucement for v0.14.0, but I wanted to use this opportunity to ask you a question:   

   Are you interested in ML libraries like tinygrad, jax or zyx, which do not use hardcoded kernels, but instead use limited number of instructions and use search to get maximum performance on all hardware?   

   Pytorch and similar libraries (like Candle, dfdx, burn) are great libraries, but they have hard time supporting various hardware. They contain dozens or hundreds of ops and each must be optimized manually not only for each platform (CUDA, HIP), but also for each device (difference between 2060 and 4090 is not just performance), to the point that many devices just don't work (like old gtx 710).   

   Tinygrad showed that we only need elementwise ops (unary, binary), movement ops (reshape, expand, pad, permute) and reduce ops (sum, max). Matmuls and convs can be written using just those ops. Zyx uses the same opset, but I believe somewhat simpler instructions, for example this is matmul in zyx:   

   global + local loops   

   Accumulator z   

   Loop   

   Load x   

   Load y   

   Mul a <- x, y   

   Add z <- a, z   

   EndLoop   

   Store z   

   This kernel gets searched over and zyx achieves 3 TFLOPS on 2060 in f32 1024x1024x1024 matmul, tinygrad gets 4 TFLOPS and pytorch achieves 6.5 TFLOPS, but I have only implemented search for local and private work sizes and tiled accumulators. No register tiling yet.   

   Zyx also does not need requires_grad=True. Since zyx is lazy it is all automatic and you can just differentiate anything anywhere. No explicit tracing.   

   Zyx currently supports opencl, cuda and wgpu. HIP backend is written, but HIPRTC does not work on my system. If it works on yours, you can finish HIP backend in just 10 lines of code mostly by copying over CUDA backend code.   

   In conclusion I would like to ask whether you find idea of automatic optimization for all hardware interesting, or whether you prefer handwritten implementations?   

   Also would you be interested in contributing to zyx?   

   At this point it would be cool if we together could get enough tests and models working so that zyx could be considered stable and reliable option. Currently it is buggy, but all of those bugs require just small fixes. With enough eyballs all bugs are shallow.   

   What needs to be done?   

   Register and local memory tiling (that should match performance of pytorch in matmuls), tensor core support and then make the kernels bigger and implement fast attention. That would be pretty much all optimizations that exist in current ML libraries.   

   Implement once, benefit on all platforms.   

   Thank you.   

   P. S.   

   I used AI to write some of the docs (not code, because AI cannot write good code) and they certainly would benefit from improvement.   
   

======>
https://git-cliff.org/blog/2.6.0/
-->>-->>
Recent posts 2024 What's new in 2.6.0? ðŸ†• What's new in 2.5.0? What's new in 2.4.0? What's new in 2.3.0? What's new in 2.2.0? What's new in 2.6.0? ðŸ†• September 22, 2024 Â· 3 min read Orhun ParmaksÄ±z Author of git-cliff git-cliff is a command-line tool (written in Rust ) that provides a highly customizable way to generate changelogs from git history. It supports using custom regular expressions to alter changelogs which are mostly based on conventional commits . With a single configuration file , a wide variety of formats can be applied for a changelog, thanks to the Jinja2/Django-inspired template engine . More information and examples can be found in the GitHub repository . What's new? â›°ï¸ â€‹ The full changelog can be found here . ðŸ› ï¸ Deprecated integration fields â€‹ The following fields are deprecated and will be removed in the next releases: commit.github , commit.gitea , commit.gitlab , commit.bitbucket You can now use the commit.remote field instead. For example: - {% if commit.github.username %} + {% if commit.remote.username %} ðŸŒ² Better branch support â€‹ If you have diverged branches for your project and want to changelog for each branch, you can now use the --use-branch-tags option. $ git cliff --use-branch-tags The generated changelog above will only include the tags from the current branch. Also, you can use it from the configuration file: [ git ] use_branch_tags = true info See the implementation for more explanation and the coolest hand-drawn diagram ever! â™¾ï¸ Render always â€‹ Do you want to always render the changelog even if there are no changes? Boom, now you can now use the render_always option: [ changelog ] render_always = true ðŸ“¤ Output from configuration â€‹ This is pretty self-explanatory: [ changelog ] output = "CHANGELOG.md" This option does not take precedence over command-line arguments which means you can override it with the --output option. ðŸ“¦ Improve Typescript API â€‹ We added the missing options and documented all options with tsdoc comments. Also, we improved the skipCommit option to accept an array of values. info See the implementation for more information. âœ‚ï¸ Trim commit messages â€‹ We now remove the trailing newline for commits, which means you can use $ anchor in your regular expressions: [ git ] commit_preprocessors = [ # remove the issue number at the end of the commit message (e.g. #123) { pattern = ' #\d+$' , replace = "" } ] ðŸŒŸ Better example templates â€‹ The example templates are now more intuitive and conventionally correct. We removed the non-beginner-friendly options and changed the defaults to be easier to start with. Weheee! ðŸ§° Other â€‹ (template) [ breaking/core ] Add name parameter to the constructor - ( e577113 ) (bump) Suppress template warning when --bumped-version is used ( #855 ) - ( 8bebbf9 ) (changelog) Do not change the tag date if tag already exists ( #861 ) - ( fbb643b ) (changelog) Correctly set the tag message for the latest release ( #854 ) - ( e41e8dd ) (changelog) Don't change the context when provided via --from-context ( #820 ) - ( ff72406 ) Contributions ðŸ‘¥ â€‹ @nejcgalof made their first contribution in #853 @pplmx made their first contribution in #824 Any contribution is highly appreciated! See the contribution guidelines for getting started. Feel free to submit issues and join our Discord / Matrix for discussion! Follow git-cliff on Twitter & Mastodon to not miss any news! Support ðŸŒŸ â€‹ If you liked git-cliff and/or my other projects on GitHub , consider donating to support my open source endeavors. ðŸ’– GitHub Sponsors: @orhun â˜• Buy Me A Coffee: https://www.buymeacoffee.com/orhun Have a fantastic day! â›°ï¸ Tags: release Edit this page Older post What's new in 2.5.0? What's new? â›°ï¸ ðŸ› ï¸ Deprecated integration fields ðŸŒ² Better branch support â™¾ï¸ Render always ðŸ“¤ Output from configuration ðŸ“¦ Improve Typescript API âœ‚ï¸ Trim commit messages ðŸŒŸ Better example templates ðŸ§° Other Contributions ðŸ‘¥ Support ðŸŒŸ
======>
https://old.reddit.com/r/rust/comments/1fmt4u7/new_update_of_enekafka_an_easy_to_use_kafka/
-->>-->>
ene-kafka is an easy to use Kafka client for Rust intended to make the communication with Kafka from Rust (micro)services easier and more intuitive to use. The goal of ene-kafka is to accelerate the speed of delivery of Kafka-related projects in Rust (such as event-driven services) and also to have an extensible kafka client. Extensible in the sense that I can replace the underlying kafka client (currently rdkafka) with another client if I need to (for example a Rust native client), or that I can extrend the interface with new functionalities.   

   Currently the kafka options for Rust are quite limited -- AFAIK there is only 1 native-rust implementation of a Kafka client and it's not very matrue yet. There is also rdkafka, which is Rust bindings for librdkafka. It works fine and has a great amount of features, but the interface was a bit low-level to use it on a regular level for me. That's why I came up with ene-kafka. I wanted something simple and high level with the right amount of functionalities for me. ene-kafka now uses rdkafka as the underlying kafka-client.   

   From the consumer side, ene-kafka works with a consumer->dispatcher->handler kind of architecture (diagram:    https://github.com/ene-rs/ene-kafka/blob/main/docs/consumer\_architecture.md   ). As a library user you mostly need to define the event handlers (what to do when an event of a specific kind comes in), and configure the consumer.   

   Today I am releasing a new update with more stability and improvement. I realized that I was doing some redundant steps in the code generation for handlers and dispatchers. The interface for adding handlers to a consumer was as follows (via a macro):   

   let consumer = kafka_consumer!(
    topic = KafkaTopic {
        name: "test".to_string(),
        content_type: ContentType::Json
    },
    dlq_topic = KafkaTopic {
        name: "test-dlq".to_string(),
        content_type: ContentType::Json
    },
    consumer_group_id = "test-group",
    bootstrap_servers = bootstrap_servers,    
    handlers = [EntityCreatedEventHandler -> EntityCreatedEventHandler {}]
);
   

   The `handlers` is a map of handlers with handler types as keys and handler instances as value. From this map, I would generate an enumeration of all the provided handler variants, which looks like this:   

       ($($handlerType: ident)*) => {
        enum HandlerTypes {
            $($handlerType($handlerType))*
        }
   

   Then I would create a vector of handlers in the dispatcher (also generated), and on every incoming event, search that vector for the correct handler. This looked like this:   

   for handler in self.handlers.iter() {
  match handler {
      $(HandlerTypes::$handlerType(handler) => {
        if ene_kafka::handlers::EventHandler::can_handle(handler, event)? {                          
           handler.deserialize_and_handle(even).await?;
         }
}),*
   

   You see the issue? The vector would just contain every enum variant exactly once. It was quite useless to have heap allocations and the enum definition for this. Then I realized that this can be much simpler if I just used the plain old way that Rust does the variable declaration. The `handlers` part of the consumer definition becomes something like this:   

   handlers = {
        entity_created_event_handler: EntityCreatedEventHandler = EntityCreatedEventHandler {}
    }
   

   From that, I don't generate an enum , but instead I just generate struct fields in the dispatcher directly for every handler   

   struct CloudEventDispatcher {
$(
     $handler_name: $handler_type,
)*
}
   

   And then during the dispatching I just run if checks on all handlers (the generated fields) until I find the correct handler   

   $(if self.$handler_name.can_handle(event)? {
        return self.$handler_name.deserialize_and_handle(event).await;
   }
)*
anyhow::bail!("No handler found for event type {:?}", event.event_type()?);
   

   No vector or enumerations :) The code is much simpler now to maintain IMO. Just a minor improvement that I wanted to demonstrate here.   

   ene-kafka has been a great project for me so far and it introduced me to the macro system of Rust and really deepened my knowledge on Kafka and Rust.   

   The project is Apache or MIT licenced -- feel free to surf around and contribute/use/copy from the project    https://github.com/ene-rs/ene-kafka/tree/main    Any feedback is welcome   
======>
https://old.reddit.com/r/rust/comments/1fn2bvl/which_gui_library_can_you_recommend_for_60_fps/
-->>-->>
Hey! I'm unsure which GUI library is the best to display a 
60fps wave-function and I'm seeking advice. Specifically, I want to split the framebuffer horizontally into two parts, where the upper part shows the past 5 seconds of the audio input waveform wheres the bottom part shows the data after being low-passed.   

   I'm aware of minifb and plotters, I created very basic stuff with both.
Plotters seems very cool, but it seems that it is focused on static renders or low fps visualizations instead of 60fps/live animations. minifb is cool and flexible, but needs more work on my side.   

   Any idea or recommendation?   

   PS: I'm experienced with Rust for many years, just not with GUI programming and visualization where I only know the most basic fundamentals.    
   

======>
https://github.com/Y-jiji/peggen
-->>-->>
Repository files navigation README Peggen A parser generator for parsing expression grammar (PEG) that use inline macros to specify PEG operations. Roadmap Help needed! There is so much to do! Contact Me Optimizations: Rule dispatch: filter rules by the first symbol, instead of trying each of them. Thinner tag: currently each tag in internal representation is 3-pointers wide, I want to make them thinner. Error Handling: Custom final error handlers when custom error capturing fails. Documentation: Demonstrate features like precedence climbing, error handling, repetition, custom FromStr , arean allocation, and left recursion handling. How is it different from (...)? / Conceptual User Experience Performance & Code Quality Error Handling PEST PEST only annotates text. Peggen generates AST directly from your text. In most cases, you still want rust enum s for your AST, which is directly provided by Peggen , but you have to manually create enums from PEST rules. PEST use an optimizer to memorize your grammar rules, and use memorization for better performance; Peggen doesn't use memorization, arguably this gives better performance over memorization for most grammars. / Chumsky Chumsky provides parser combinators. Peggen is a parser generator. Both Chumsky and Peggen provides ast directly. However, Peggen supports arena allocation. Chumsky deallocates successful sub-rules when a rule fails; Peggen uses a internal representation to eliminate deallocation. Besides, Peggen handles left recursion, while in Chumsky left recursion causes in stack overflow. / LALRPOP Peggen is PEG-based; LALRPOP uses LR(1) grammar. Peggen is more intuitive to use than LALRPOP ; LR(1) grammar is hard to extend and debug. LALRPOP has better performance over Peggen . LR(1) grammar can report errors far away from normally percepted cause; Peggen allows you to capture errors from customary cause. Performance I roughly tested the peggen on a sample json file against chumsky. CPU Model: Intel(R) Core(TM) i7-14700HX Suprisingly, Peggen is faster than Chumsky. Here are some numbers: Peggen : 867913 ns/iter Chumsky: 1464737 ns/iter Example: Json Parser Step By Step Final Result Before we start this tutorial, let's look at how it looks like after your first try. # [ derive ( Debug , ParseImpl , Space , Num , EnumAstImpl ) ] pub enum Json { # [ rule ( r"null" ) ] Null , # [ rule ( r"{0:`false|true`}" ) ] Bool ( bool ) , # [ rule ( r"{0:`-?(0|[1-9][0-9]*)\.([0-9]+)`}" ) ] Flt ( f32 ) , # [ rule ( "{0:`0|-?[1-9][0-9]*`}" ) ] Num ( i32 ) , # [ rule ( r#""{0:`[^"]*`}""# ) ] Str ( String ) , # [ rule ( r#"\{ [*0: "{0:`[^"]*`}" : {1} , ][?0: "{0:`[^"]*`}" : {1} ] \}"# ) ] Obj ( RVec < ( String , Json ) > ) , # [ rule ( r"\[ [*0: {0} , ][?0: {0} ] \]" ) ] Arr ( RVec < Json > ) } Use Your Parser We have a Parser type that help you use annotated enum . use peggen :: * ; fn main ( ) { let json = Parser :: < Json > :: parse ( "{x: 1, y: 2}" ) . unwrap ( ) ; println ! ( "{json:?}" ) ; } Step 1: Formatting String A rule attribute looks like the following: # [ rule ( "..." ) ] The string ensembles rust formatting string that you use in println!() . Rust formatting string represents a sequence of chars/structures printed one after another. Peggen formatting string represents a sequence of chars/parsers that eat the input string one after another. For example, the following statement prints: A boolean false as the first argument A token and An integer 19 as the second argument println ! ( "{0} and {1}" , false , 19 ) ; You can write a parser that parses <bool> and <int> using a rule attribute. However, Peggen needs to know what kind of string can be recognized as bool and what kind of string can be recognized as i64 . For this purpose, we can write regular expressions . # [ derive ( Debug , ParseImpl , Space , Num , EnumAstImpl ) ] # [ rule ( "{0:`false|true`} and {1:`[0-9]+`}" ) ] struct BoolAndInt ( bool , i64 ) ; Question What will the following statement print? println ! ( "{:?}" , Parser ::< BoolAndInt >::parse ( "false and 19" ) .unwrap ( ) ) ; Answer BoolAndInt(false, 19); An enum is a collection of rules, during parsing, the rules declared in an enum is tried one by one until one of them matches. # [ derive ( Debug , ParseImpl , Space , Num , EnumAstImpl ) ] pub enum Json { # [ rule ( r"null" ) ] Null , # [ rule ( r"{0:`false|true`}" ) ] Bool ( bool ) , # [ rule ( r"{0:`-?(0|[1-9][0-9]*)\.([0-9]+)`}" ) ] Flt ( f32 ) , # [ rule ( "{0:`0|-?[1-9][0-9]*`}" ) ] Num ( i32 ) , # [ rule ( r#""{0:`[^"]*`}""# ) ] Str ( String ) , } Question How to parse a string with " escaped to \" ? For example: "\"a string\"" . Answer # [ rule ( r#""{0:`([^"]|\\")*`}""# ) ] Str ( String ) Question Given that you can have multiple rules on the same enum variant, what is the alternative way of writing the Bool(bool) operation? Answer # [ rule ( r#""{0:`false`}"# ) ] # [ rule ( r#""{0:`true`}"# ) ] Bool ( bool ) Step 2: Repetition TODO
======>
https://old.reddit.com/r/rust/comments/1fmxpm9/should_i_start_with_rust/
-->>-->>
I'm in high school, and because high school, I do new stuff. I've programmed before, but I've never    developed    anything. I've done ML in python and have done Olympiad kind of stuff with C++. Would rust be a good place to start building more serious projects?   
   

======>
https://github.com/Byron/gitoxide/discussions/1602
-->>-->>
Discussion options {{title}} Something went wrong. Quote reply Byron Sep 22, 2024 Maintainer Original comment in English - Translate to English As hoped, after last months dip my total time worked went back to 156h, up from 128, with 57h spent on open source maintenance (up from 39), and 48 of which went directly into Gitoxide (up from 32). This is a surprisingly high number considering that the one feature that is still pending still wasn't worked on. Probably this means there was a lot of maintenance and smaller topics, with some of them hopefully big enough to highlight here. New it tool for better fixtures With jj now ramping up its use of gitoxide , inevitably they run into shortcomings. Only one of them was reported and is related to rename-tracking . It's known that it uses less heuristics than Git, but thus far I thought it produces decent-enough results nonetheless. However, when using it on the real-world jj repository, the difference to what Git produces is large enough to warrant an improvement. The problem was that in order to fix it, one should reproduce it in a test, and the only known way of doing that is the actual jj repository. How would it be possible to create a fixture based on it? Enter the internal-tools , short it , which aren't published and can use clap and anyhow to provide a nice CLI experience, while making tool-development easy. It comes with a git-to-sh subcommand which can extract a portion of the linearized history of a Git repository, filtered by pathspecs and with all blobs 'degenerated' to prevent having to deal with licensing. Degenerated blobs will still diff exactly the same as before, which helped to create a fixture to reproduce the exact case jj wanted to fix. The output of the tool is some files and a shell-script which regenerates a Git repository with blobs, trees and commits. With this, I hope that `jj` developers will be able to contribute improvements to the rename-tracker, and make it more similar to what Git currently offers. It's notable that such real-world fixtures can also be used to have more meaningful benchmarks, something that right now is also notably absent in the rename-tracker. Various API improvements driven by GitButler When coming from git2 , recently one has been missing various little methods that were incredibly practical to have despite most of them being available 'differently' already. In order to make conversions from git2 to gix simpler, some convenience was added, namely: Repository::find_commit , find_tag , find_tree and find_blob Previously one would use find_object(id).try_into_<kind>() in order to do the conversion in a separate step. Reference::peel_to_id_in_place() now has peel_to_kind(kind) and peel_to_commit() , peel_to_tag() , peel_to_tree() and peel_to_blob() . Emulating reference-based peeling wasn't possible previously as peel_to_id_in_place() was a very specific and is what Git thinks of peeling. Having the new shortcuts definitely helps going from ref to usable object quickly, and without using a refspec. More improvements are planned, and I see how this will greatly facilitate switching from git2 to gix . GitButler - from so-so to 'wow'! It took a while, but I am now very busy translating existing git2 code to gix . It's done in such a way that not only specific tests are added to assure correctness at least around the boundary, but also there is the addition of benchmarks to assure the new implementation is at least as fast. First off, to compare apples to apples git2 is now configured to be more similar to gix , which also doesn't constantly verify objects. This alone is the reason for a measurable speedup in GitButler, which still runs mostly on git2 after all. With that, it was possible to improve the generation of branch details by a factor of 2.6x , as a follow-up to listing branches which got faster from 1.5x, up to over 20x to 35x. And even though gix typically ends up faster, git2 offers great value nonetheless as it's fast-enough by default while sporting a pretty simple and usable API. gix isn't like that at all for a lot of functionality and needs plenty of API improvements to get closer or just reach similar usability. Simple APIs also comes with the drawback that sometimes, that it is hiding so much under the hood that ultimately it becomes too slow in bigger repositories, which is something that a Git client has to be mindful of. So API complexity is also a strength as it provides great levels of control that often are needed to not fold in the face of real-world scenarios. Security gitoxide (CLI) and terminal-escape code Thanks to Eliah there is a new advisory that points out what could happen if an attacker decided to abuse terminal escape codes to guide the gix (CLI) user into executing malicious commands. Git will escape these consistently across all of its commands, but gix does none of that right now. Mitigations are planned for this low-risk advisory. Ongoing investigation: executing git and how it can be abused gix-path has one very 'interesting' method that tries to find the location of the Git installation on the system by deriving it from special installation-wide git configuration files. In order to do that, it needs to invoke git config , and there is always a risk in that. Not only can attackers possibly control the binary, but they might also try to affect what the run returns, maybe to inject configuration into gitoxide . A recent change now places the invocation into the $TMP directory of the system, which was originally meant to fix performance issues when it would try to detect a Git repository traversing upwards from the current working directory. As it turned out, starting in /tmp on most systems actually increases security as Git will reject repositories (even in /tmp/.git ) that aren't from the current user. Also will it hit the ceiling very fast. Thus, said change actually increases security. Community 'Blame' is getting there @cruessler , a long-time contributor to gitui has started contributing a first implementation of the git blame algorithm , which is planned to one day revolutionize the way blames are done in gitui . The idea is to start simple, but refine the control the caller has over the algorithm to get to the point of a fully-streaming implementation that provides results continutiously. This way, the user-interface can be populated interactively and most importantly, the user can change what's blamed at any time, or skip over commits that they don't want to see, all in real-time and without loosing progress. There is no other way but for this to become the blame I always wanted! Git Meetup in Berlin On the 14th of August a lot of like-minded people met to revive the Git Meetup Berlin group, which was generously hosted by GitButler. There we learned about GitButler, RefTables, and things nobody ever knew about gitoxide (and probably will never hear again :D). Gitoxide and the Sovereign Tech Fund No progress was made and it's a bit embarassing. In my mind I am blocked on not really wanted to do all the project-planning that they maybe needed. Another part just wants to get something done though, so I am still eying to try focus on this while I can't do anything else, like when in transit. Gix in Cargo There is nothing to report here, except for one issue related to failing to correctly deal with .gitignore -based excludes. There one would start out with a blanked ignore rule like /* and follow with !<what you want to include> to undo the exclude selectively. Fortunately, the issue was already fixed previously in gitoxide as discovered by Eliah, and an upgrade to gix v0.64 would be all that was needed. Cheers Sebastian PS: The latest timesheets can be found here (2024) . Beta Was this translation helpful? Give feedback. 7 You must be logged in to vote 6 11 5 All reactions 6 11 5 Replies: 0 comments Sign up for free to join this conversation on GitHub .
    Already have an account? Sign in to comment
======>
https://fractalfir.github.io/generated_html/rustc_codegen_clr_v0_2_1.html
-->>-->>
I am currently working on a Rust to .NET compiler, rustc_codegen_clr . To get it to work, I need to implement many Rust features using .NET APIs. One of such features is panicking and unwinding. This article is the first one in a series about Rust panics, unwinding, and my implementation of them in .NET. In this part, I will look at unwinding (the compiler side of panicking), and in the next one, I will explain the Linux GNU std implementation of panicking. How a Rust to .NET compiler works: Quick recap Before I talk about the project, I should probably explain what it is. It is a "rust compiler backend" - but what does that mean? You can imagine it as a compiler plugin, which replaces the very last step of compilation (code generation). Instead of using LLVM to generate native code, my project turns the internal Rust representation called MIR, into .NET Common Intermediate Language. CIL is then stored inside .NET assemblies, which allows the .NET runtime to load, and execute the compiled Rust code easily. From the perspective of the Runtime, the compiled Rust looks identical to unsafe C#. So, the Rust code can easily call .NET functions and create .NET objects. In theory, there is nothing you can do in C# that can't be done in Rust too. I am also working on making calling Rust from C# easier. The project allows you to define .NET classes in pure Rust. In the future, the safety of your interop code will be fully checked by the Rust compiler. // Early WIP syntax, subject to change. dotnet_typedef! {
  class MyClass inherits [Some::External::Assebmly]SomeNamespace::SomeClass{ virtual fn ToString (_this:MyClass) -> MString{ "I am a class defined in Rust!" . into_managed ()
    },
  }
} Copy The end goal is to allow near-seamless interop between Rust and C# / F#. Ideally, the users of your Rust .NET library may not even realize that it is written in Rust. Panicking vs unwinding Now that I laid down some basics about how rustc_codegen_clr works, I will probably need to quickly explain what unwinding and panicking are. While the terms panicking and unwinding are related, and sometimes used interchangeably, they are nonetheless 2 different things. Panicking is the name for the feature of the Rust language. When you use the panic! macro, this is the feature you are using. Panicking is very similar to exceptions, and is used in Rust to signal an error in the program logic. So, when you expect an operation could fail, you use the Result type. When an operation should not fail(a failure indicates a bug), you should use panics! . // Reading and parsing a file can fail, so we return a `Result` to tell the consumers of our API that they should handle it. fn decode_file (path:&Path) -> Result <DecodedFile, DecodeError>{ // ... } // A person's birth date can't be earlier than the current date. // If it is, then either that person is a time traveler, or we have a bug. fn calcualte_age (date_of_birth:Date) -> Duration{ if Date:: now () < date_of_birth{ panic! ( "Found a time traveler!" );
  } // ... } Copy In general, Result s are used where an error is expected, and should be handled by the user of the API, while panics tend to be used when something unexpected happens(eg. an assertion fails, an array is indexed out of bounds). While panicking is not used as commonly as Result s, it is still used in quite a few places(eg. in unit tests). So, this feature must work well. Panicking is currently implemented using unwinding, but that does not have to be the case in the future - you can implement panicking in other ways(eg. a panic can simply terminate the program). Those other implementations have their drawbacks, but there are (rare) situations in which using something other than unwinding makes sense. Unwinding refers to a specific implementation of panicking, where the stack is unwound - traversed upwards, function by function, while the data on the stack is dropped. When I refer to unwinding, I will talk about the specific implementation on the compiler level. Here, things like cleanup blocks or the exact implementation of certain intrinsic matter. When I talk about panicking, I will refer to the implementation present inside the standard library. From the perspective of std, it just calls a few intrinsics and functions, and it does not care how things are implemented under the hood. Unwinds on the compiler level. At the conceptual level, an unwind is relatively easy to understand. An unwind travels up the call stack, popping stack frames of each function along the way, and dropping (disposing of) the stack-allocated objects held by that function. So, if we had some functions like this: fn a (){ let numbers = vec! [ 0 ; 100 ]; b ();
} fn b (){ let hello_string = "Hello World" . to_string (); c ();
} fn c (){ let some_value = Box :: new ( 64 ); panic! ();
} Copy During an unwind(caused by the panic in c ), first the some_value would be dropped, then the unwind would go up the call stack, into b . The hello_string would then be dropped, unwind would continue up to a , and drop the vector numbers it allocated. This process would continue until the unwind encounters a special intrinsic named catch_unwind . The exact signature and behavior of catch_unwind is not relevant for now, so I will discuss it in the next article. Looking at this process, you might ask yourself a question: How does an unwind know what and how to drop? I mean, dropping a Vec<i32> is very different from dropping a Box<i32> , so how do we know how all the objects in that stack frame need to be dropped? How can we tell where a pointer to the string hello_string is on the stack, and how can we know what to do with it? This process of dropping (disposing of) all the data in a stack frame is the responsibility of special pieces of code called "cleanup blocks" Blocks in general Before I explain what a cleanup block is, I should probably explain MIR blocks in general. Each function in the Rust Mid-level Intermediate Representation (MIR) is made up of a series of basic blocks. Those blocks represent the control flow of a program. A block is made up of a bunch of statements, and a terminator. The statements in a block are executed in sequence, and can't diverge or branch in any way. No matter what happens, they will get executed, one by one. The terminator, on the other hand, can change the control flow of a function. It can call other pieces of code, jump to some other block, resume an unwind, or abort the execution of a program. While this may seem a bit odd at first, it is a very convenient representation. It makes analyzing control flow almost trivial - since we know that only terminators can change how the code is executed, we don't have to check each statement to figure out the relationships between blocks. Another nice side-effect of this distinction is that it simplifies unwinding and cleaning up stack frames. Since only a terminator can call other functions,  only terminators can panic. Executing a statement will never panic, since statements can't change the control flow of a program. So, when we want to specify how to deal with a stack frame during an unwind, we only need to do so for each terminator. Now that I explained what a basic block is in the context of MIR, I can show exactly what a cleanup block is. Cleanup blocks As I mentioned before, a cleanup block is not called during â€œnormalâ€ execution. The sole purpose of a cleanup block is dropping (disposing of) all the things in a given stack frame. So, during an unwind, when we want to figure out how to drop a particular frame, we simply call the specified cleanup block. The Rust compiler generates such a cleanup block every time it needs something to get dropped during an unwind. This concept can be a bit hard to grasp, but a small example should make it easier to follow. I will demonstrate how a cleanup block works, by using some Rust-inspired pseudocode. Let's say we have a program like this: fn test (x: u32 ){
bb0:{ let x = 7 ; // If an unwind happens during this function call, `numbers` will not be yet owned by us // Since we only have ownership of `numbers` once the we the vector, //dropping it during an unwind is the responsibility of the callee. // So, we don't need a cleanup block. let numbers = vec! [x] -> [ return bn1, unwind continue ];
}
bb1:{ // Up till this point, there was nothing to drop. Now, after the variable `numbers` was allocated, there is something to drop, // so we specify that the cleanup blocks bb4 need to be called if an unwind happens. x = 8 ; do_something (&numbers) -> [ return bn2; unwind bn4];
}
bb2:{
 x = 9 ; // Since calling pass_ownership requires passing the ownership of `numbers`, dropping it is the responsibility of the callee. // So, we don't specify any cleanup blocks. pass_ownership (numbers) -> [ return bn3, unwind continue ];
}
b3:{ return ;
} // The cleanup block generated by the compiler bb4 (cleanup):{ // If this block is called, then we need to dispose of `numbers`. So, we drop it. drop (numbers);
 unwind resume; // Continues the unwind }
} Copy The real MIR looks a bit different(no variable names, explicit types of locals, calls that return nothing assign a () value), but this should give you a rough idea about how everything works. As you can see here, cleanup blocks are not really all that scary. Despite the fancy name, they are eerily similar to finally or using in C#. // If an exception occurs, numbers.Dispose() will be called. using (Vec< int >  numbers = new Vec{x}){
  do_something( ref numbers);
} Copy The main difference between using / finaly and a cleanup block is that it is only called when an unwind happens, while using disposes of the resource when the control flow leaves it. So, it will get called when an exception occurs, but also just during normal execution. Still, despite those differences, unwinds and exceptions map to each other nicely, and exception handlers can be used to implement cleanup blocks. There are, of course, some differences between the two which make this not a 1 to 1 translation. MIR cleanup blocks can jump to other cleanup blocks, and a .NET execution handler canâ€™t jump into another exception handler. Working around that required some tinkering, but it was not a big problem. Now that I talked a little bit about cleanup blocks in general, I thought I might mention another oddity of unwinds in Rust. Drop flags are variables, whose sole purpose is to tell a cleanup block if it should drop something or not. Hang on a minute, did I not just tell you that we only need at most one cleanup block for each terminator, since only terminators can cause an unwind? If we already have separate cleanup blocks for different terminators, why do we need additional flags to tell the cleanup block what to drop? To drop or not to drop: that is the question! Suppose we have some code like this: // Example taken from the Rustonomicon // https://doc.rust-lang.org/nomicon/drop-flags.html let x ; if condition {
    x = Box :: new ( 0 ); // x was uninit; just overwrite. println! ( "{}" , x);
} // x goes out of scope; x might be uninit; // check the flag! Copy As you can see, x may or may not be initialized, depending on the condition. If x is not initialized, then dropping it is UB. If it is initialized, then not dropping it would be a mistake. To solve this, the compiler introduces a second variable, which checks if x is initialized. let x ; // Hiden, compiler-generated drop flags let _is_x_init = false ; if condition{
    x = Box :: new ( 0 );  
    _is_x_init = true ; println! ( "{}" , x);
} if _is_x_init{ drop (x);
} Copy You may say: well, that example is very contrived, and no one writes code like this! You just have to move the definition of x within the body of the if statement, and there would be no problem! if condition{ let x = Box :: new ( 0 ); println! ( "{}" , x); // x is always initialized here, so we can drop it. } Copy Yeah, I would be shocked if someone wrote code exactly like this anywhere. But, this is the simplest possible example of the need for drop flags. More realistic examples are far more complex, and this is the easiest way to showcase why drop flags are needed. Optimizing cleanup blocks is hard. Now that I explained how rust cleanup block works, and how they get turned into exception handlers, I want to highlight a few problems  I encountered while optimizing them. Currently, the Rust code compiled for .NET suffers from massive slowdowns in a few key areas. While I don't want to discuss the exact numbers since I am not 100% confident in them, I can at least highlight some general trends. Rust compiled for .NET is, as expected, always slower than native Rust. However, it is not that much slower. In quite a few benchmarks, it is within 1.5 - 2x of native code - which I feel like is a pretty acceptable result. In the majority of cases, it is no more than 5x slower than native code. This is not great, but it is also not a bad starting point, seeing as my optimizations are ridiculously simple, and not all that effective. Being wrong fast is not impressive, so I am mostly focusing on compilation accuracy and passing tests right now. However, a select few benchmarks jump out. Some of them are pretty much expected (eg. a benchmark showing the vectorization capabilities of LLVM), but other ones may seem a bit more surprising at first. One particularly problematic area is complex iterators, some of which take 70x more time to execute in NET. That is, of course, unacceptable. You may ask yourself: what causes this slowdown? Even though those benchmarks never panic, the cost of the unwinding infrastructure is still there. Just disabling unwind support(which just removes all the exception handlers) speeds up this problematic benchmark by 2x. Of course, being 35x slower than native is still nothing to brag about, but it at least guides us toward the underlying problems. Let us look at one of the problematic functions, decompiled into C# for readability. // NOTE: for the sake of this example, the custom optimizations performed by `rustc_codegen_clr` were disabled. public unsafe static void _ZN4core4iter8adapters3map8map_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hce71e23625930189E(Closure2n22Closure1n11Closure1pi8v * P_0, RustVoid acc, long elt) { //Discarded unreachable code: IL_002b, IL_006f Closure1n11Closure1pi8 * f_; bool flag; long item; try {
    f_ = & P_0 - > f_0;
    flag = true ; void * ptr = ( void * ) 8 u;
    Tuple2i8 tuple2i;
    tuple2i.Item1 = elt;
    tuple2i = tuple2i;
    item = _ZN4core3ops8function5FnMut8call_mut17hd1566fb973e9735aE(ptr, tuple2i.Item1);
  } catch { if (flag) {} throw ;
  } try {
    flag = false ;
    Tuple3vi8 tuple3vi;
    tuple3vi.Item2 = item;
    tuple3vi = tuple3vi;
    RustVoid rustVoid;
    _ZN4iter13for_each_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hfc540be8426f74d2E(f_, rustVoid, tuple3vi.Item2);
  } catch { if (flag) {} throw ;
  }
} Copy You may immediately notice something very odd. All the exception handlers in this function are effectively nops! In some other examples, those catch blocks do assign some values to local variables, but, as soon as the handler finishes execution, those locals will be discarded anyway - since the exception will be rethrown(an unwind will resume). catch { nint num6 = ( nint ) self.b.v; if (num6 != 1 || flag) {} throw ;
} Copy And what about those useless ifs? They are empty, so they can't do anything at all! Those ifs, weird assignments, and empty handlers are ghosts of removed drops. In search for the empty cleanup blocks The example I showed you is nothing more than a faithful recreation of the original optimized MIR passed to me by the Rust compiler front end. All those useless blocks and instructions are there because the compiler explicitly asked me for them. You may think: why would rustc explicitly request a useless block? Canâ€™t it see that it does nothing? I mean, the sole reason for MIRs existence is optimization. If this is its only job, surely it would be better at it? Well, what if I told you that the purpose of MIR optimizations is not to make your code faster, but to make compilation faster? How on earth does an extra step in compilation make it faster? Doing more to do less. Well, as you may know, LLVM is not exactly the fastest thing in the world. It is very good at producing fast code, but it takes its sweet time to do it. Let us say we know that a certain kind of LLVM optimization is quite slow, but we found a faster way to achieve the same thing, by using the unique properties of Rust. Well, if rustc did just that, then LLVM wouldn't need to perform its own costly optimization, and compilation time would improve. This kind of scenario is not very common. We can, however, exploit a different property of Rust to make a lot of optimizations way faster. The nice thing about generics is that they allow for a lot of code to be shared. What if, instead of optimizing the â€œfinalâ€ monomorphized versions of a function(like what LLVM does), we could optimize the generic one? Look at this function: fn do_sth <T: Copy >(val:&T){ let tmp = *val; do_sth_inner (&tmp, 8 );
} Copy For each different type, T LLVM will receive a different version of this function from rustc Each of those variants needs to be optimized separately. Let us imagine we call this function with 1000 different Ts. This means LLVM will have to optimize 1000 different versions of this function. This will take time. However, we could try optimizing the generic version of this function. We might not be able to perform all the optimizations, but we should be able to perform quite a few of them. In this example, we could avoid storing the value of val in tmp . Since we only use the address of tmp to pass it to another function, val is an immutable reference to T , and tmp is a bitwise copy of what val points to, this optimization is OK. fn do_sth <T: Copy >(val:&T){ do_sth_inner (val, 8 );
} Copy In one move, we optimized all the possible variants of do_sth . LLVM  would have to do the same thing each time do_sth is used. So, for any function that is used more than once, we should save some compilation time. This is the main reason MIR exists. If we can optimize stuff before handing it to LLVM, we can save time. Disabling MIR optimizations would not slow the final native code all that much, but it would make LLVM have to do more work - which would increase compile times. Limitations of MIR So, we now know that MIR optimization operates on generic functions and that they can leverage Rust-specific information(eg. borrows) to perform certain optimizations faster. However, there are some problems with this approach. rustc can't perform certain kinds of MIR optimizations, since they might not be valid for all possible T s. For example, if the type in our example was Clone , and not Copy , we would not be able to optimize it all that much. fn do_sth <T: Clone >(val:&T){ let tmp = val. clone (); do_sth_inner (&tmp, 8 );
} Copy The clone could have side effects(eg. incrementing an atomic reference counter), and dropping tmp could also do something(eg. decrementing an atomic reference counter). This is not a problem in most cases - since LLVM operates on monomorphized functions(functions with T replaced by some type, like i32 ), it can still perform this optimization, where applicable. You can now probably guess why the iterator code I showcased had empty exception handlers(and cleanup blocks) - those blocks are not always empty. Ghost drop When you look at the original example, you might notice it operates on long s(aka i64 s). Dropping an i64 is a NOP. During compilation, when the cleanup block in question is processed, my backend encounters a drop. When it encounters a drop, I use Instance::resolve_drop_in_place to get information about how to drop a variable. let drop_instance = Instance:: resolve_drop_in_place (ctx. tcx (), ty). polymorphize (ctx. tcx ()); if let InstanceKind :: DropGlue (_, None ) = drop_instance.def { //Empty drop, nothing needs to happen. vec! [CILRoot::GoTo { target: target. as_u32 (), sub_target: 0 }. into ()]
  } else { // Handle the drop } Copy One of the possible ways to drop something is calling the "DropGlue". Drop glue is basically a function that calls the drop implementation. However, when something (like an i64 ) does not need dropping, the drop glue can be None . If it is None , then there is nothing to do during a drop. In such a case, I do nothing(besides jumping to the next block). So, this "empty" drop compiles into nothing - that explains our empty exception handlers. In reality, those handlers look more like this: catch { nint num6 = ( nint ) self.b.v; if (num6 != 1 || flag) { // Ghost drop drop_i64(&item); // Drops the i64 - does nothing. } throw ;
} Copy But the drop_i64 is never generated, since it is not needed. Optimizing exception handlers Optimizing exception handlers is very important for Rust to run well inside the .NET runtime. The biggest problem with them is the amount of byte code those things create. Remember - each Rust block can have its own cleanup bloc, and jumping between exception handlers is not allowed in .NET. This means that I have to store multiple copies of certain blocks. This huge amount of exception handlers is not liked by the .NET JIT. I will admit that I am not an expert when it comes to understanding how RyuJIT works - I only have a very rough idea about how all the pieces fit together. Still, I feel like I know enough to speculate about the cause of the slowdowns I observed. Since rustc_codegen_clr is not all that good at optimizing code, it generates quite a lot of it. Each unneeded exception handler, unused variable - all that adds up. While the JIT can optimize most of that away, the complexity of those functions still has an impact. Internally, most JITs use certain heuristics to estimate if certain optimizations are worth it. So, a JIT may use metrics like the number of exception handlers, locals, and the size of bytecode, to try to guess how large the final compiled function is. Since I emit a lot of code(quite a bit of it useless), the JIT "thinks" my functions are bad optimization and inlining candidates. When I removed all the exception handlers from an assembly, I drastically cut down the complexity of the generated code, which encouraged the JIT to optimize it more. JITs try to be fast at generating code, so they weigh the benefit of optimization against the time it would take. If the CIL I generate is simpler, then the JIT can see that optimizing it will not take too long. Of course, you should take this paragraph with a pinch of salt. While I am pretty confident in what I wrote, I am still an amateur when it comes to JITs. So, I might have misunderstood something and missed some finer detail. Removing empty handlers To remove a useless cleanup block(or exception handlers) I first need a good way of detecting them. Originally, I went with a more complex solution: First, I would simplify each block separately, removing all the side-effect-free statements before a rethrow . // Since assigning a constant to a local has no other side effects, and this bit of CIL is followed by a rethrow instruction, we are free to remove it. ldc . i4 .0 stloc .0
rethrow // Simpler, equivalent version rethrow Copy Next, I would replace unconditional jumps to lone rethrow ops with rethrow s. br bb1
bb1:
rethrow // Simpler, equivalent version rethrow Copy After a few more steps dealing with conditionals, most handlers would get simplified away into a single rethrow instruction. catch [System.Runtime]System.Object{
pop // Ignore the exception Object rethrow
} Copy As a last step, I would simply remove those trialy-NOP handlers. This approach had a lot of advantages - in cases where it was not able to fully optimize something away, it would still simplify it considerably. However, the added complexity made this optimization a bit error-prone. For the sake of simplicity, I have overlooked some of the non-obvious issues. Most of them were caused by some of the tech debt the project has. Fixing them requires some additional refactors, which are just not done yet. Since any bug in this code would lead to non-obvious and hard-to-debug issues, I have decided to go with a much simpler, more conservative approach for now. It is not as good at optimizing stuff away, but it can still improve things a fair bit. I simply go through all the CIL in the handler, checking if it can cause observable side effects. For example, assigning a const value to local has no side effects, while setting a value at some address has side effects. If a handler only consists of assignments to locals, jumps, and a rethrow, then it can't have any side effects, so we can just remove it. After this optimization(and other CIL optimizations implemented by rustc_codegen_clr ), the iterator function I showed before looks like this: public unsafe static void _ZN4core4iter8adapters3map8map_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hce71e23625930189E(Closure2n22Closure1n11Closure1pi8v * P_0, RustVoid acc, long elt) { //Discarded unreachable code: IL_0025, IL_0047 Closure1n11Closure1pi8 * f_ = & P_0 - > f_0; void * ptr = ( void *) 8u ;
  Tuple2i8 tuple2i;
  tuple2i.Item1 = elt;
  Tuple3vi8 tuple3vi; long num = (tuple3vi.Item2 = _ZN4core3ops8function5FnMut8call_mut17hd1566fb973e9735aE(ptr, tuple2i.Item1));
  RustVoid rustVoid;
  _ZN4iter13for_each_fold28_$u7b$$u7b$closure$u7d$$u7d$ 17 hfc540be8426f74d2E(f_, rustVoid, tuple3vi.Item2);
} Copy Don't get me wrong - there are still a lot of optimizations to be done, but this is far better than what we had before. Conclusion I hope you liked this deep dive into Rust unwinding, drop flags, and MIR optimizations. Originally, this was just supposed to be a "short introduction" to unwinding, after which I would talk about how panics work in std . Well, it turns out unwinds are a bit complex. Since this article is already one of my longest, I will have to split it into 2-3 parts. I also plan to explain the difficulties with Rust code catching arbitrary .NET exceptions and talk a little bit more about safe Rust / .NET interop. So, look forward to that :). The project is 1 year old, GSoC, and special thank you s. Oh, I almost forgot! `rustc_codegen_clr` turned 1 year old on August 28th! I would like to thank You for the amazing reception it received (over 1.4 K stars on Github, and a lot of very nice comments). I(and my project) also participated in Rust GSoC 2024. I originally planned to write an article summarizing GSoC and celebrating the project's birthday, but I was unable to do so due to health reasons. I want to thank my GSoC mentor, Jack Huey, for mentoring me and helping me with some of the organizational stuff. Helping me to stay on track was really important for me since I work better when I have a sense of direction. This may seem small, but receiving even a tiny bit of feedback helps a lot. I also wanted to thank Jakub BerÃ¡nek(one of the Rust people behind Rust GSoC 2024) for his work organizing and managing Rust GSoC. Good management is almost invisible: everything just goes according to plan. Still, it is something to appreciate. I am, in general, a bit of a messy and forgetful person, so I really appreciate that everything was going smoothly. Since I am talking about the organizational stuff, it would be a crime not to thank the Google GSoC team for making GSoC in the first place. To be quite honest, I assumed that the team behind GSoC had to be pretty sizable since everything worked like a well-oiled machine. Learning how small it is(2.5 people) was a big shock to me, and made me appreciate their work even more. I also want to express my gratitude towards the wider members of the Rust project, who helped me tremendously. It would be hard to list everyone who answered my question by name, but I would like to explicitly mention Bjorn3 , who answered quite a lot of them. One of the greatest things about GSoC was receiving feedback and help from a lot of different people. Overall, GSoC has been a blast, and I met a lot of truly amazing people there. Each project felt like something impactful, and all my interactions with other Rust GSoC contributors were amazing. Mething all of those people passionate about Rust gave me a lot of confidence in the future of the language. I am currently working on a Rust to .NET compiler, rustc_codegen_clr . To get it to work, I need to implement many Rust features using .NET APIs. One of such features is panicking and unwinding.

======>
https://old.reddit.com/r/rust/comments/1fmznnh/hyperion_10k_player_minecraft_game_engine/
-->>-->>
In March 2024   , I stumbled upon the    EVE Online 8825 player PvP World Record   . This seemed beatable, especially given the popularity of Minecraft.   

   Sadly, however, the current vanilla implementation of Minecraft stalls out at around a couple hundred players and is single-threaded.   

   Hence, Iâ€™ve spent months making Hyperion â€”    a highly performant Minecraft game engine built on top of       flecs   . Unlike    many other wonderful Rust Minecraft server initiatives   , our goal is not feature parity with vanilla Minecraft. Instead, we opt for a modular design, allowing us to implement only what is needed for each massive custom event (think like Hypixel).   

   https://i.redd.it/9gi74oeepeqd1.gif   

   With current performance, we estimate we can host ~50k concurrent players. We are in communication with several creators who want to use the project for their YouTube or Livestream content. If this sounds like something you would be interested in being involved in feel free to reach out.   

   GitHub:    https://github.com/andrewgazelka/hyperion   
Discord:    https://discord.gg/WKBuTXeBye   
   

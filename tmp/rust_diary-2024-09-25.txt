https://github.com/ALEX11BR/emscripten-functions/tree/main/examples/simple-game
-->>-->>
README.md A simple web-compatible game with Rust and SDL2 Web version demo This is a little game where you move an image across the screen with the arrow keys and change the background to black with Enter and to white with Space. It is meant to serve as a starting point in building SDL2 games in Rust with web support via emscripten using the emscripten-functions crate. Notes for developers Building For web builds we have a Makefile .
Just run make build-web or just make and you'll have the page with the game and everything needed in the out folder.
Make sure you have the Emscripten SDK in your PATH when compiling. Customizing the project name You'll need to change it in the following places: Makefile : change the PROJECT variable Cargo.toml : change the package.name src/main.rs : change the title parameter given to the window function Customizing the HTML game shell The default HTML shell shows the game on the entire page, with a little loading animation. You can change the shell.html file to your liking.
Keep in mind that you'll need to keep the final {{{ SCRIPT }}} thing as this is where the emscripten js import is placed by the Makefile rule. Images The game images are in the assets folder.
For other formats than PNG you need to add support for the ones you need in: build.rs - change the --use-port=sdl2_image:formats=png linker argument to add support for your desired formats src/main.rs - add the desired formats' flags to the parameter of sdl2::image::init For native (non-web) builds you'll need to run the executable in the folder where the assets are located.
This usually means copying the assets folder with the executable and the eventual dynamic libraries.
=====>
https://oneirical.github.io/3-getting-chased-around/
-->>-->>
Bevy Traditional Roguelike Quick-Start - 3. Establishing the Hunting Grounds 2024-09-23 :: tags: #bevy #rust #tutorial Cleaning Our Room Before continuing, it must be noted that the main.rs file is slowly reaching critical mass with its 161 lines of code. Before it swallows the Sun, it would be wise to divide it into multiple files, using Plugins . As an example, let's bundle up everything that has something to do with displaying things on screen into a single GraphicsPlugin . Create a new file in src/graphics.rs . Write within: // graphics.rs use bevy::prelude::*; // Note the imports from main.rs use crate::{Player, OrdDir, Position}; pub struct GraphicsPlugin ; impl Plugin for GraphicsPlugin { fn build (& self , app : & mut App) { app.init_resource::<SpriteSheetAtlas>(); app.add_systems(Startup, setup_camera); app.add_systems(Update, adjust_transforms); } } Then, add the resource and the two systems, as they appeared in Part 2 of the tutorial: // graphics.rs #[ derive (Resource)] pub struct SpriteSheetAtlas { // Note the pub! handle : Handle<TextureAtlasLayout>, } impl FromWorld for SpriteSheetAtlas { fn from_world ( world : & mut World) -> Self { let layout = TextureAtlasLayout::from_grid(UVec2::splat( 16 ), 8 , 1 , None, None); let mut texture_atlases = world .get_resource_mut::<Assets<TextureAtlasLayout>>() .unwrap(); Self { handle: texture_atlases.add(layout), } } } fn setup_camera ( mut commands : Commands) { commands.spawn(Camera2dBundle { transform: Transform::from_xyz( 0. , 0. , 0. ), ..default() }); } /// Each frame, adjust every entity's display location to be offset /// according to the player's location. fn adjust_transforms ( player : Query<&Position, With<Player>>, mut npcs : Query<(&Position, & mut Transform), Without<Player>>, ) { // There should only be one player on any given frame. let player_pos = player.get_single().expect( " 0 or 2+ players " ); // Get the player's position. let (px, py) = (player_pos.x, player_pos.y); // For each Position and Transform of each non-player creature... for (npc_pos, mut npc_tran) in npcs.iter_mut() { // Measure their offset distance from the player's location. let (off_x, off_y) = (npc_pos.x - px, npc_pos.y - py); // Adjust their visual position to match this offset. (npc_tran.translation.x, npc_tran.translation.y) = ( // Multiplied by the graphical size of a tile, which is 64x64. off_x as f32 * 4. * 16. , off_y as f32 * 4. * 16. , ); } } This can finally be connected to main.rs : // main.rs mod graphics ; // NEW! use graphics::GraphicsPlugin; // NEW! fn main () { App::new() .add_plugins(DefaultPlugins.set(ImagePlugin::default_nearest())) .add_plugins(GraphicsPlugin) // NEW! // Note that the following have been removed: // - SpriteSheetAtlas // - setup_camera // - adjust_transforms .add_systems(Startup, spawn_player) .add_systems(Startup, spawn_cage) .add_systems(Update, keyboard_input) .run(); } Note that this reorganization comes with the necessity of many import ( use ) statements. In the future of this tutorial, inter-file imports will no longer be represented in the code snippets. rust-analyzer offers auto-importing of unimported items as a code action, and compiler errors for this particular issue are clear and offer precise suggestions. Also remember to clean as you go, and remove unused imports marked by warnings. I have organized the rest of the Part 2 components, bundles, systems and resources in the following way: creature.rs ( No plugin! Only struct definitions. ) Player Creature input.rs keyboard_input map.rs Position spawn_player spawn_cage And, as it was only just done: graphics.rs SpriteSheetAtlas setup_camera adjust_transforms We will also add pub markers to the structs and enums moved over (but not the systems). As Component s and Resourcè s tend to travel around quite a bit, they will often need to be imported across other Plugin s. Not to worry, missing a pub will simply have the compiler complain a bit and provide a helpful error message to correct the issue, mentioning that "this struct is inaccessible". This leads to this main() function: // main.rs fn main () { App::new() .add_plugins(DefaultPlugins.set(ImagePlugin::default_nearest())) .add_plugins((GraphicsPlugin, MapPlugin, InputPlugin)) .run(); } Note the tuple in the second add_plugins̀ . Just as it was shown in Part 2 for commands.spawn() , many Bevy functions can take either a single item or a tuple of items as an argument! Compile everything with cargo run to make sure all is neat and proper, and to fix potential still-private or unimported structs/struct fields. If it works, you may notice strange black lines on the periphery of the walls: This can happen when working with a 2D spritesheet in Bevy. To fix it, disable Multi Sample Anti-aliasing: impl Plugin for GraphicsPlugin { fn build (& self , app : & mut App) { app.init_resource::<SpriteSheetAtlas>(); app.insert_resource(Msaa::Off); // NEW! app.add_systems(Startup, setup_camera); app.add_systems(Update, adjust_transforms); } } Much better. If you'd like to see how the fully reorganized code looks like, check in tutorial/source_code/3-getting-chased-around/3.1-reorganized . Detecting the Happening of Things - Events You may remember keyboard_input and how it adjusts the Player 's position: // input.rs // SNIP if input.pressed(KeyCode::KeyW) { player.y += 1 ; } // SNIP This is very weak programming! As the game expands, we might need to detect when the player steps on slippery goo or when it collides with another entity. We'll need to implement these checks on each possible direction to step in, have error-prone repeated code blocks, and end up with a towering heap of function arguments that looks like this: fn dispense_functions ( mut creatures : ParamSet<( Query<(&Transform, & mut Species, & mut SoulBreath, & mut AxiomEffects, & mut Animator<Transform>, & mut Position, Has<RealityAnchor>)>, Query<&Position>, Query<&Species>, Query<&SoulBreath>, Query<(&Position, &Transform), With<RealityAnchor>>, )>, mut plant : Query<& mut Plant>, faction : Query<&Faction>, check_wound : Query<Entity, With<Wounded>>, mut next_state : ResMut<NextState<TurnState>>, mut world_map : ResMut<WorldMap>, mut souls : Query<(& mut Animator<Transform>, &Transform, & mut TextureAtlasSprite, & mut Soul), Without<Position>>, ui_center : Res<CenterOfWheel>, time : Res<SoulRotationTimer>, mut events : EventWriter<LogMessage>, mut zoom : ResMut<ZoomInEffect>, mut commands : Commands, mut current_crea_display : ResMut<CurrentEntityInUI>, texture_atlas_handle : Res<SpriteSheetHandle>, ){ /* endless misery */ } Yes, this is a real function, from one of my old (and bad) Bevy projects. We wish to avoid this. Enter: Event s! This revolution will be neatly contained in a new plugin, EventPlugin , inside a new file, events.rs . It will serve as a repository of the "actions" being taken within our game. The player taking a step is one such action of interest. // events.rs pub struct EventPlugin ; impl Plugin for EventPlugin { fn build (& self , app : & mut App) { app.add_event::<PlayerStep>(); } } #[ derive (Event)] pub struct PlayerStep { pub direction : OrdDir, } Don't forget to link all this to main.rs . mod creature ; mod events ; // NEW! mod graphics ; mod input ; mod map ; use bevy::prelude::*; use events::EventPlugin; // NEW! use graphics::GraphicsPlugin; use input::InputPlugin; use map::MapPlugin; fn main () { App::new() .add_plugins(DefaultPlugins.set(ImagePlugin::default_nearest())) .add_plugins((EventPlugin, GraphicsPlugin, MapPlugin, InputPlugin)) // CHANGED .run(); } Note the new struct: OrdDir , short for "Ordinal Direction". This will be a very common enum throughout the game's code - so common, in fact, that I have opted to place it within ̀ main.rs̀̀̀ . This is personal preference and it could have very well been integrated into one of the plugins. // main.rs #[ derive (PartialEq, Eq, Copy, Clone, Debug)] pub enum OrdDir { Up, Right, Down, Left, } impl OrdDir { pub fn as_offset ( self ) -> ( i32 , i32 ) { let (x, y) = match self { OrdDir::Up => ( 0 , 1 ), OrdDir::Right => ( 1 , 0 ), OrdDir::Down => ( 0 , - 1 ), OrdDir::Left => (- 1 , 0 ), }; (x, y) } } And, at last, the very first ̀ Event -based system can be implemented: // events.rs fn player_step ( // Incoming events must be read with an EventReader. mut events : EventReader<PlayerStep>, // Fetch the Position of the Player. mut player : Query<& mut Position, With<Player>>, ) { // There should only be one player. let mut player_pos = player.get_single_mut().expect( " 0 or 2+ players " ); // Unpack the event queue - not that it will be very long in this case! for event in events.read() { // Calculate how to modify the player's Position from the OrdDir. let (off_x, off_y) = event.direction.as_offset(); // Change the player's position. player_pos.shift(off_x, off_y); } } Register it. // events.rs impl Plugin for EventPlugin { fn build (& self , app : & mut App) { app.add_event::<PlayerStep>(); app.add_systems(Update, player_step); // NEW! } } First, note the EventReader argument, which is a requirement to unpack the contents of received Event̀ s, which are getting produced by... nothing at the moment. An EventReader , of course, needs a companion EventWriter . This is how the previously unwieldy keyboard_input system can be reworked! // input.rs fn keyboard_input ( mut events : EventWriter<PlayerStep>, input : Res<ButtonInput<KeyCode>>, ) { if input.pressed(KeyCode::KeyW) { events.send(PlayerStep { direction: OrdDir::Up, }); } if input.pressed(KeyCode::KeyD) { events.send(PlayerStep { direction: OrdDir::Right, }); } if input.pressed(KeyCode::KeyA) { events.send(PlayerStep { direction: OrdDir::Left, }); } if input.pressed(KeyCode::KeyS) { events.send(PlayerStep { direction: OrdDir::Down, }); } } Instead of this system handling the player's motion - and being responsible for the implementation of all the subtleties that may imply, the heavy work is now all offshored to an Event specialized in handling this task! cargo ruǹ 's results should be fairly disappointing - as, from a non-developer perspective, nothing about the game has fundamentally changed - at least not our ability to phase through walls at lightspeed. However, our codebase will be much more extensible for the near future - not to mention that this Event is only the first of many. Enforcing Basic Physics - Collisions & The Map A wall should wall things. It's in the name. There are multiple ways to implement this - the simplest would be to query every single creature with a Position on the player's move, check if any of them occupies the destination tile, and abort the move if that's the case. Computers today are decently fast, but that is still a very naive implementation. The alternative is to keep a tidy phone book of everyone's location! Enter - the Map Resource. // map.rs /// The position of every creature, updated automatically. #[ derive (Resource)] pub struct Map { pub creatures : HashMap<Position, Entity>, } impl Map { /// Which creature stands on a certain tile? pub fn get_entity_at (& self , x : i32 , y : i32 ) -> Option<&Entity> { self .creatures.get(&Position::new(x, y)) } /// Is this tile passable? pub fn is_passable (& self , x : i32 , y : i32 ) -> bool { self .get_entity_at(x, y).is_none() } } It's a HashMap which contains only entries where a creature exists, which gives it the ability to fetch whoever is standing on, say, (27, 4) in record time with no ̀ Query or iterating over entities required! When importing the HashMap , I suggest using the use bevy::utils::HashMap instead of Rust's std implementation. The Bevy version bases itself off of hashbrown , which is weaker to flooding hacks but more performant - an interesting characteristic for game development, unless one is making the next CIA agent training simulator. Don't forget to register this new Resourcè . // map.rs pub struct MapPlugin ; impl Plugin for MapPlugin { fn build (& self , app : & mut App) { // NEW! app.insert_resource(Map { creatures: HashMap::new(), }); // End NEW. app.add_systems(Startup, spawn_player); app.add_systems(Startup, spawn_cage); } } It's now possible to test the waters before venturing into a new tile, thus avoiding any further phasing incidents. // events.rs fn player_step ( mut events : EventReader<PlayerStep>, mut player : Query<& mut Position, With<Player>>, map : Res<Map>, ) { let mut player_pos = player.get_single_mut().expect( " 0 or 2+ players " ); for event in events.read() { let (off_x, off_y) = event.direction.as_offset(); // REPLACES player_pos.shift(off_x, off_y) // Get the destination tile. let destination = Position::new(player_pos.x + off_x, player_pos.y + off_y); // Check if the destination tile is empty. if map.is_passable(destination.x, destination.y) { // If yes, authorize the move. player_pos.shift(off_x, off_y); } // End REPLACES. } } Don't cargo run just yet! Our ̀ Map is completely empty and unaware of the existence of walls. This can be fixed with a single new system. // map.rs /// Newly spawned creatures earn their place in the HashMap. fn register_creatures ( mut map : ResMut<Map>, // Any entity that has a Position that just got added to it - // currently only possible as a result of having just been spawned in. displaced_creatures : Query<(&Position, Entity), Added<Position>>, ) { for (position, entity) in displaced_creatures.iter() { // Insert the new creature in the Map. Position implements Copy, // so it can be dereferenced (*), but `.clone()` would have been // fine too. map.creatures.insert(*position, entity); } } The most unique part about this new system is the ̀ Added filter, which fetches only entities who have newly received the Position component and not been handled by this system yet. Right now, it means all newly created creatures will be processed by this system once, and then ignored afterwards. Register it. // map.rs pub struct MapPlugin ; impl Plugin for MapPlugin { fn build (& self , app : & mut App) { app.insert_resource(Map { creatures: HashMap::new(), }); app.add_systems(Startup, spawn_player); app.add_systems(Startup, spawn_cage); app.add_systems(Update, register_creatures); // NEW! } } Activate cargo run ... and the walls finally have tangibility! A Very Sticky Critter - The First NPC It's about time the Player got some company. Not a particularly affable one, I must admit, but we all start from somewhere. // map.rs, spawn_cage let cage = " ##########H......##.......##.......##.......##.......##.......##.......########## " ; Edit the wall placement string to include a (H)unter. Yes, this is messy - a proper map generator will be the topic of a future chapter. This Hunter also earns itself a separate sprite: // map.rs, spawn_cage let position = Position::new(idx as i32 % 9 , idx as i32 / 9 ); let index = match tile_char { ' # ' => 3 , ' H ' => 4 , // NEW! _ => continue , }; And the ability to be differentiated from walls, with a new Hunt component... // creature.rs #[ derive (Component)] pub struct Hunt ; ...added to any 'H' character in the initial spawn function. // map.rs, spawn_cage let mut creature = commands.spawn(Creature { // CHANGED - note the variable assignment position, sprite: SpriteBundle { texture: asset_server.load( " spritesheet.png " ), transform: Transform::from_scale(Vec3::new( 4. , 4. , 0. )), ..default() }, atlas: TextureAtlas { layout: atlas_layout.handle.clone(), index, }, }); if tile_char == ' H ' { creature.insert(Hunt); } cargo run , and our new companion is here. Excellent. Now, to give it motion of its own... The first problem is that motion, in our game, is currently only supported by player_step , which solely refers to the player character and nothing else. There should be a more generic Event , capable of controlling absolutely any creature to move around... // events.rs #[ derive (Event)] struct TeleportEntity { destination : Position, entity : Entity, } impl TeleportEntity { fn new ( entity : Entity, x : i32 , y : i32 ) -> Self { Self { destination: Position::new(x, y), entity, } } } Its matching system has a lot of similarity to player_step . // events.rs fn teleport_entity ( mut events : EventReader<TeleportEntity>, mut creature : Query<& mut Position>, map : Res<Map>, ) { for event in events.read() { let mut creature_position = creature // Get the Position of the Entity targeted by TeleportEntity. .get_mut(event.entity) .expect( " A TeleportEntity was given an invalid entity " ); // If motion is possible... if map.is_passable(event.destination.x, event.destination.y) { // ...move that Entity to TeleportEntity's destination tile. creature_position.update(event.destination.x, event.destination.y); } else { // Nothing here just yet, but this is where collisions between creatures // will be handled. continue ; } } } Don't forget to register all of this... // events.rs impl Plugin for EventPlugin { fn build (& self , app : & mut App) { app.add_event::<PlayerStep>(); app.add_event::<TeleportEntity>(); // NEW! app.add_systems(Update, player_step); app.add_systems(Update, teleport_entity); // NEW! } } ...and, of course, to actually use it in player_step so all entity motion of any kind is handled by this specialized system. // events.rs fn player_step ( mut events : EventReader<PlayerStep>, mut teleporter : EventWriter<TeleportEntity>, // NEW! // CHANGED, no longer needs mutable access, and also fetches the Entity component. player : Query<(Entity, &Position), With<Player>>, ) { // CHANGED, no longer needs mutable access, and also fetches the Entity component. let (player_entity, player_pos) = player.get_single().expect( " 0 or 2+ players " ); for event in events.read() { let (off_x, off_y) = event.direction.as_offset(); // CHANGED, Send the event to TeleportEntity instead of handling the motion directly. teleporter.send(TeleportEntity::new( player_entity, player_pos.x + off_x, player_pos.y + off_y, )); } } And there we go! player_step is now only an intermediate point leading to a central teleport_entity system, which can handle any and all creature motion. This means every creature will be on the same footing, with no repeated code! Just like when player_step was first added, cargo run on this will not change gameplay whatsoever. However, all this has finally allowed us to gift motion to our new Hunter. First, define a very naive "algorithm" to move towards a point on the map. Start with this helper function to calculate a distance between two points: // map.rs fn manhattan_distance ( a : Position, b : Position) -> i32 { (a.x - b.x).abs() + (a.y - b.y).abs() } And then, a way to find the best move among all four orthogonal options: // map.rs impl Map { // SNIP - all other impl Map functions /// Find all adjacent accessible tiles to start, and pick the one closest to end. pub fn best_manhattan_move (& self , start : Position, end : Position) -> Option<Position> { let mut options = [ Position::new(start.x, start.y + 1 ), Position::new(start.x, start.y - 1 ), Position::new(start.x + 1 , start.y), Position::new(start.x - 1 , start.y), ]; // Sort all candidate tiles by their distance to the `end` destination. options.sort_by(|& a , & b | manhattan_distance(a, end).cmp(&manhattan_distance(b, end))); options .iter() // Only keep either the destination or unblocked tiles. .filter(|& p | *p == end || self .is_passable(p.x, p.y)) // Remove the borrow. .copied() // Get the tile that manages to close the most distance to the destination. // If it exists, that is. Otherwise, this is just a None. .next() } } Finally, implement that Hunt implies chasing the player around. // events.rs fn player_step ( mut events : EventReader<PlayerStep>, mut teleporter : EventWriter<TeleportEntity>, player : Query<(Entity, &Position), With<Player>>, hunters : Query<(Entity, &Position), With<Hunt>>, // NEW! map : Res<Map>, // NEW! Bringing back the map, so "pathfinding" can be done. ) { let (player_entity, player_pos) = player.get_single().expect( " 0 or 2+ players " ); for event in events.read() { let (off_x, off_y) = event.direction.as_offset(); teleporter.send(TeleportEntity::new( player_entity, player_pos.x + off_x, player_pos.y + off_y, )); // NEW! for (hunter_entity, hunter_pos) in hunters.iter() { // Try to find a tile that gets the hunter closer to the player. if let Some(move_target) = map.best_manhattan_move(*hunter_pos, *player_pos) { // If it is found, cause another TeleportEntity event. teleporter.send(TeleportEntity { destination: move_target, entity: hunter_entity, }); } } // End NEW. } } cargo run , and let the hunt begin! There is only the slight issue that our Hunter is rather on the incorporeal side of things. Indeed, as it moves, the Map fails to update and the Hunter is still considered to have phantasmatically remained in its spawn location. Not to mention that the centre of the cage, where we spawned, is also mysteriously blocked by an invisible wall. There exists another filter like Added , named Changed , which triggers whenever a specified component is not only added for the first time, but also when an already existing instance is modified - such as in the case of moving a creature around. However, using it would be unwise. Here is why - the following happen in order: The user presses a button on their keyboard to move. PlayerStep is triggered. Two TeleportEntity are sent out. The Player's TeleportEntity happens first, moving the Player onto coordinates (2, 3). The Map is NOT updated yet, because it is located in a different system ( register_creatures ), and ̀ teleport_entity isn't done yet, as it has another event to get through. The Hunter's TeleportEntity happens, moving the Hunter onto coordinates (2, 3) too! This appears to be a legal move to the game, because the Map̀ hadn't been updated yet. teleport_entity is done, and register_creatures triggers, editing Map to "knock out" the Player and leave only the Hunter, while the Player is now off the Map and completely untargetable. To fix this, we need to modify the Map immediately after a creature moves. Leave register_creatures set to Added , and instead, modify teleport_entity : // events.rs fn teleport_entity ( mut events : EventReader<TeleportEntity>, mut creature : Query<& mut Position>, mut map : ResMut<Map>, // CHANGED, this needs mutability now. ) { for event in events.read() { let mut creature_position = creature .get_mut(event.entity) .expect( " A TeleportEntity was given an invalid entity " ); if map.is_passable(event.destination.x, event.destination.y) { map.move_creature(*creature_position, event.destination); // NEW! creature_position.update(event.destination.x, event.destination.y); } else { continue ; } } } map.move_creature is a new impl Map function. // map.rs impl Map { /// Move a pre-existing entity around the Map. pub fn move_creature (& mut self , old_pos : Position, new_pos : Position) { // As the entity already existed in the Map's records, remove it. let entity = self .creatures.remove(&old_pos).expect(&format!( " The map cannot move a nonexistent Entity from {:?} to {:?} . " , old_pos, new_pos )); self .creatures.insert(new_pos, entity); } } And with that, everything is going according to plan. The next chapter of this tutorial will introduce basic animation, as well as a cleaner way to generate the starting map, free of mega one-line "#####H....#####"̀ -style strings and match statements! ← Bevy Traditional Roguelike Quick-Start - 2. Tiles to Frolic Around In
======>
https://oneirical.github.io/2-tiles-to-run-around-in/
-->>-->>
Query<&Position, With<Player>> grants us access to all Entities with both Position and Player , and exposes their Position component for read-only access. Query<(&Position, &mut Transform), Without<Player>> grants us access to all Entities with Position and Transform , and which do not contain Player . The Position component is exposed in read-only mode, while the Transform component is exposed in read-write (mutable) mode.
======>
https://oneirical.github.io/1-drawing-the-player/
-->>-->>
Oneirical blog tags archive about me github Bevy Traditional Roguelike Quick-Start - 1. Drawing the Player Character 2024-09-16 :: tags: #bevy #rust #tutorial Traditional roguelikes are an ancient genre of games which earned the peak of their fame in the 20th century. They are the ancestors of modern indie roguelikes beloved by many such as Hades, The Binding of Isaac or Risk of Rain. What a "true roguelike" is has been the driving force of multiple Internet flamewars, but it almost always revolves around this list: The game takes place on a grid, like Chess. The game is turn-based: the player moves, then monsters do. The environment is randomized with procedural generation. When the player dies, the game restarts from scratch. Traditional roguelikes, despite their age, still live today. Major releases with a still active community today include: Caves of Qud Dungeon Crawl Stone Soup Cataclysm Dark Days Ahead Nethack There are multiple reasons why I think this genre is well suited for a beginner-level Bevy Quickstart guide: Traditionally terrible minimalistic graphics. No great artistic expectations will be forced on you! Infinitely extensible design that lends itself very easily to imagining new abilities, foes and challenges! An Event-centric, turn-based architecture to show off the power of Bevy's ECS. Fun! For nerds. This tutorial assumes: That you have never touched Bevy before. That you have some beginner Rust experience and know what "borrow checking" is. If you do not, I recommend going through the first half of the rustlings suite of interactive exercises to get you up to speed. No concurrency or dynamic dispatch Rust technowizardry will be required. Only the basics. The nature of ECS has been covered earlier in the Quick Start guide. Here is a metaphorical reminder: Entities - The actors on the stage. Components - The costumes and characters worn by the actors. Systems - The script of the play. Setting the Stage - The App Writing cargo new bevy-quick-start creates a Rust version of the evergreen Hello World program. It feels quite distant from anything resembling a game you can play in a contained window, except perhaps a prehistoric text adventure. Let's fix that by replacing the code in fn main : /// The entry point into the game, where everything begins. fn main () { App::new() .add_plugins(DefaultPlugins) .run(); } It would also be wise to import Bevy in a Bevy project. Place this on the very first line of main.rs : use bevy::prelude::*; The App is the window which contains the game, the theatre in which the actors will play. To ensure it starts off with all the basic Bevy features, the DefaultPlugins plugin must be tacked onto it. Running this code with cargo run will result in an empty, boring window. Progress! The First Denizen - The Player The player needs an avatar through which they will interact with the game world. In a traditional roguelike, this player will be a Creature - just like the foes and friends they will meet - susceptible to motion, health and other restrictions that come from having a physical body. In fact, our game will likely be populated by a lot of these Creatures. Let us define what that is: /// Common components relating to spawning a new Creature. #[ derive (Bundle)] struct Creature { sprite : SpriteBundle, } A ̀ Bundle is a "starter pack" to define which Components a certain Entity has. Right now, a Creature doesn't have much more beyond a sprite , which is its graphical representation on screen. And let us spawn the player: /// Spawn the player character. fn spawn_player ( // Bevy's Commands add, modify or remove Entities. mut commands : Commands, // A builtin Bevy resource that manages textures and other assets. asset_server : Res<AssetServer>, ) { // The spawn command summons a new Entity with the components specified within. commands.spawn(Creature { sprite: SpriteBundle { // What does the player look like? texture: asset_server.load( " otter.png " ), // Where is the player located? transform: Transform::from_xyz( 0. , 0. , 0. ), // Everything else should be default (for example, the player should be Visible) ..default() }, }); } At any moment while using Bevy, should one need to: Spawn a new Entity Delete an Entity Attach a new Component to an Entity Remove a Component from an Entity A mutable Commands argument will likely be required. This is the case here, as a new Entity is being spawned. Since we are also pulling a visual asset from a file into the game, the AssetServer is similarly required. It is a Resource , shown by the wrapping Res<_> , which is a Bevy type to be discovered later in this tutorial. We will also need to run this new function - or, in Bevy terms, System - by attaching it to the ̀ App : fn main () { App::new() .add_plugins(DefaultPlugins) .add_systems(Startup, spawn_player) // NEW! .run(); } Startup systems run once when booting up the app, then are never ran again. The player is thus spawned with the texture "otter.png" at the position (0.0, 0.0, 0.0). Note the Z-coordinate - in a 2D game, it is still useful, as it determines which Entities get to be drawn on top of each other. More on that later. Using cargo run on this will result in an error: ERROR bevy_asset::server: Path not found: your/path/here/bevy-quick-start/assets/otter.png Of course, we need to provide the image. In your game's root directory (where src and Cargo.toml currently exist), create a new directory named assets . If the App is the theatre, this is the costume storage - containing all the image data that Entities can take up as their visual sprite representations. Then, place any image of your choosing within it, renaming it so its filename matches the one used in the ̀ texture field in your code. Are you ready? cargo run ! This will result in the exciting sight of... absolutely nothing. Bird's Eye View - The Camera In Bevy, spawning entities left and right isn't very interesting if we are incapable of viewing them. We need to give ourselves eyes to see - a Camera: /// The camera, allowing Entities to be seen through the App window. fn setup_camera ( mut commands : Commands) { commands.spawn(Camera2dBundle { transform: Transform::from_xyz( 0. , 0. , 0. ), ..default() }); } Quite similar to spawning the player - after all, a Camera is just another Entity, just like anything in a Bevy game. The only difference is in its Components, which include Camera . Somewhere, deep in Bevy source code, there is a System fetching any Entity that contains Camera and doing some magic to make it display the screen's contents. We also need to welcome this new actor onto the stage by tying it to the App : fn main () { App::new() .add_plugins(DefaultPlugins) .add_systems(Startup, spawn_player) .add_systems(Startup, setup_camera) // NEW! .run(); } Running ̀ cargo run will now - assuming you had as much good taste as I did to pick a cute otter as my example image - display the player "character"! Bundling Them Up - The Spritesheet Unfortunately, we cannot build our epic roguelike dungeon out of otters. There will be different Creatures - foes, friends, or even walls - the latter behaving like other beings, letting them have health and be shoved around. They will be represented by different glyph-like sprites, and throwing around 100+ .png files in the ̀ assets directory is not my definition of careful organization. This is where the Spritesheet comes in - one image containing all game sprites next to each other, with a special Atlas to dictate which part of this image will be cropped out to represent each Creature. #[ derive (Resource)] struct SpriteSheetAtlas { handle : Handle<TextureAtlasLayout>, } This marks the creation of our first Resource . A Resource , in Bevy, is basically a global mutable variable. You can imagine it in the same way that you need to spend "wood resources" to build houses in a construction game - the wood is likely represented by a little icon on the side of the screen with a number next to it. It doesn't belong to a certain Entity - it belongs to the game as a whole. In this case, this Resource is an Atlas, mapping the spritesheet to divide it in tidy 16x16 squares. It will be accessible each time a new Creature is made, to assign it a specific region of that spritesheet. This Resource must be initialized: /// An initialization of the sprite sheet atlas, ran from `init_resource`. impl FromWorld for SpriteSheetAtlas { fn from_world ( world : & mut World) -> Self { // The spritesheet is composed of 16x16 squares. // There are 8 sprite columns, spread across 1 row. // There is no padding between the cells (None) and no offset (None) let layout = TextureAtlasLayout::from_grid(UVec2::splat( 16 ), 8 , 1 , None, None); // Grab the active atlases stored by Bevy. let mut texture_atlases = world .get_resource_mut::<Assets<TextureAtlasLayout>>() .unwrap(); // Add the new Atlas in Bevy's atlases and store it in the Resource. Self { handle: texture_atlases.add(layout), } } } Any Resource which implements FromWorld will, upon running init_resource , run the code contained in the impl block to create a new instance of it. The TextureAtlasLayout specifies the crop layout. Each sprite is 16x16 ( UVec2::splat(16) is a shortform of UVec2::new(16, 16) ), there are 8 sprite columns and 1 row, and there is no padding nor offset (None, None). This is stored into Bevy's atlases list, and is saved into the Resource for future usage. At last, this spritesheet must be properly welcomed through the App : fn main () { App::new() .add_plugins(DefaultPlugins) .init_resource::<SpriteSheetAtlas>() // NEW! .add_systems(Startup, setup_camera) .add_systems(Startup, spawn_player) .run(); } Now that we have our Atlas, we need to extend Creature. Not only do they have a sprite (the spritesheet), they also have a select crop of that spritesheet (the Atlas) to represent them: #[ derive (Bundle)] struct Creature { sprite : SpriteBundle, atlas : TextureAtlas, // NEW! } fn spawn_player ( mut commands : Commands, asset_server : Res<AssetServer>, atlas_layout : Res<SpriteSheetAtlas>, // NEW! ) { commands.spawn(Creature { sprite: SpriteBundle { // CHANGED to spritesheet.png. texture: asset_server.load( " spritesheet.png " ), // CHANGED to "from_scale" to make the // player character 64x64 for good visibility. transform: Transform::from_scale(Vec3::new( 4. , 4. , 0. )), ..default() }, // NEW! atlas: TextureAtlas { // The atlas is copied for usage by this Entity. layout: atlas_layout.handle.clone(), // We assign to it the first sprite - writing "2" would pick the third sprite, // and so on. index: 0 , }, // End NEW. }); } Running cargo run again will display the player glyph, cropped appropriately! But... Hmm, did I forget my glasses? fn main () { App::new() .add_plugins(DefaultPlugins.set(ImagePlugin::default_nearest())) // CHANGED! .init_resource::<SpriteSheetAtlas>() .add_systems(Startup, setup_camera) .add_systems(Startup, spawn_player) .run(); } Much better. Activating default_nearest in Bevy options like this helps render pixel art in pixel-perfect mode. Enter cargo run again to finish the first part of this tutorial! ← My Day at the Rust Conference - because let's grab all the most hardcore introverts in the world and put them in the same room, why not Bevy Traditional Roguelike Quick-Start - 2. Tiles to Frolic Around In → © 
    2024
 Julien Robert :: Theme: Terminimal by pawroman
======>
https://bevyengine.org/news/dream-job/
-->>-->>
Quasi-Hobby to Day Job Dream Jobs Are Still Jobs Learn or Drown Working Groups: Self-Organization and Empowerment Technical Future: UI That Doesn't Suck Product Future: Beyond Rust Gamedev I landed my dream job making a Rust game engine. Now what? Posted on September 10, 2024 by Alice I. Cecile ( @alice-i-cecile ) So I landed my dream job. Full-time open source building a game engine in Rust. Absolutely no meetings. What more could you ask for?
With a very unconventional background (plant ecology, self-taught programming and years of disability), it's flatly astonishing to be supported by such an awesome community: surrounded and empowered by experts and beginners alike making incredible things together. But now what? Bevy's fourth birthday has rolled around, and @cart has put out a call for others to reflect on the year behind us, and plan for the year ahead.
How have my first few months of Serious Employment at the Bevy Foundation gone, and what am I cooking up? Quasi-Hobby to Day Job # For years before I started working at the Bevy Foundation, I've helped out around Bevy: writing docs, triaging issues, reviewing PRs, designing complex systems and polishing off rough corners. I'd pitch in where I could, tackling the million small tasks in moments between working on side projects, the occasional bits of consulting and making sure life at home was running smoothly.
And over the years that really added up!
Reading, thinking, communicating: I've never been the sort to write reams of code for specific subsystems, or to dive into the most obscure bugs.
But if I listened to people talk about what they needed, chewed on it for a bit, and then passed it on to the people who wanted to help, I found I could be remarkably effective! It was a good groove.
But then all of a sudden, working on Bevy was my day job!
That means 40 hours a week, 8 hours a day, 9-5 Monday to Friday in the office, right?
Well, okay, sure, no one cares when I work. Or exactly how much I work per day. And we don't even have an office!
But surely, that was the platonic ideal of what Real Work should look like, and I should be aspiring to it, even if it would forever be out of reach. Actually no, not so much.
At first, I tried to stick to this (entirely of my own volition!): Monday to Friday, 8 hours a day, strict start and stopping time.
I focused on big initiatives, tried to code more, and made sure I wasn't working outside of the allowed time blocks. It made me miserable .
Pushing myself to work for long blocks at a time was physically and mentally demanding.
I had to actively force myself not to triage that issue, leave that comment, or tackle that PR.
And conversations across our team of dozens of globally-distributed contributors simply didn't have the decency to stay nicely confined to my 9-5 schedule! So, how did I square the circle?
The time and freedom to tackle bigger projects was a huge advantage, but all those little tasks still mattered!
And even though moving to a more flexible schedule was probably healthy for me,
I needed to make sure that work didn't consume my whole life. My solution: focus tasks .
Every work day (but not days off!), I would pick a single task to focus on.
Writing a piece of documentation, reviewing a complex PR, adding a feature, refactoring a complex bit of code, preparing a design doc, running the weekly Merge Train...
I would focus on that when I was feeling well, get done what I could, and as soon as my focus broke or the task was completed, that was it. I was "done work" for the day. To complement this, the strict 9-5 M-F schedule would be relaxed. I could work whenever I felt like it (turns out, for me, that's quite a bit):
tackling the million little things that help keep Bevy ticking along.
But there would be no obligation : no sense that I must do these things, or must do them in a timely fashion.
I could be there for the impromptu late night design chats, but still disconnect during date night because, after all, I finished work hours ago. This balance has worked really well for me: letting me drive forward larger initiatives ( bevy_color ! A huge leafwing-input-manager refactor! The 0.14 release!) without falling behind on the flood of notifications.
Would I recommend you use it? Probably not! It requires a degree of flexibility that many organizations won't afford you, you need to be driven and self-motivated and frankly, the work-life boundary is far blurrier than most people can live with.
But for someone working full-time in open source? Absolutely! Dream Jobs Are Still Jobs # So, having worked at the Bevy Foundation for a few months: is this really my dream job? How does it live up to my expectations?
Despite my boundless idealism, working for a non-profit you care about is not a panacea: if the working conditions suck,
the fact that you're Doing Good won't get you through the day. On the bright side, I have: a mission I believe in a comfortable living (my latest budget says $84k CAD pre-tax) the opportunity to meet and learn from incredible people: within Bevy, Rust and game dev more broadly a work-from-home arrangement with incredible flexibility huge levels of agency over what I work on generous European-style vacation and sick-leave policies But it's not perfect, that's all balanced out by: a remarkably public role, where everything I do from day-to-day is visible an incredibly small team of non-volunteers that serve as a backstop for all of the needful but tedious things that need to be done fuzzy work-life boundaries a salary that is much lower than what I could be making existential dread caused by relying entirely on generous donors to keep both myself and the project I love afloat Honestly, it's a lot like founding a startup.
It's just instead of having a small-but-nonzero chance of becoming wealthier than anyone ever needs to be,
I have a chance to change an industry for the better and help people make cool things! Learn or Drown # The most striking thing about it though, is the extent to which I have to keep learning and growing.
Rust? ECS? Technical writing? Project management? Input management? Community management? I can't get complacent and only work within my comfort zone of skillsets that I've mastered.
There's always a new stalled-out work area that needs leadership, brilliant but complex PRs to review, and thorny problems to cut through.
Even if my natural proclivities are focused on design and communication, I need to be fluent in every single area needed to make games.
Experts are great, but keeping things moving along means I have to be able to understand what they're saying and integrate it into a broader project context. Over the next year, I hope to get comfortable with basic custom rendering, muck about with Bevy's asset solution and learn about reactive UI in earnest.
Should be fun, and it might even drive some progress on my own games. Working Groups: Self-Organization and Empowerment # I've long believed that building systems and altering incentives is the best way to fix problems for good.
When I first started working at Bevy, we had a pair of twin problems: contributors were frustrated by the lack of direction and reviews for complex but valuable initiatives, while maintainers were out of bandwidth and frustrated with efforts that petered out as individual contributors got busy. Enter working groups : scoped, collaborative efforts to define, explore and tackle complex problems.
I've been incredibly pleased to see how they've turned out: an emphasis on clear, well-motivated designs a clear list of things that we're actively tackling a space to focus community efforts towards a shared goal a ready-made source of reviewers for complex efforts a mechanism for ambitious contributors to build consensus and radically change things The bevy_color working group was a wonderful bubbling hub of activity that tackled all of the gnarly corners of writing a color library from scratch,
while the 0.14 release working group really helped take the pressure off of Cart and I, even if it was a slog. I'm really excited to see what the open groups (relations, text, audio, picking, contributing guide, scenes, render graph and curves) put together over the next year!
I'll stir the pot periodically to keep things moving along smoothly, but overall I'm delighted by how well this experiment in self-organization has gone. Technical Future: UI That Doesn't Suck # Right now, Bevy has a critical technical limitation: our UI solution sucks.
While of course you could argue that all UI solutions suck, bevy_ui is remarkably underbaked.
There's too much boilerplate, there are virtually no premade widgets, and most of the hard problems are handed off to the user. While things have improved substantially since Bevy's initial release, bevy_ui operates at the same level as HTML: just raw data structures to be manipulated.
And for most of our users, that simply isn't enough!
Bevy's an incredible choice for CAD software, or complex simulation games, or dev tooling (like the fabled editor), except that building UIs takes too long and polishing them is nearly impossible. While I've shared my own vision for what bevy_ui could be , I trust @cart (and @viridia, @StarArawn, @UkoeHB and @SanderMertens) to figure out most of the fancy incremental, reactive, data-driven bits.
Instead, I want to clean up all the low-hanging fruit that drags our UI solution down, no matter what solution they land on. Over the course of the next year, I want to: swap to cosmic_text , and properly support non-Latin character sets make it easier to reason about fonts and how they're related support basic localization and markup-style rich text port bevy_ui to a polished picking paradigm make sure that simple alternatives to flexbox layout can be used for those who prefer it add focus management, making it easy to navigate UI with keyboards, gamepads and screen readers upstream leafwing-input-manager , giving Bevy first-party support for keybindings for both UI and gameplay ship a modest collection of functional standard widgets: radio buttons, text input, sliders and more write examples that cover real use cases: splash screens, settings menus, drag-and-drop inventories, UI mockups for various game genres... None of these things will radically change how bevy_ui works, but taken together, should lead to a night-and-day difference in the experience for both devs and end users. Product Future: Beyond Rust Gamedev # In both Rust-focused and gamedev-focused spaces, Bevy is often defined by the fact that it's written in Rust.
But thinking about game engines in this way is a trap, both for Bevy and for the ecosystem as a whole. Overwhelmingly, professional game developers don't decide that they're going to use Rust, look at the options within the set of Rust game engines and then choose one.
While hobbyists and those seeking to learn Rust might choose an engine that way (Rust is delightful and making games is a great way to learn),
it's an ineffective way to make business-critical decisions! Instead, they look at the full set of engines and what they have to offer: Unity, Unreal, Godot and dozens more.
Every team and every project has different needs and idiosyncratic preferences: there will never be an ur-engine that others are simply wrong for not choosing.
But if you want to compete within a crowded space, you need to both carve out and communicate a niche: a set of things that you're uniquely good at. Simply being the best, most popular, or most featureful game engine in Rust isn't enough, and frankly, barely matters at all.
Rust is fantastic, and I think in the long-run it'll be a major advantage, but right now, the game industry perception is that it's immature and slow to develop games using it. To survive as anything beyond a hobby engine, you need to attract commercial teams building serious games, built on a budget with mixed teams of programmers, artists and designers.
While you don't need to be better than the big kids in every way, you need to be better than them in some ways, and the rational choice for some set of teams needs to be to pick your engine. While we're not the rational choice yet for most projects and teams, I want to set us up for real success in the future.
In the coming year, I want to network more with gamedevs outside of Rust (say hi!), learn more about the workflows that real indie teams use and challenges that they face.
While the small improvements trickle in, one PR at a time, I want to set my eyes on the horizon, understand what talented small teams are looking for out of their next engine, and make sure we're building towards those goals. No one is writing the next Call of Duty in Bevy (yet!): the requirements around rendering, tooling, training, console support and risk are way too strict.
But what would it take to convince teams to write the next Factorio, Terraria, Slay the Spire, Hollow Knight or Hades in Bevy?
Time to find out! import { enable_image_compare } from '/components.js';

  document.addEventListener("DOMContentLoaded", function () {
    enable_image_compare();
  });
======>
https://oneirical.github.io/bevyrage/
-->>-->>
Bashing Bevy To Bait Internet Strangers Into Improving My Code 2024-03-27 :: tags: #bevy #rust #writeups "Online, the best way to obtain information is not to ask a question, but to state incorrect information and wait for someone to correct you." - Common Internet Wisdom It is said that one can judge the nerdiness level of a programming language based off the ratio of games-to-game-engines it has. Considering that - without looking anything off and purely off the top of my head - I am incapable of naming a single published Rust game, but can name at least 4 Rust game engines, (Bevy, Macroquad, ggez, Fyrox) I'd say we're dealing with a particularly severe case Terminal Technowizardry Syndrome. Like many brave dungeoneers before me, I have ventured off into the damp caves of Rust game development in a quest to right these wrongs. And just like all those aforementioned adventurers, I am failing epically. However, instead of being reasonable and recognizing my personal shortcomings, I will now proceed to become the protagonist of the adage "The shoddy craftsman blames their tools." Background Bevy is currently the most popular Rust game engine out there. In fact, it is so popular that non-Rust programmers might have actually heard of it, which is a feat of its own. As it is designed by Rust programmers, it is sculpted in the image of the substance which composes it - that means, it is extremely opinionated and devoid of any user interface. Based on the ECS (Entity, Component, Systems) design style, it adopts a philosophy that banishes feeble concepts such as "node trees" or "inheritance" back into the OOP abyss where they belong. "I don't think ECS is a new concept... like a lot of current trends it's making the old ways of doing things sound new again. It does do some exciting stuff... but it's ultimately just structs and composition, like how we used to do things - but adapted for the modern Unity Node Tree palette." - Evie, my most highly esteemed and very opinionated mentor Because looking at the people currently raking in boatloads of money and deciding to do things completely differently is cool, Bevy stretches ECS to the extreme, making even UI elements be ECS entities and forcing all code to run inside Systems. I have completed 2 small projects with Bevy, both open source: Pleroma & Kenoma , a 48 hour game jam submission The Tango Problem , an experiment with neuroevolutive genetic algorithms (holy buzzwords!) And, I am currently working on my larger scale forever-project: The Games Foxes Play , don't ask me what it's about, even I do not know, but you can see what it looks like . I will now proceed to bash what I believe are currently Bevy's greatest shortcomings, with the highly devious plan of getting Internet strangers to go " You absolute buffoon! How can you forget to use [HIGHLY ESSENTIAL BEVY FEATURE I NEVER HEARD OF] to save yourself hours of time? You dunce! You fluff-bloated addlepated simpleton!" Because finding those things myself can be... actually, that just brings me to my first main point. Bevy Contributors Are Way Too Productive I was only just finishing up updating my game to match Bevy 0.12's new conventions (December 2023) when they released Bevy 0.13 (February 2024). I must now once again comb through my code to knock out all now-obsolete conventions, such as the TextureAtlasSprite s liberally scattered throughout my code. Bevy needs to be hyped up to get people working on it, I fully understand that. But it is not a complete project by any means. Working with it feels like constantly reaching for a tool in your toolbox which you expect to be there, but instead, your hand simply passes through the case and enters a wormhole reaching into a Draft pull request on their GitHub. Tutorials become full of incorrect and outdated information in a matter of months - and as a result, people are not very motivated to actually make them. A significant portion of Bevy-related knowledge is found on YouTube, which is, in my opinion, NOT a good resource when it comes to fetching a quick answer like "how do I schedule this System to only run in this specific GameState". Bevy programmers probably already know this, but beyond the official docs.rs Bevy documentation, these two resources are highly valuable and tend to be updated... occasionally. Tainted Coders Bevy Cheatbook In addition to this, a large portion of Bevy development is fetching third-party crates to complement your Bevy development experience. For instance, I make extensive use of the Bevy Tweening crate in my own projects. I am fully dependent on their maintainers to keep up the pace with new Bevy releases - if a cosmic ray vaporizes every atom of their bodies in an unfortunate astral accident, I will be forced to update their libraries myself to keep my game running with the latest Bevy version. Bevy Tweening maintainers are very active right now, but that might not be the case for the dozen of external Bevy crates a game developer might see themselves using for a larger scale project. Bevy Types Expand In Mass Until They Blot Out The Sun Hmm, yes, dear waiter, I believe I'll be getting a ParamSet<(Query<(&Transform, &mut Species, &mut SoulBreath, &mut AxiomEffects, &mut Animator<Transform>, &mut Position, Has<RealityAnchor>)>, Query<&Position>, Query<&Species>, Query<&SoulBreath>, Query<(&Position, &Transform), With<RealityAnchor>>)> for breakfast, por favor . Keep the change. Bevy Queries are black magic. Given thousands of entities with multiple components, they will somehow fetch every single one that possess only the ones you want in record time, once every frame. This somehow does not turn my 8 GB of RAM computer into a localized micro-Sun, and is quite efficient. I probably should devote some time to understanding at least 10% of the process of how this is done. However, this is still Rust, and as the commandments go, Thou shall not access a mutable and an immutable reference to the same object simultaneously . This causes issues. One of the Systems in The Games Foxes Play is tracking the spells cast by creatures and executing their effects. It looks roughly like this: /* We obtain the entity on which the spell is getting cast, what the function of the spell is, and what info we can get from the creature that casted it. */ let (entity, function, mut info) = world_map.targeted_axioms.pop().unwrap(); /* We grab a bunch of mutable fields and get ready to modify the targeted creature's data. */ if let Ok((transform_source, mut species, mut breath, mut effects, mut anim, mut pos, is_player)) = creatures.p0().get_mut(entity.to_owned()) { match function { Function::Teleport { x, y } => { //SNIP // The creature has been teleported, // edit the "x" and "y" fields on its Position component. (pos.x, pos.y) = (x, y); //SNIP } Function::ApplyEffect { effect } => { //SNIP // The creature has received a status effect, // edit the "status" field on its AxiomEffects component. effects.status.push(effect); //SNIP }, // LOTS, and I mean LOTS, of other spell effects As you can see, I require accessing all those fields to manage all these possible spell effects. That is the Query<(&Transform, &mut Species, &mut SoulBreath, &mut AxiomEffects, &mut Animator<Transform>, &mut Position, Has<RealityAnchor>)> part of the giga-type I posted at the summit of this chapter. That is creature.p0() . But, let's say that we want to only grab an entity's Position or Species component. No modifications, no mutability, we simply want to know where a creature is currently located in terms of "x" and "y" coordinates, or what species it belongs to. Function::Collide { with } => { // "with" is the entity being collided with. let coll_species = creatures.p2().get(with).unwrap().clone(); let coll_pos = creatures.p1().get(with).map(| e | (e.x, e.y)).unwrap(); We cannot reuse creatures.p0() as it is currently in use by get_mut . Its ownership is unavailable, and creating a new immutable reference to Position while it is currently mutably borrowed by creatures.p0() (in order to mutate coordinates to fulfill Teleport) is impossible. We must use Bevy's ParamSet type, designed to handle these conflicts. Even in a situation where I could re-use creatures , I wouldn't necessarily need all those fields but would still need to call every single one, resulting in tons of unused variables: let stem_block = if let Ok((entity, _queue, _species, _effects, _breath, pos, _is_player)) = creatures.get(*segment) { And that is the source of this extreme bloat. This entire spell handling system thus has the following function fields: fn dispense_functions ( mut creatures : ParamSet<( Query<(&Transform, & mut Species, & mut SoulBreath, & mut AxiomEffects, & mut Animator<Transform>, & mut Position, Has<RealityAnchor>)>, Query<&Position>, Query<&Species>, Query<&SoulBreath>, Query<(&Position, &Transform), With<RealityAnchor>>, )>, mut plant : Query<& mut Plant>, faction : Query<&Faction>, check_wound : Query<Entity, With<Wounded>>, mut next_state : ResMut<NextState<TurnState>>, mut world_map : ResMut<WorldMap>, mut souls : Query<(& mut Animator<Transform>, &Transform, & mut TextureAtlasSprite, & mut Soul), Without<Position>>, ui_center : Res<CenterOfWheel>, time : Res<SoulRotationTimer>, mut events : EventWriter<LogMessage>, mut zoom : ResMut<ZoomInEffect>, mut commands : Commands, mut current_crea_display : ResMut<CurrentEntityInUI>, texture_atlas_handle : Res<SpriteSheetHandle>, ){ Rust's Clippy (an assistant that tries to improve your code) is NOT a fan. Not only is this massively bulky, this forces me to keep my code contained inside this single system. It is very hard to outsource work to other functions, because passing around data is a nightmare. I need to unpack components into relevant fields, call the function, get an output, and then mutate the fields with that output, back inside the giga-system. For example, if I want to pass around Position to do some calculations and mutate it, I can't just have a function that accepts &mut Position. The actual Bevy type is Mut<'_, Position> . It needs to be turned into fields "x" and "y". Only then can I pass those numbers to a "calculate_pathfinding" function, which returns numbers, and allows me to edit Bevy Components back inside the System. As a result, that dispense_functions System is currently at 594 lines of code, and will keep growing until it swallows the universe. I find it quite peculiar how Bevy demos focus very highly on presentation and little on gameplay. Art is obviously a huge part of game development, but reading changelogs show there is a bias towards talking about the latest shader and high performance wgpu computing improvements. Spawning a transparent 3D glass sphere in Bevy that distorts your vision when you gaze through is now well-supported. But games in the vein of "Dwarf Fortress" or "Zachtronics" with a large quantity of interactions feel much, much harder to do in Bevy due to how much data needs to be passed around. Bevy Is Not "Rusteic" "Getting it to compile will be difficult. But when it does compile, it will usually do everything you expected it to." - Rust Cultists I am proud to say I have never once used a classical runtime debugger with breakpoints in Rust. I never felt like I needed one. The compiler (and its trusty watchman Rust-Analyzer) judge my every move, like they are a team of senior engineers with decades of experience hovering over my shoulders and slowly drooling on the floor, waiting for the moment I make a mistake to bark out at me. "THAT VARIABLE DOESN'T LIVE LONG ENOUGH!!" As a former JavaScript plebeian who has only been semi-recently illuminated by the suspiciously pastel pink, white and blue radiance of Rust developers, NOT having to sit in my web console debugger for hours pushing some lovingly crafted [object Object] or undefined is a blessing. But Bevy, despite being built in Rust, does not offer such guarantees, and I find myself constantly reaching for those dbg! macros I thought I could mostly forget. There is the lack of warnings. In the previous chapter, I presented some particularly bloated Queries - well, I occasionally find one with some unneeded Without filters. The compiler, of course, is blind to such things. There is also the grave matter of System Scheduling. You see, without a precise order being given to them, Bevy systems run in complete chaos, and there is no guarantee that they will always continue their execution in the same order . It took me a little while to figure this out, and I now carefully specify game states to ensure Systems are running when they should: impl Plugin for TurnPlugin { fn build (& self , app : & mut App) { app.add_systems(Update, calculate_actions .run_if(in_state(TurnState::CalculatingResponse))); app.add_systems(Update, execute_turn .run_if(in_state(TurnState::ExecutingTurn))); app.add_systems(Update, dispense_functions .run_if(in_state(TurnState::DispensingFunctions))); app.add_systems(Update, unpack_animations .run_if(in_state(TurnState::UnpackingAnimation))); app.add_systems(Update, fade_effects); app.insert_resource(TurnCount{turns: 0 }); } } However, unscheduled systems can be very stealthy in how they induce bugs. A single oversight can mean your game works correctly 99% of the time, then suddenly has unexpected behaviour in a very specific situation because a System outsped another when it was least predictable. Another feature is the extremely bloated standard "Bevy library". Yes, Bevy is divided into multiple sub-crates (like "bevy-ui"), but most developers will simply import the whole thing. This makes compilation times quite severe (my 2016 glorified calculator laptop is incapable of even finishing the compilation process). It is possible to activate "dynamic linking" for a mild speedup: bevy = { version = "0.13", features = ["dynamic_linking"]} but, you'll need to REMEMBER to remove this when releasing your game, or you'll need to bundle some obscure module libbevy_dylib alongside your game to ensure it works at all. Fun. Yes, it's just 8-12 seconds, but when trying to design an UI and reloading every so often to see how it looks, it adds up. My Rust mentor is developing her own Rust game in SDL, and compiling that is more in the realm of a single second. There is an ocean of features, structs and functions in Bevy you won't ever use if you are merely making a 2D game. It is meant to be a general purpose game engine and a competitor to the likes of Unity, and it shows. Conclusion Bevy is an amazing piece of technology that isn't ready yet. I often end up feeling like I don't actually "own" every part of my game when using Bevy, which is what makes Rust so fun for me. When using Generic JavaScript Framework #28375, I don't feel like a programmer, I feel like a clueless child who has found black boxes of ancient alien technology and a tube of superglue to stitch them together. This is naturally a requirement in some giga-enterprise codebase - you can't understand the whole thing top to bottom - but when it comes to a personal gamedev project, I'd prefer having at least an idea of what is going on behind the scenes. When reading Bevy changelogs, I constantly think "if only that feature was around when I was developing THAT system in my game!" Bevy will be a great thing eventually that will challenge the status quo of the gamedev sphere. But, in my opinion, that time has not yet come. If you enjoyed this writeup, feel free to contact me! I am currently looking for: Summer internships Full time positions after April 2025 People to join my Northsec team - no large proficiency in security required, but the drive to learn about technology is mandatory Discord: oneirical Email: julien-robert@videotron.ca ← Proper Work-Life Balance In The Packet Factory - First Place in Operating Systems at CS Games It Works, And I Will Touch It →
======>
https://github.com/ventengine/Vent-Engine
-->>-->>
Repository files navigation README Code of conduct Apache-2.0 license Vent-Engine A game engine written in Rust with the goal to be very fast & user-friendly 🏆 Goals Built in Rust: This engine leverages the power of Rust and avoids external language bindings as much as possible. Performance Optimization: Vulkan is used for top-tier performance through native APIs. User-Friendly Design: The engine prioritizes ease of use. Cross-Platform Support: One goal of the engine is to support various platforms ( Platforms ). 🏗 Current Status Vent-Engine is currently in heavy development, Here is how it currently looks: (27.07.2024) How to run? This section explains how to compile and run the Vent Engine from source code. Since it's under heavy development, there are currently no pre-built releases available. Prerequisites: Rust compiler : Download and install Rust from the official website: https://www.rust-lang.org/tools/install Vulkan-compatible GPU The Vent Engine utilizes Vulkan for graphics rendering. You'll need a graphics card that supports Vulkan Steps: Clone the repository: git clone https://github.com/ventengine/Vent-Engine.git Compile & Run: cargo run --bin vent-runtime How to contribute? Contributions are welcome in any way, shape, or form. See Contributing to know how you can get started. 🎮 Platforms Vent-Engine Platform Support: Platform Runtime Editor Windows ✅️ ❓ MacOS ❓ ❓ Linux ✅️ ❓ Redox ❓ ❓ VR ❓ ❌ Android ❓ ❌ IOS ❓ ❌ WASM ❓ ❌ ✅: Works as intended ❌ Will not be Supported 😬: Mostly works but Unstable ❓: Unknown status 📝 License Vent-Engine uses the Apache 2.0 License
======>
https://github.com/monadbobo/skiplist-rust
-->>-->>
Repository files navigation README Rust SkipList A Rust implementation of the SkipList data structure, inspired by LevelDB's SkipList. This project provides a
concurrent, lock-free SkipList implementation that can be used for efficient key-value storage and retrieval. Features Lock-free concurrent operations Efficient insertion and lookup Iterator support for traversal Configurable maximum height and branching factor Written in safe Rust with minimal unsafe code Usage Add this to your Cargo.toml : [ dependencies ] skiplist-rust = " 0.1.0 " Then you can use the SkipList in your Rust code: use skiplist_rust :: SkipList ; fn main ( ) { let mut list = SkipList :: new ( ) ; // Insert some values list . insert ( 5 ) ; list . insert ( 2 ) ; list . insert ( 8 ) ; // Check if a value exists assert ! ( list.contains ( & 5 ) ) ; assert ! ( !list.contains ( & 3 ) ) ; // Iterate over the list let mut iter = SkipListIterator :: new ( & list ) ; while iter . valid ( ) { println ! ( "Key: {}" , iter.key ( ) ) ; iter . next ( ) ; } } API SkipList<K> new() -> SkipList<K> : Create a new SkipList insert(key: K) : Insert a key into the SkipList contains(&key: &K) -> bool : Check if a key exists in the SkipList iter(&self) -> SkipListIterator<K> : Get an iterator over the SkipList SkipListIterator<K> new(list: &SkipList<K>) -> SkipListIterator<K> : Create a new iterator over a SkipList valid(&self) -> bool : Check if the iterator is pointing to a valid node key(&self) -> &K : Get the key of the current node next(&mut self) : Move to the next node prev(&mut self) : Move to the previous node seek(&mut self, target: &K) : Seek to the first node with a key >= target seek_to_first(&mut self) : Seek to the first node seek_to_last(&mut self) : Seek to the last node Performance This implementation aims to provide similar performance characteristics to LevelDB's SkipList. It uses atomic operations
for concurrent access and a similar probabilistic balancing strategy. Contributing Contributions are welcome! Please feel free to submit a Pull Request. License This project is licensed under the MIT License. Acknowledgments Inspired by LevelDB's SkipList implementation Built with Rust's powerful type system and memory safety guarantees
======>
https://old.reddit.com/r/rust/comments/1fplvv0/a_simiple_minimal_working_sdl2_rust_game_with/
-->>-->>
https://github.com/ALEX11BR/emscripten-functions/tree/main/examples/simple-game   

   Part of my    emscripten-functions    project that provides emscripten system functions bindings.   

   Made to give a starting point to those that seek to use rust for building web games with SDL and maybe other C libraries that don't work well in wasm32-unknown-unknown.   
   

======>
https://old.reddit.com/r/rust/comments/1fpn1xs/how_can_i_clone_one_variable_multiple_times_with/
-->>-->>
Here is the code that is having the similar issue :   

   use std::{cell::RefCell, rc::Rc};

fn main() {
    let data = Rc::new(RefCell::new(100));

    let temp1 = Rc::clone(&data);
    let temp2 = Rc::clone(&data);

    println!("Data before mutation : {}", temp1.borrow());
    println!("Data brfore mutation : {}", temp2.borrow());

    {
        let mut ref_data = temp1.borrow_mut();
        *ref_data += 100;
    }
    println!("Mutation performed");

    println!("Data after mutation : {}", temp1.borrow());
    println!("Data after mutation : {}", temp2.borrow());
}
   

   In this code, the mutation changes both the cloned variables. The initial value is 100 but I want to make one become 200 whereas the other will still be 100. I know that the integer 100 is being held as the data source in this case but I want to have a different way to manipulate the thing and only make changes on the clones and not on the initial data source.   
   

======>
https://old.reddit.com/r/rust/comments/1fpcphn/how_to_improve_my_understanding_on_async_rust_and/
-->>-->>
I have gone through the books like async-book, atomics and locks but I still feel not very confident in those topics.    
   

======>
https://lwn.net/SubscriberLink/991062/b0df468b40b21f5d/
-->>-->>
Committing to Rust in the kernel [LWN subscriber-only content] By Jonathan Corbet September 24, 2024 Maintainers Summit The project to enable the writing of kernel code in Rust has been underway
for several years, and each kernel release includes more Rust code.  Even
so, some developers have expressed frustration at the time it takes to get
new functionality merged, and an air of uncertainty still hangs over
the project.  At the 2024 Maintainers Summit, Miguel Ojeda led a discussion
on the status of Rust in the kernel and whether the time had come to stop
considering it an experimental project.  There were not answers to all of the
questions, but it seems clear that Rust in the kernel will continue
steaming ahead. Ojeda started with the topic of the flexibility needed from the kernel's
subsystem maintainers.  Two years ago, before the initial Rust support was
pulled into the kernel, he had requested that flexibility because there
would be the need to change some core APIs at times to fit Rust code in.
The need for that flexibility is being felt now, he said. There are some clear differences in the expectations around Rust in the
kernel, he continued.  He has read through thousands of comments and emails
on recent events, and has seen a wide range of opinions on the state of the
Rust-for-Linux project and where it is headed.  It would be a good thing to
converge on a common understanding of what the goals are.  People and
companies want to invest in Rust for the kernel, but they are unsure about
its future. Jason Gunthorpe said that he, like many other kernel developers, has not
participated in this work so far.  The project was intended to demonstrate
that Rust is suitable for kernel usage; he is waiting for the decision on
the outcome.  Dave Airlie said that the experiment is not complete, but Greg
Kroah-Hartman said that it is clear at this point that Rust in the kernel
is viable.  Part of the reason for the apparent slow pace of the work, he
said, is that the Rust developers are concentrating on device drivers;
since drivers must interface with many other kernel subsystems, there is a
lot of support code that must be merged.  That takes time. Nobody covers the Linux kernel like LWN ; be in the know with a one-month trial subscription , no credit card needed. Gunthorpe said that he would like to see a clear message that Rust is a
success before jumping into it; he also is unable to work with Rust until a
suitable compiler is available in RHEL.  Airlie said that perhaps, for
Gunthorpe, the time had not yet come. Tooling and help Arnd Bergmann said that there was no doubt that drivers written in Rust
would be better than those in C, but he wondered how long it would
take to merge support in all the necessary subsystems, and when the tooling
would be widely available.  When, he
asked, will he be able to build kernel code with a Rust compiler shipped by
his distribution?  Ojeda answered that multiple compiler versions are now
supported by the kernel code, and that suitable compilers are available
from many community-oriented distributions.  Airlie said that it is too
soon to ask the Rust community for a completely stable compiler to build
kernel code with; there just is not yet enough Rust code in the kernel to
make that happen. Linus Torvalds admonished the group that he did not want to talk about
every subsystem supporting Rust at this time; getting support into some of
them is sufficient for now.  When Airlie asked what would happen when some
subsystem blocks progress, Torvalds answered "that's my job".  Christian
Brauner said that the binder driver is
motivating much of the subsystem work now, including the somewhat
contentious filesystem abstractions .  That
code is being reviewed now.  Airlie added that the first real driver to be
merged will be a sort of inflection point, after which the pace will pick
up; the next challenge after that will be the creation of Rust
infrastructure that is callable from C. Will Deacon asked Ojeda about the support that the Rust community was
offering to kernel developers; Ojeda answered that he has been building a
team of experts to help where needed.  Some of these people are core Rust
developers who know the language thoroughly; they can help to review
patches even if they lack deep kernel experience. Torvalds pointed out that there are kernel features that are currently
incompatible with Rust; that is impeding Rust support overall.  He
mentioned modversions in particular; that problem is being worked on .  The list of blocking
features is getting shorter, he said, but it still includes kernel features
that people need. Managing expectations Dan Williams pointed out that he once spent two years just getting a new mmap() flag merged.  It is necessary to manage expectations on the
Rust side, he said; merging all of that code will be a slow process.  Ojeda
acknowledged this point, but said that the companies funding
the Rust work are not seeing it going upstream; that is making them
reluctant to continue that funding going forward. Brauner said that nobody has ever declared that the filesystem abstractions
would not be merged; the discussion is all about the details of how that
will happen. Ted Ts'o said that the Rust developers have been trying to avoid scaring
kernel maintainers, and have been saying that "all you need is to learn a
little Rust".  But a little Rust is not enough to understand filesystem
abstractions, which have to deal with that subsystem's complex locking
rules.  There is a need for documentation and tutorials on how to write
filesystem code in idiomatic Rust.  He said that he has a lot to learn; he
is willing to do that, but needs help on what to learn.  (See this article for a discussion of how the
Rust-for-Linux developers are working to meet this need). Torvalds said that it is not necessary to understand Rust to let it into a
subsystem; after all, he said, nobody understands the
memory-management subsystem, but everybody is able to work with it.  I
pointed out that the Rust developers are not just creating subsystem
bindings; they are trying to create inherently safe interfaces, and that
often requires changes on the C side.  That increases the impact on
subsystems, but also makes the C code better.  Airlie added that the Rust
developers have to bring maintainers along with them, or the maintainers
will not understand what is happening. Deacon raised the question of refactoring on the C side.  Changing C
interfaces will often have implications for the Rust code and may break it;
somebody will the have to fix the problems.  Torvalds said that, for now,
breaking the Rust code is permissible, but that will change at some point
in the future.  Kroah-Hartman said that the Rust developers can take
responsibility for the maintenance of the abstractions they add. Steam right ahead Torvalds said that nothing depends on Rust in the kernel now, and nothing
will for some time yet.  What is important is to make forward progress, so
developers should "steam right ahead" and not worry about these problems
for now.  It is enough to get things working, even though the details are
not right.  Once users are depending on Rust code, it will be necessary to
worry more, he said, but kernel developers should not fail by being too
careful now. Thomas Gleixner said that the Rust developers are careful about documenting
their code, and he is not frightened by the prospect of refactoring it.  If
he does not understand something, he will simply send an email to the
developer, just as he does with C code.  Torvalds added that Rust has a lot
to offer, and the kernel should try to take advantage of it.  Kroah-Hartman
said that it could eliminate entire classes of bugs in the kernel. Deacon asked how many developers are working on the Rust side now; Ojeda
answered that there are currently six or seven people, most of whom are
"real Rust experts".  The strongest kernel expertise in the group had been
Wedson Almeida Filho, who had recently left the
project .  That was a real loss, but Ojeda is working to recruit others. Gleixner said that, 20 years ago, there had been a lot of fear and concern
surrounding the realtime kernel work; he is seeing the same thing now with
regard to Rust.  We cannot let that fear drive things, he said.  Torvalds
said that Rust has been partially integrated for two years.  That is
nothing, he said; the project to build the kernel with Clang took a decade,
and that was the same old language. Julia Lawall asked what happens when things change on the C side; how much
will leak through into the Rust code?  Bergmann said that reviewing Rust
abstractions for a C subsystem without knowing Rust is not difficult; he can
reach a point where he understands the code, but would not feel able to
change it. Torvalds said that the community can play around with Rust for a few years.
Gunthorpe, though, said that it would be good to get something into
production; that would give the project some needed momentum.  The binder
driver might be a good choice.  Ojeda said that would help to justify more
support from companies.  As the session closed, though, the primary outcome
may well have been expressed by Torvalds, who suggested telling people that
getting kernel Rust up to production levels will happen, but it will take
years. Index entries for this article Kernel Development tools/Rust Conference Kernel Maintainers Summit/2024 to post comments RHEL Support Posted Sep 24, 2024 14:20 UTC (Tue)
                               by jgg (subscriber, #55211)
                              [ Link ] (3 responses) > Gunthorpe said that he would like to see a clear message that Rust is a success before jumping into it; he also is unable to work with Rust until a suitable compiler is available in RHEL. Airlie said that perhaps, for Gunthorpe, the time had not yet come. To clarify my remark, I was speaking from the perspective of our community members that are working on server/enterprise/hyperscale projects where deplyoment of the project usually requires running on some older kernel via backports. Using RHEL as some shorthand for this ecosystem. This is a distinctly different set of concerns from Fedora, Android or Embedded communities. The issue here is not that a "suitable compiler" exists in RHEL, but that all the distros have enabled Rust in their kernels, that they fully support Rust on their commercially relavent server architectures (S390 and Power! I have users!) and everything is in place to backport and consume an add-on-top "driver backport package". There are many technical gaps still to be resolved here. This was, in part, a reaction to Miguel's earlier remark that companies were reluctant to participate. It will be very hard for a company to fund writing new code in Rust if none of their customers can run that code and they have to write a C version anyhow. RHEL Support Posted Sep 25, 2024 4:57 UTC (Wed)
                               by admalledd (subscriber, #95347)
                              [ Link ] (2 responses) FWIW, in -theory- 360, POWER, Sparc,  etc, should work with Rust. Just they aren't "Tier 1 officially supported" yet since they lack commercial backing/QA/validation, and getting them into the kernel would be its own minor project. For clarity on ISA/Platform support: https://doc.rust-lang.org/nightly/rustc/platform-support.... Though the above is more about applications, not kernels, you can (mostly) just pay attention to the first bit of a platform-triple there for what ISA is supported. That page also clarifies better than I could what T1/T2/T3/etc all mean on "officially supported". That is all for the LLVM backend, and there are of course the two projects (codegen-gccjit and GCC-rs) to use GCC in some flavor instead. So yea, there is concern still on missing platforms vs the Linux Kernel, but the hope is the commercial interests involved with those will either (1) support LLVM to those, or (2) support one or the other GCC project. All that said, still likely years away for any of the missing platforms to need to care about Rust: many of the subsystems and drivers likely to be written won't ever be intended for such legacy platforms anyways. RHEL Support Posted Sep 25, 2024 8:44 UTC (Wed)
                               by farnz (subscriber, #17727)
                              [ Link ] (1 responses) As a really quick summary, only x86-32, x86-64 and AArch64 are in Tier 1. Tier 2 brings in AArch32, LoongArch, RISC-V, PowerPC, SPARC64 and WASM. Tier 3 then brings in AVR, C-SKY, MIPS, SPARC32, S390x (but not plain S390) and 68k. Very roughly, the tiers are: Tier 1 blocks merging of PRs if any builds or tests fail in CI. Tier 2 blocks merging of PRs if any builds fail in CI; tests are allowed to fail. Tier 3 simply has one or more people promising to work on fixes if it fails to build in CI. RHEL Support Posted Sep 25, 2024 10:28 UTC (Wed)
                               by sam_c (subscriber, #139836)
                              [ Link ] What ends up being problematic for us is that only certain tiers get "host tools" built which means we have to build our own binaries for distributing Rust. sparc64 is an example. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 16:31 UTC (Tue)
                               by josh (subscriber, #17465)
                              [ Link ] (17 responses) The effort to get Rust-for-Linux to build using entirely stable Rust features (which involves stabilizing and in some cases designing several Rust features) is also progressing full steam ahead. I expect that that'll make it much easier for Linux distributions to include drivers written in Rust. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 18:42 UTC (Tue)
                               by sam_c (subscriber, #139836)
                              [ Link ] (16 responses) Genuinely quite pleased to hear this as it's been causing us a lot of concern. But I do wonder how this also isn't a sign that Rust isn't stable enough for use yet in the kernel? Nobody is adding dependencies on bleeding-edge GCC or Clang for unratified C features which may change at any point ("unstable"). Extensions do get used but that is once the compilers agree they're an interface and not subject to change, and the kernel always has fallbacks for older GCC and Clang where they're not available. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 18:51 UTC (Tue)
                               by mb (subscriber, #50428)
                              [ Link ] (2 responses) >Extensions do get used but that is once the compilers agree That's pretty much not true historically. For most of the time Linux just used what gcc provided as an extension and didn't care about anything else or whether it was "mature enough". >and the kernel always has fallbacks for older GCC Distributions used to ship special versions of gcc just for the kernel. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 18:54 UTC (Tue)
                               by sam_c (subscriber, #139836)
                              [ Link ] (1 responses) > For most of the time Linux just used what gcc provided as an extension and didn't care about anything else or whether it was "mature enough". It took a very long time indeed to agree to use newer C: https://lwn.net/Articles/885941/. > Distributions used to ship special versions of gcc just for the kernel. That doesn't mean we want to return to that. But the minimum GCC version even now is pretty conservative anyway? Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 23:50 UTC (Tue)
                               by Heretic_Blacksheep (subscriber, #169992)
                              [ Link ] Kernel docs says minimum version is GCC v. 5.1, but it varies depending on architecture.  Obviously recently ported hardware would require a newer GCC so take that with a grain of salt.  If a (micro)arch was only integrated into GCC last year, that's your likely minimal version.  The rest of the build tools versions vary similarly. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 20:25 UTC (Tue)
                               by cesarb (subscriber, #6266)
                              [ Link ] (12 responses) The issue is that Rust developers are very conservative about new language features; unlike GCC (and clang which copied the behavior from GCC) which always allows the use of language extensions, on Rust all these language features (which would be extensions in GCC) are considered experimental, and only enabled when using the pre-release "nightly" version of the compiler. This allows them to iterate on the feature's design, without having to worry about projects outside the compiler and its standard library depending on them. > and the kernel always has fallbacks for older GCC and Clang where they're not available. It doesn't; the kernel uses several non-standard GCC extensions with no fallback, and requires a version of GCC or clang which is recent enough to provide all these required extensions. Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 20:50 UTC (Tue)
                               by pizza (subscriber, #46)
                              [ Link ] (6 responses) > It doesn't; the kernel uses several non-standard GCC extensions with no fallback, and requires a version of GCC or clang which is recent enough to provide all these required extensions. While what you are saying is _technically_ correct, the details paint a very different story. Linux only requires GCC 5.1, which was released *nine years* ago.   Versus rustc 1.78, which only landed *four months* ago. (Or you could use LLVM/Clang intstead of GCC, that requires a ~2.5-year old version (13.0.1) Source: https://www.kernel.org/doc/html/next/process/changes.html Building Rust-for-Linux on stable Rust Posted Sep 24, 2024 22:33 UTC (Tue)
                               by intelfx (subscriber, #130118)
                              [ Link ] (4 responses) > While what you are saying is _technically_ correct, the details paint a very different story. Only if you conveniently ignore some of these details and highlight the others. > Linux only requires GCC 5.1, which was released *nine years* ago. Versus rustc 1.78, which only landed *four months* ago. Yes, so how old is the C code in Linux, and how old is the Rust code in Linux? And how much time did it take (since the inception of $LANGUAGE code in Linux) to arrive at this state of affairs? I’ve said it before, and I’ll say it again: it is unfair and disingenuous that the “new contender” is immediately required to clear the bar that nothing else was subjected to for a damn long time. Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 0:06 UTC (Wed)
                               by pizza (subscriber, #46)
                              [ Link ] (3 responses) > I’ve said it before, and I’ll say it again: it is unfair and disingenuous that the “new contender” is immediately required to clear the bar that nothing else was subjected to for a damn long time. It is unfair to deliberately misstate Linux's C compiler requirements as well. When Linux 5.15 bumped the minimum version to GCC 5.1, the latter had been available for *seven years*.     (As opposed to the then-brand-new GCC 11) Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 4:48 UTC (Wed)
                               by khim (subscriber, #9252)
                              [ Link ] (2 responses) And when Linus presented Linux in comp.os.minix he explicitly have written: It also uses every feature of gcc I could find . And he used the most recent version of gcc he could find. Now, that doesn't mean that then need to use bleeding version of Rust would stay with us forever, but I don't see why Rust have to be treated differently from how C was treated in the Linux bringup  phase. Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 11:56 UTC (Wed)
                               by pizza (subscriber, #46)
                              [ Link ] (1 responses) > Now, that doesn't mean that then need to use bleeding version of Rust would stay with us forever, but I don't see why Rust have to be treated differently from how C was treated in the Linux bringup phase. Because unlike Linus's original announcement of Linux 30-odd years ago, Linux is no longer a just-for-fun toy. If Rust proponents claim that the quality standards/practices of yesteryear are wholly insufficient today then they can't claim "waaah we're being held to a higher standard than ysteryear" when the mirror is turned on them. The response to the "Rust tooling is still immature and doesn't cover all use cases that some folks need" *fact* should be (and according to the _actual_ R4L devs, is) "We're still working on it; we'll get there", not "waah different standards, give us special treatment instead" or "those use cases don't matter" of the _very_ annoying fanbois that keep coming out of the woodwork. Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 14:15 UTC (Wed)
                               by intelfx (subscriber, #130118)
                              [ Link ] > The response to the "Rust tooling is still immature and doesn't cover all use cases that some folks need" *fact* should be (and according to the _actual_ R4L devs, is) "We're still working on it; we'll get there", not "waah different standards, give us special treatment instead" or "those use cases don't matter" of the _very_ annoying fanbois that keep coming out of the woodwork. Now this is a case of shifting the goalposts. The response is, and always was, "We're still working on it and we'll get there". Nobody has ever said "waah different standards" while asking for "special treatment". The problem with different standards is merely that certain vocal people want "getting there" to happen in zero time. C did not get there in zero time, so why does Rust have to? Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 1:22 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] > Linux only requires GCC 5.1, which was released *nine years* ago. Versus rustc 1.78, which only landed *four months* ago. istr that for many years Linux had a *maximum* version of gcc, which was positively ancient! If Linux can complain, at different times, that some gccs are too old, while other gccs are too new, then why can't it complain that some rustcs are too old, while others are too new? It is what it is. You just have to use whatever compiler that works. At the end of the day, the version of the compiler is irrelevant. Seeing as most of the Rust code currently appears to be drivers, if the compiler is contemporaneous with the hardware or feature, what's the problem? Cheers, Wol Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 2:15 UTC (Wed)
                               by milesrout (subscriber, #126894)
                              [ Link ] (4 responses) >are considered experimental, and only enabled when using the pre-release "nightly" version of the compiler. This allows them to iterate on the feature's design, without having to worry about projects outside the compiler and its standard library depending on them. So why is Linux going to depend on them? How is that supposed to work? Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 5:11 UTC (Wed)
                               by admalledd (subscriber, #95347)
                              [ Link ] (2 responses) There are a few approaches to this, but the basic one that R4L is (mostly) taking is thus: 1. Recommend a specific known nightly or beta build. 2.  Often, the specific nightly build chosen is *actually* the same as the then-just-released "stable compiler", but with the nightly features enabled. Work-in-progress to get to (2), and really I've used a nightly version from over a year ago (mostly) just fine, the stability-or-not of the total compiler is still (nearly) production worthy, it is the specific features themselves that are more a risk. To mitigate that, Rust-For-Linux *explicitly* tracks and lists exactly which ones they use and why: https://github.com/Rust-for-Linux/linux/issues/2 and that it is (strongly) recommended to not add to that list without good reason. Further, by linking that github issue as they have, any *changes* in the upstream Rust compiler known to change those unstable features the R4L project depends on gets ping'd to them, which they can then address. Most of the time, the features used by R4L are "user stable" in that "Rust *wants* to enable and let users use it exactly like X, but there are some corner cases that we choose to cause the compiler to error on that should work as well". IE: The features R4L are (mostly) using aren't expected to change much if at all from the developer's perspective. If they are likely to need to change though, due to github bot-magic, they will have plenty of warning and involvement before it even hits a nightly release. Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 10:03 UTC (Wed)
                               by sam_c (subscriber, #139836)
                              [ Link ] Replying to both you and asahilina: thanks. I'm pleased to hear that there's a strong presumption against adding more such cases especially. Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 16:56 UTC (Wed)
                               by ojeda (subscriber, #143370)
                              [ Link ] To clarify: we have never recommended nightly or beta compilers (specific or not). We have always supported stable (released) compilers only, until v6.11. Since v6.11, we support a minimum Rust version, currently 1.78.0. Thus 1.79, 1.80 and 1.81 work too. Beta and nightly compilers should also generally work now. The Rust project and bindgen have now the kernel in their pre-merge CI. This means that unintentional changes to unstable features in Rust that break Linux do not get merged into Rust. The #2 issue (and the sublists) definitely helps as you say (and it is one of the reasons why I started it in GitHub, since it cross-references issues), but having the CI is even better. Upstream Rust also has a tag for us, A-rust-for-linux, that we are starting to use. This has also allowed us to cover the toolchains of a few Linux distributions too. Please see https://rust-for-linux.com/rust-version-policy , as well as the Kernel Summit talk from LPC a few days ago (the stream/video is not available yet, as far as I understand). Building Rust-for-Linux on stable Rust Posted Sep 25, 2024 7:06 UTC (Wed)
                               by asahilina (subscriber, #166071)
                              [ Link ] There is a backdoor in the stable compiler to enable nightly features. R4L uses this backdoor, and therefore works with stable Rust builds. Since the features are still "nightly features" they can and do change, which can break the build with newer compiler versions, but the list keeps getting shorter and we try not to add any new required nightly features unless they are necessary or very important to achieve the R4L goals. At some point all of the features will be promoted to stable, the backdoor will no longer be necessary, and R4L builds will be guaranteed not to break with newer compilers. Positive vibes Posted Sep 24, 2024 16:59 UTC (Tue)
                               by Sesse (subscriber, #53779)
                              [ Link ] (5 responses) Is it just me, or does this mood sound much more cooperative than just a few weeks ago? Positive vibes Posted Sep 24, 2024 21:20 UTC (Tue)
                               by pbonzini (subscriber, #60935)
                              [ Link ] It's not just you, indeed. Positive vibes Posted Sep 24, 2024 21:22 UTC (Tue)
                               by rywang014 (subscriber, #167182)
                              [ Link ] (3 responses) Yes. I feel the whole fiasco is about "when C breaks Rust, who is responsive to fix". C people stated hard "Rust people are gonna fix them" and in this article we got a nod on this decision. So people can move forward with code. Positive vibes Posted Sep 25, 2024 8:40 UTC (Wed)
                               by pbonzini (subscriber, #60935)
                              [ Link ] (2 responses) At the same time there's also more willingness to collaborate than what transpired from the infamous "you're not going to make us learn Rust" quote. Positive vibes Posted Sep 25, 2024 12:05 UTC (Wed)
                               by pizza (subscriber, #46)
                              [ Link ] (1 responses) > At the same time there's also more willingness to collaborate than what transpired from the infamous "you're not going to make us learn Rust" quote. FWIW, sampling two points from from a locally-noisy source does not make for good generalizations. Positive vibes Posted Sep 25, 2024 12:16 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] The problem is, the squeaky wheel gets all the oil. Cheers, Wol Unstable compilers Posted Sep 24, 2024 18:39 UTC (Tue)
                               by sam_c (subscriber, #139836)
                              [ Link ] (12 responses) > Airlie said that it is too soon to ask the Rust community for a completely stable compiler to build kernel code with; there just is not yet enough Rust code in the kernel to make that happen. I don't understand this, this feels like it ought to be a prerequisite. If the language isn't ready yet with the pace at which it moves and the lack of a LTS release compiler, perhaps it isn't ready for the kernel yet either. Unstable compilers Posted Sep 24, 2024 21:44 UTC (Tue)
                               by Baughn (subscriber, #124425)
                              [ Link ] (10 responses) This describes C, for most of the history of thethe kernel. Unstable compilers Posted Sep 25, 2024 1:02 UTC (Wed)
                               by sam_c (subscriber, #139836)
                              [ Link ] (9 responses) Please elaborate. I am unfamiliar with the C you describe. Unstable compilers Posted Sep 25, 2024 5:16 UTC (Wed)
                               by admalledd (subscriber, #95347)
                              [ Link ] (8 responses) TL;DR: Until "recent history" of about as recent as ten years ago, compiling the Linux Kernel had a very odd list of "these are the only compiler versions *known* to work correctly, try any others and they are between you and the mantissa of your FPU doing a DIV 0". Such as specific patch levels of GCC or CLANG. You could (especially as you approach the modern era) still often *compile* with whatever GCC (and sometimes CLANG) version so long as it was above some horribly old bare minimum, but it was not uncommon to have subtle miscompilations in more esoteric drivers, or flat out some drivers not compiling, etc. Many forget this, because distros would baseline on these known-good compiler versions, so the majority of users would never know, often even kernel developers. Unstable compilers Posted Sep 25, 2024 9:32 UTC (Wed)
                               by sam_c (subscriber, #139836)
                              [ Link ] (5 responses) Thanks. I guess I'd make the distinction between new features only available in and bugginess though. Unstable compilers Posted Sep 25, 2024 9:32 UTC (Wed)
                               by sam_c (subscriber, #139836)
                              [ Link ] I meant to add: ... because you can more easily backport bug fixes. Unstable compilers Posted Sep 25, 2024 9:46 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] (3 responses) >  I guess I'd make the distinction between new features only available in and bugginess though. You need to add to that list "new features either enabled by default, or often enabled by force, that break the kernel". One only has to look at all the complaints about UB, where gcc was optimising code away. Quite often (a) this stuff was NEEDED, and (b) there was no flag to disable the optimisation responsible for removing them. So the compiler wasn't buggy, it was working as designed. And if the die-hards are going to complain and say "well Rust needs to call C to access the hardware", it was not at all unusual for C to have to call assembler to access the hardware, because the C compiler was just deleting the code as UB with no way to stop it. Hence my comments in various places about mathematics is not reality. The difference is, it appears the Rust devs seem much more amenable to treating "you're optimising away necessary code" as a bug than the C folks were. Cheers, Wol Unstable compilers Posted Sep 25, 2024 9:52 UTC (Wed)
                               by pbonzini (subscriber, #60935)
                              [ Link ] (2 responses) > it appears the Rust devs seem much more amenable to treating "you're optimising away necessary code" as a bug than the C folks were. The optimization backend in the end is the same, the difference is that the language itself is designed to make it harder to trigger undefined behavior. For example the level of aliasing is declared explicitly by separating shared and exclusive references (& and &mut). Developers can only turn a shared reference into an exclusive one via UnsafeCell (and then the code needs to be explicitly marked as unsafe) or wrappers thereof (which are carefully designed to avoid undefined behavior). Unstable compilers Posted Sep 25, 2024 10:42 UTC (Wed)
                               by Wol (subscriber, #4433)
                              [ Link ] (1 responses) > > it appears the Rust devs seem much more amenable to treating "you're optimising away necessary code" as a bug than the C folks were. > The optimization backend in the end is the same, the difference is that the language itself is designed to make it harder to trigger undefined behavior. Maybe that's the effect. But even today the C devs seem keen on creating new undefined behaviours. In Rust, undefined behaviour is viewed as a bug in the language (or as "you can only do that in an unsafe block") as far as I can tell. Completely diametric views on UB. Cheers, Wol Unstable compilers Posted Sep 25, 2024 18:10 UTC (Wed)
                               by sunshowers (subscriber, #170655)
                              [ Link ] Yes, that is correct, and Rust is better for it. Under the assumption that unsafe code is correct, safe code doesn't have UB. I would rather optimization 'alpha' be extracted through principled approaches like enabling the use of "restrict" on &mut pointers. Unstable compilers Posted Sep 25, 2024 17:56 UTC (Wed)
                               by ballombe (subscriber, #9523)
                              [ Link ] (1 responses) > You could (especially as you approach the modern era) still often *compile* with whatever GCC (and sometimes CLANG) version so long as it was above some horribly old bare minimum, but it was not uncommon to have subtle miscompilations in more esoteric drivers, or flat out some drivers not compiling, etc. Because all rust compilers are guaranteed to be bug-free ? Unstable compilers Posted Sep 25, 2024 18:29 UTC (Wed)
                               by admalledd (subscriber, #95347)
                              [ Link ] Certainly not, but the stability of `rustc` is often much higher than many give it credit for, and that Rust gives much better ABI(*)/linking guarantees. It is often not plausible to use a different GCC version from what compiled the kernel to compile custom modules for example. Rust can provide much stronger guarantees here by default, though as mentioned in the article there is in-kernel work to be done for the module symbol naming/linking. Rust in *most* cases will either halt compilation with an Error, and InternalCompilerError, or result in symbols that will fail at linking time instead of (used to be more) commonly "compiling" fine but crashing/failing at runtime or `insmod` time. * Rust technically has no ABI, but you can expose/use one via various exports/macro things, commonly of course a "c-abi". This is mostly handled by `rust-bindgen` for automation with a dash of human control when required. Unstable compilers Posted Sep 25, 2024 12:47 UTC (Wed)
                               by tialaramex (subscriber, #21167)
                              [ Link ] Take for example 128-bit integer support. Rust has 128-bit integers. Code to handle these e.g. formatting them for output as text - exists but in a typical Rust application which doesn't actually have any 128-bit integers the code for them never ends up in the resulting binary. But, Linux currently just says nope, we'll take everything just in case, because a loadable module might use it. A few kilobytes of formatting for types we never use? Give me those too. So for Stable Rust Linux would get a bunch of 128-bit integer formatting code it has no use for, on the off chance that somebody might some day write a loadable module which expects 128-bit integer formatting to work. In Rust for Linux they've instead just made this code conditional and switched off the condition, it won't compile in your loadable module and the kernel avoids a few kilobytes of "format 128-bit integers" code. The push back from the Rust community is they're not interested in doing a bunch of extra work to help people who want all the symbols but then wish somebody else would do the work to slim it down for them, pick a lane. So the Rust for Linux change will never land in the stable Rust in this form.
======>
https://github.com/tracyspacy/mastering_rust
-->>-->>
Repository files navigation README Rust Ownership Basics : Immutable Reference, mutable Reference, Rc and Refcell tldr Avoid `clone()` options ^_^
                |
    ---------------------------------------------
    |                                            |
Read-only access?                          Need to mutate?
    |                                            |
    --------------------                    -------------------
    |                  |                    |                 |
No Ownership?   Shared Ownership?  Exclusive mutability?   Shared mutability?
    |                  |                    |                 |
  Use &T           Use Rc<T>           Use &mut         Use Rc<RefCell<T>> Rust Ownership rules: Each value in Rust has an owner . There can only be one owner at a time. When the owner goes out of scope, the value will be dropped.[1] In this post, we will discuss heap-allocated variables and when to use references (&), mutable references (&mut), Rc, or RefCell. Simple basics from the Book: To ensure memory safety after we assign s1 to s2 (shallow copy), Rust considers s1 as no longer valid - s1 goes out of scope , ownership is transfered to s2. Therefore, it is impossible to print s1 afterward. fn main ( ) { let s1 = String :: from ( "hello" ) ; let s2 = s1 ; // ownership moved to s2 println ! ( "{s1} {s2}" ) ; // s1 is no longer valid } playground In this case, we might be tempted to use clone() (deep copy of heap data) to satisfy the compiler: fn main ( ) { let s1 = String :: from ( "hello" ) ; let s2 = s1 . clone ( ) ; // deep copy of s1 println ! ( "{s1},{s2}!" ) ; // we can print s1 and s2 } playground But copying heap data is an expensive operation and to achieve the same result for this read-only functionality we have nore efficient option without transfering ownership or deep copy : an immutable refference. Immutable refference (&T) fn main ( ) { let s1 = String :: from ( "hello" ) ; let s2 = & s1 ; // borrowing s1 ie creating reference println ! ( "{s1},{s2}!" ) ; // we can print s1 and s2 } playgroung So we can use an immutable refference when we do not need to take ownership of variable and we do not need to mutate it. We can have multiple immutable references to a variable, but these references are only valid for as long as the variable itself remains in scope. Once the variable goes out of scope, all its immutable references become invalid. But what if different parts of our code need to access the variable simultaneously? fn main ( ) { let s2 ; { let s1 = String :: from ( "hello" ) ; s2 = & s1 ; // borrowing s1, i.e., creating a reference } // s1 goes out of scope, so the borrowed value is no longer valid println ! ( "{s2}" ) ; // Error: cannot print s2 because the borrowed value does not live long enough } playground If we will try to compile this code we will get error: borrowed value does not live long enough . This is where smart pointers like Rc can help. Refference Count (Rc) (single-threaded) Rc<T> stands for Reference Count. It provides shared ownership of a value of type T , allocated on the heap. It allows multiple parts of a program to have ownership of the same data, and the value will only be dropped when the last reference goes out of scope. Invoking clone on Rc produces a new pointer to the same allocation in the heap.[2] use std :: rc :: Rc ; fn main ( ) { let s2 ; { let s1 = Rc :: new ( String :: from ( "hello" ) ) ; // Create a reference-counted string. s2 = Rc :: clone ( & s1 ) ; //Cloning the s1 produces a new pointer to the same allocation in the heap } // s1 goes out of scope, but the String it points to remains valid because s2 still references it println ! ( "{s2}" ) ; // we can print s2 } playground Unlike an immutable reference, Rc allows multiple owners of the same value. So even though s1 goes out of scope, the value it points to remains valid because s2 is still referencing it. The reference count ensures the value will only be deallocated when the last owner ( s2 in this case) is dropped. Similar to an immutable reference ( &T ), Rc<T> allows multiple references to data on the heap, without allowing mutation by default . use std :: rc :: Rc ; fn main ( ) { let s1 = Rc :: new ( String :: from ( "hello" ) ) ; let s2 = Rc :: clone ( & s1 ) ; let s3 = Rc :: clone ( & s1 ) ; println ! ( "{s1} {s2} {s3}" ) ; // s2 and s3 both point to the same memory location as s1. } playground These are some primitive examples that simply help illustrate the logic. For more advanced examples and use cases, you can refer to the official documentation std::rc - Rust As we mentioned above, immutable reference and Rc are both not allowing mutation as default, but what if we need to mutate data on the heap? Mutable reference (&mut T) When we need to mutate a borrowed value, the most obvious option is to use mutable reference (& mut). fn main ( ) { let mut s1 = String :: from ( "hello" ) ; change ( & mut s1 ) ; println ! ( "{s1}" ) ; } fn change ( some_string : & mut String ) { some_string . push_str ( ", world" ) ; } playground Mutable references have one big restriction: if you have a mutable reference to
a value, you can have no other simultaneous references to that value. It is not allowed to have immutable refference if there is already mutable reference exists. fn main ( ) { let mut s1 = String :: from ( "hello" ) ; let s2 = & mut s1 ; // mutable borrowing s1 let s3 = & s1 ; // cannot borrow `s1` as immutable because it is also borrowed as mutable println ! ( "{s1},{s2},{s3}!" ) ; } It's also not allowed to have two simultaneous mutable references to the same value: fn main ( ) { let mut s1 = String :: from ( "hello" ) ; let s2 = & mut s1 ; // mutable borrowing s1 let s3 = & mut s1 ; // cannot borrow `s1` as mutable more than once at a time println ! ( "{s1},{s2},{s3}!" ) ; } playground However, Rust's borrowing rules don’t allow more than one mutable reference at a time, which can be limiting in certain scenarios. For example, what if you need multiple parts of your program to mutate the same data simultaneously, without violating Rust’s safety? This is where the combination of Rc<T> and RefCell<T> comes into play, allowing shared ownership and mutation. Combining RefCell and Rc we can have multiple owners of mutable data. (single-threaded) A common way to use RefCell<T> is in combination with Rc<T> . Recall that Rc<T> lets you have multiple owners of some data, but it only gives immutable access to that data by default. If you have an Rc<T> that holds a RefCell<T> , you can get a value that can have multiple owners and that you can mutate! [3] use std :: rc :: Rc ; use std :: cell :: RefCell ; fn main ( ) { let s1 = Rc :: new ( RefCell :: new ( String :: from ( "hello" ) ) ) ; let s2 = Rc :: clone ( & s1 ) ; let s3 = Rc :: clone ( & s1 ) ; // Now both s2 and s3 can mutate the shared string s2 . borrow_mut ( ) . push_str ( ", world" ) ; s3 . borrow_mut ( ) . push_str ( "!" ) ; println ! ( "{:?}" , s1 ) ; // hello, world! } playground But of course true power of combining Rc and RefCell comes with complex data structures such as linked lists, trees etc. Binary tree example [4]: use std :: cell :: RefCell ; use std :: fmt :: Debug ; use std :: rc :: Rc ; # [ derive ( Debug , PartialEq , Eq , Clone ) ] pub struct TreeNode < T > { pub val : T , pub left : Option < Rc < RefCell < TreeNode < T > > > > , pub right : Option < Rc < RefCell < TreeNode < T > > > > , } impl < T > TreeNode < T > { pub fn new ( val : T ) -> Self { TreeNode { val , left : None , right : None , } } } pub fn preorder_traversal < T : Debug > ( root : & Option < Rc < RefCell < TreeNode < T > > > > ) { match root { None => { } Some ( node ) => { let borrowed_node = node . borrow ( ) ; println ! ( "{:?}" , borrowed_node.val ) ; preorder_traversal ( & borrowed_node . left . clone ( ) ) ; preorder_traversal ( & borrowed_node . right . clone ( ) ) ; } } } pub fn inorder_traversal < T : Debug > ( root : & Option < Rc < RefCell < TreeNode < T > > > > ) { match root { None => { } Some ( node ) => { let borrowed_node = node . borrow ( ) ; inorder_traversal ( & borrowed_node . left . clone ( ) ) ; println ! ( "{:?}" , borrowed_node.val ) ; inorder_traversal ( & borrowed_node . right . clone ( ) ) ; } } } pub fn postorder_traversal < T : Debug > ( root : & Option < Rc < RefCell < TreeNode < T > > > > ) { if let Some ( node ) = root { let borrowed_node = node . borrow ( ) ; postorder_traversal ( & borrowed_node . left . clone ( ) ) ; postorder_traversal ( & borrowed_node . right . clone ( ) ) ; println ! ( "{:?}" , borrowed_node.val ) ; } } pub fn calculate_max_depth < T : Debug > ( root : & Option < Rc < RefCell < TreeNode < T > > > > ) -> i32 { match root { None => return 0 , Some ( node ) => { let borrowed_node = node . borrow ( ) ; let left_depth = calculate_max_depth ( & borrowed_node . left . clone ( ) ) ; let right_depth = calculate_max_depth ( & borrowed_node . right . clone ( ) ) ; if left_depth > right_depth { return left_depth + 1 ; } else { return right_depth + 1 ; } } } } /* 1 /    \ 2      3 /  \    /  \ 4    5  6    7 */ fn main ( ) { let mut root = TreeNode :: new ( 1 ) ; root . left = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 2 ) ) ) ) ; root . right = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 3 ) ) ) ) ; if let Some ( left_node ) = root . left . as_mut ( ) { //same as with root we should make root.left mut to modify left_node . borrow_mut ( ) . left = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 4 ) ) ) ) ; // borrowing is obtain mut ref to Refcall containing root.left, // ie to access and mutate the left field of the root inside the RefCell contained within the Rc. } if let Some ( right_node ) = root . left . as_mut ( ) { right_node . borrow_mut ( ) . right = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 5 ) ) ) ) ; } if let Some ( left_node ) = root . right . as_mut ( ) { left_node . borrow_mut ( ) . left = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 6 ) ) ) ) ; } if let Some ( right_node ) = root . right . as_mut ( ) { right_node . borrow_mut ( ) . right = Some ( Rc :: new ( RefCell :: new ( TreeNode :: new ( 7 ) ) ) ) ; } println ! ( "{:?}" , root ) ; println ! ( "Preorder Traversal:" ) ; preorder_traversal ( & Some ( Rc :: new ( RefCell :: new ( root . clone ( ) ) ) ) ) ; println ! ( "Inorder Traversal:" ) ; inorder_traversal ( & Some ( Rc :: new ( RefCell :: new ( root . clone ( ) ) ) ) ) ; println ! ( "Postorder Traversal:" ) ; postorder_traversal ( & Some ( Rc :: new ( RefCell :: new ( root . clone ( ) ) ) ) ) ; println ! ( "Max depth is: {:?}" ,
        calculate_max_depth ( & Some ( Rc ::new ( RefCell ::new ( root ) ) ) ) ) ; } playground Summary: Use &T for non-owning, read-only access. Use &mut T for non-owning, exclusive mutable access. Use Rc for shared ownership when you don't need mutability. Combine Rc with RefCell for shared ownership with mutability. Source: [1] What is Ownership? - The Rust Programming Language [2] std::rc - Rust [3] RefCell<T> and the Interior Mutability Pattern - The Rust Programming Language [4] algos_data_structures_rust/src/trees/binary_trees/binary_tree.rs at master · tracyspacy/algos_data_structures_rust · GitHub
======>
https://old.reddit.com/r/rust/comments/1fpc0ia/feel_like_i_know_nothing/
-->>-->>
I've been developing my second project in Rust, and it's a music player/manager that downloads music, searches for it, plays it, etc., but I currently use my own custom implementation and then Rodio for playback. I was going to go into Rodio and see if I can implement agc (automatic gain control). I went into the project and    looked at this file   . After messing with it for awhile, I felt like I knew nothing trying to figure out where everything goes, and all the methods and traits felt like a kid in a pool who didn't know how to swim. This is a normal feeling for devs?   

   Also, due to not using github collaboratively, I don't really know the etiquette and how to do pull requests, like must I have it in a functioning state before doing the pull request, etc.   

   For reference Rust is my first "proper" language that I've been learning for around 1.5 years. Before that, I only really did bash scripting and batch/powershell, so I kind of dove right into the deep end and "kind of liked it."   
   

======>
https://old.reddit.com/r/rust/comments/1fpmvso/finding_neat_way_to_access_utf8_characters/
-->>-->>
In Python if you want second character in '你好' you just execute    '你好'[1]    and it returns '好'.   

   But in Rust you need    '你好'.chars().nth(1).unwrap()   , which is tedious.   

   Any neat way to do that?   
   

======>
https://old.reddit.com/r/rust/comments/1fpnjl0/giving_bevy_the_quickstart_guide_it_deserves/
-->>-->>
If you go to the    Bevy    homepage and click    Learn   , it will only take a few clicks until you reach    this page    after only learning the absolute basics. Bevy developers then proceed with the next arduous step of scraping possibly-outdated unofficial resources and videos to learn the absolute building blocks of a Bevy game. This can lead to the following scenario:   

   randomly clicking through documentation   

   "Oh. They already made this thing for me, and I didn't have to write endless spaghetti code to implement it on my own. Well that would have been good to know... a little earlier."   

   I encountered    this issue    myself back when I was learning Bevy and wrote some truly terrible code, for which I blame half my lack of skill, and half the lack of documentation.   

   I have learned a lot, but still do not know everything. Bevy has so much to offer - did you know it has a built in    animation clip system   ? Did you know you can    attach observers to specific events    instead of fishing for them in a generic    Update    system? Did you know you can create    custom Query structs    to avoid bloating systems with huge bulky types?   

   I am in the process of writing a Quick-Start guide that will hopefully be able to dispel some of this obscurity, with an example game being made at the same time which aims to be actually enjoyable to some extent and not yet another Breakout or Pong clone.   

   The first 3 chapters are available already - the third one was written while I was seated in person next to Alice, who currently works    full time    on the Bevy engine!   

   1. Drawing things on screen   

   2. Using Events and Components to respond to situations   

   3. Getting chased around by a sticky friend, introducing some more advanced    Resource    and    QueryFilter    usage   

   This is currently quite nascent and I want it to be the best it can be. I will be reviewing this with seasoned Bevy users to find ways of improving it... but as the quote goes, "the best way to get what you want is not to ask a question, but to affirm incorrect things and wait to be corrected", so scathing Reddit criticism is always appreciated as well. Each fix now prevents bulky refactorings later.   
   

======>
https://old.reddit.com/r/rust/comments/1fp7ti8/i_made_a_youtube_video_about_making_my_own_engine/
-->>-->>
Hey. I recently made an YouTube video, about my own Game Engine which is completely written in Rust. I put a lot of effort into it. Would love to hear feedback :D   

   GitHub:    https://github.com/ventengine/Vent-Engine   

   Video:    https://youtu.be/_Ttc8vFpn94   
   

======>
https://okubrowser.github.io/
-->>-->>
Making browsing fun again. Create & publish sites Share files directly with friends Storing personal data on the devices you control Synchronise files between your devices Make & share your own sites. Oku lets you create replicas , virtual drives that you can share online. Replicas can contain anything, including photos, videos, or documents. You can put a site in a replica and share it with the world. Your data on your machine. Let sites keep your information with you, not on servers they control. Replicas are sent with tickets . A read-only ticket can be used to view, but not edit, your replica. A read & write ticket can be used to view or edit your replica. Sites can keep your data with you in a replica by asking for a read & write ticket. Let your garden grow wild. Build your own space on the Internet, independent from social media platforms. Replicas can be viewed by their ID. Every time you visit a new replica, your browser saves its ID. When you update a replica, those with its ID will see the newest version. Not Google Chrome in disguise. Powered by WebKit, Oku is a genuine alternative to Google Chrome, not a derivative. Looking to contribute? Oku is free and open-source software, accepting code contributions on GitHub . Learn more about Oku → © Emil Sayahi 2024
======>
https://security.googleblog.com/2024/09/eliminating-memory-safety-vulnerabilities-Android.html?m=1
-->>-->>
September 25, 2024 Eliminating Memory Safety Vulnerabilities at the Source Posted by Jeff Vander Stoep - Android team, and Alex Rebert - Security Foundations Memory safety vulnerabilities remain a pervasive threat to software security. At Google, we believe the path to eliminating this class of vulnerabilities at scale and building high-assurance software lies in Safe Coding , a secure-by-design approach that prioritizes transitioning to memory-safe languages. This post demonstrates why focusing on Safe Coding for new code quickly and counterintuitively reduces the overall security risk of a codebase, finally breaking through the stubbornly high plateau of memory safety vulnerabilities and starting an exponential decline, all while being scalable and cost-effective. We’ll also share updated data on how the percentage of memory safety vulnerabilities in Android dropped from 76% to 24% over 6 years as development shifted to memory safe languages. Counterintuitive results Consider a growing codebase primarily written in memory-unsafe languages, experiencing a constant influx of memory safety vulnerabilities. What happens if we gradually transition to memory-safe languages for new features, while leaving existing code mostly untouched except for bug fixes? We can simulate the results. After some years, the code base has the following makeup 1 as new memory unsafe development slows down, and new memory safe development starts to take over: In the final year of our simulation, despite the growth in memory-unsafe code, the number of memory safety vulnerabilities drops significantly, a seemingly counterintuitive result not seen with other strategies: This reduction might seem paradoxical: how is this possible when the quantity of new memory unsafe code actually grew? The math The answer lies in an important observation: vulnerabilities decay exponentially . They have a half-life. The distribution of vulnerability lifetime follows an exponential distribution given an average vulnerability lifetime λ: A large-scale study of vulnerability lifetimes 2 published in 2022 in Usenix Security confirmed this phenomenon. Researchers found that the vast majority of vulnerabilities reside in new or recently modified code: This confirms and generalizes our observation, published in 2021 , that the density of Android’s memory safety bugs decreased with the age of the code, primarily residing in recent changes. This leads to two important takeaways: The problem is overwhelmingly with new code , necessitating a fundamental change in how we develop code. Code matures and gets safer with time, exponentially , making the returns on investments like rewrites diminish over time as code gets older. For example, based on the average vulnerability lifetimes, 5-year-old code has a 3.4x (using lifetimes from the study) to 7.4x (using lifetimes observed in Android and Chromium) lower vulnerability density than new code. In real life, as with our simulation, when we start to prioritize prevention, the situation starts to rapidly improve. In practice on Android The Android team began prioritizing transitioning new development to memory safe languages around 2019. This decision was driven by the increasing cost and complexity of managing memory safety vulnerabilities. There’s much left to do, but the results have already been positive. Here’s the big picture in 2024, looking at total code: Despite the majority of code still being unsafe (but, crucially, getting progressively older), we’re seeing a large and continued decline in memory safety vulnerabilities. The results align with what we simulated above, and are even better, potentially as a result of our parallel efforts to improve the safety of our memory unsafe code. We first reported this decline in 2022, and we continue to see the total number of memory safety vulnerabilities dropping 3 . Note that the data for 2024 is extrapolated to the full year (represented as 36, but currently at 27 after the September security bulletin ). The percent of vulnerabilities caused by memory safety issues continues to correlate closely with the development language that’s used for new code. Memory safety issues, which accounted for 76% of Android vulnerabilities in 2019, and are currently 24% in 2024, well below the 70% industry norm, and continuing to drop. As we noted in a previous post, memory safety vulnerabilities tend to be significantly more severe, more likely to be remotely reachable, more versatile, and more likely to be maliciously exploited than other vulnerability types. As the number of memory safety vulnerabilities have dropped, the overall security risk has dropped along with it. Evolution of memory safety strategies Over the past decades, the industry has pioneered significant advancements to combat memory safety vulnerabilities, with each generation of advancements contributing valuable tools and techniques that have tangibly improved software security. However, with the benefit of hindsight, it’s evident that we have yet to achieve a truly scalable and sustainable solution that achieves an acceptable level of risk: 1st generation: reactive patching. The initial focus was mainly on fixing vulnerabilities reactively. For problems as rampant as memory safety, this incurs ongoing costs on the business and its users. Software manufacturers have to invest significant resources in responding to frequent incidents. This leads to constant security updates, leaving users vulnerable to unknown issues, and frequently albeit temporarily vulnerable to known issues, which are getting exploited ever faster . 2nd generation: proactive mitigating. The next approach consisted of reducing risk in vulnerable software, including a series of exploit mitigation strategies that raised the costs of crafting exploits. However, these mitigations, such as stack canaries and control-flow integrity, typically impose a recurring cost on products and development teams, often putting security and other product requirements in conflict: They come with performance overhead, impacting execution speed, battery life, tail latencies, and memory usage, sometimes preventing their deployment. Attackers are seemingly infinitely creative, resulting in a cat-and-mouse game with defenders. In addition, the bar to develop and weaponize an exploit is regularly being lowered through better tooling and other advancements . 3rd generation: proactive vulnerability discovery. The following generation focused on detecting vulnerabilities. This includes sanitizers , often paired with fuzzing like libfuzzer , many of which were built by Google. While helpful, these methods address the symptoms of memory unsafety, not the root cause. They typically require constant pressure to get teams to fuzz, triage, and fix their findings, resulting in low coverage. Even when applied thoroughly, fuzzing does not provide high assurance, as evidenced by vulnerabilities found in extensively fuzzed code. Products across the industry have been significantly strengthened by these approaches, and we remain committed to responding to, mitigating, and proactively hunting for vulnerabilities. Having said that, it has become increasingly clear that those approaches are not only insufficient for reaching an acceptable level of risk in the memory-safety domain, but incur ongoing and increasing costs to developers, users, businesses, and products. As highlighted by numerous government agencies, including CISA, in their secure-by-design report , "only by incorporating secure by design practices will we break the vicious cycle of constantly creating and applying fixes." The fourth generation: high-assurance prevention The shift towards memory safe languages represents more than just a change in technology, it is a fundamental shift in how to approach security. This shift is not an unprecedented one, but rather a significant expansion of a proven approach. An approach that has already demonstrated remarkable success in eliminating other vulnerability classes like XSS. The foundation of this shift is Safe Coding , which enforces security invariants directly into the development platform through language features, static analysis, and API design. The result is a secure by design ecosystem providing continuous assurance at scale, safe from the risk of accidentally introducing vulnerabilities. The shift from previous generations to Safe Coding can be seen in the quantifiability of the assertions that are made when developing code. Instead of focusing on the interventions applied (mitigations, fuzzing), or attempting to use past performance to predict future security, Safe Coding allows us to make strong assertions about the code's properties and what can or cannot happen based on those properties. Safe Coding's scalability lies in its ability to reduce costs by: Breaking the arms race: Instead of an endless arms race of defenders attempting to raise attackers’ costs by also raising their own, Safe Coding leverages our control of developer ecosystems to break this cycle by focusing on proactively building secure software from the start. Commoditizing high assurance memory safety: Rather than precisely tailoring interventions to each asset's assessed risk, all while managing the cost and overhead of reassessing evolving risks and applying disparate interventions, Safe Coding establishes a high baseline of commoditized security , like memory-safe languages, that affordably reduces vulnerability density across the board. Modern memory-safe languages (especially Rust) extend these principles beyond memory safety to other bug classes. Increasing productivity : Safe Coding improves code correctness and developer productivity by shifting bug finding further left, before the code is even checked in. We see this shift showing up in important metrics such as rollback rates (emergency code revert due to an unanticipated bug). The Android team has observed that the rollback rate of Rust changes is less than half that of C++. From lessons to action Interoperability is the new rewrite Based on what we’ve learned, it's become clear that we do not need to throw away or rewrite all our existing memory-unsafe code. Instead, Android is focusing on making interoperability safe and convenient as a primary capability in our memory safety journey. Interoperability offers a practical and incremental approach to adopting memory safe languages, allowing organizations to leverage existing investments in code and systems, while accelerating the development of new features. We recommend focusing investments on improving interoperability, as we are doing with Rust ↔︎ C++ and Rust ↔︎ Kotlin . To that end, earlier this year, Google provided a $1,000,000 grant to the Rust Foundation, in addition to developing interoperability tooling like Crubit and autocxx . Role of previous generations As Safe Coding continues to drive down risk, what will be the role of mitigations and proactive detection? We don’t have definitive answers in Android, but expect something like the following: More selective use of proactive mitigations : We expect less reliance on exploit mitigations as we transition to memory-safe code, leading to not only safer software, but also more efficient software. For instance, after removing the now unnecessary sandbox, Chromium's Rust QR code generator is 95% faster. Decreased use, but increased effectiveness of proactive detection : We anticipate a decreased reliance on proactive detection approaches like fuzzing, but increased effectiveness, as achieving comprehensive coverage over small well-encapsulated code snippets becomes more feasible. Final thoughts Fighting against the math of vulnerability lifetimes has been a losing battle. Adopting Safe Coding in new code offers a paradigm shift, allowing us to leverage the inherent decay of vulnerabilities to our advantage, even in large existing systems . The concept is simple: once we turn off the tap of new vulnerabilities, they decrease exponentially, making all of our code safer, increasing the effectiveness of security design , and alleviating the scalability challenges associated with existing memory safety strategies such that they can be applied more effectively in a targeted manner. This approach has proven successful in eliminating entire vulnerability classes and its effectiveness in tackling memory safety is increasingly evident based on more than half a decade of consistent results in Android. We'll be sharing more about our secure-by-design efforts in the coming months. Acknowledgements Thanks Alice Ryhl for coding up the simulation. Thanks to Emilia Kasper, Adrian Taylor, Manish Goregaokar, Christoph Kern, and Lars Bergstrom for your helpful feedback on this post. Notes Simulation was based on numbers similar to Android and other Google projects. The code base doubles every 6 years. The average lifetime for vulnerabilities is 2.5 years. It takes 10 years to transition to memory safe languages for new code, and we use a sigmoid function to represent the transition. Note that the use of the sigmoid function is why the second chart doesn’t initially appear to be exponential. ↩ Alexopoulos et al. "How Long Do Vulnerabilities Live in the Code? A Large-Scale Empirical Measurement Study on FOSS Vulnerability Lifetimes" . USENIX Security 22. ↩ Unlike our simulation, these are vulnerabilities from a real code base, which comes with higher variance, as you can see in the slight increase in 2023. Vulnerability reports were unusually high that year, but in line with expectations given code growth, so while the percentage of memory safety vulnerabilities continued to drop, the absolute number increased slightly. ↩ Edward Fernandez Share No comments: Post a Comment You are welcome to contribute comments, but they should be relevant to the conversation. We reserve the right to remove off-topic remarks in the interest of keeping the conversation focused and engaging. Shameless self-promotion is well, shameless, and will get canned. Note: Only a member of this blog may post a comment. BLOG_CMT_createIframe('https://www.blogger.com/rpc_relay.html'); › Home View web version
======>
https://bevyengine.org/news/bevy-foundation-501c3/
-->>-->>
What does this mean for Bevy? Every.org: our new donation platform New Bevy Foundation Emails Please Donate! Bevy Foundation is now a 501(c)(3) Public Charity Posted on September 25, 2024 by Carter Anderson ( @cart @cart_cart cartdev ) In March we announced the Bevy Foundation , our new non-profit dedicated to developing Bevy and teaching people how to use it. At that time, we applied for federal 501(c)(3) public charity status, which would give us tax-exempt status in the United States and make donations tax-deductible. It introduces stricter operational constraints that help protect the integrity of our mission and our community. I am excited to announce that the Bevy Foundation has been granted 501(c)(3) public charity status! What does this mean for Bevy? # We are exempt from income tax on donations we receive (including the donations we have already received this year). This means 100% of the donations we receive go toward furthering our mission. Donations to Bevy Foundation (including all past donations that occurred this year) are now tax-deductible in the United States. Many employers have donation-matching programs for 501(c)(3)s. It is worth checking to see if your employer will match your donation! There are many grant opportunities out there for 501(c)(3)s. These are now on the table for the Bevy Foundation. We now must operate under the constraints of a 501(c)(3) public charity We must file a yearly report that outlines our activities (IRS form 990). I am personally very excited about this. 501(c)(3) public charity status, because of its constraints, buys us more public trust than the more general state-level non-profit status. Each donation can go further, and there are now more opportunities for financial support! Every.org: our new donation platform # As a 501(c)(3) we are now qualified to use Every.org as our donation platform. Every.org is also a 501(c)(3) that exists to make 501(c)(3) fundraising as straightforward and cheap as possible. They take no platform fees, and if you pay directly with a bank account, there are literally zero fees. 100% of your donation goes to Bevy Foundation. If you pay with a credit card or a payment processor like Apple Pay, you still pay those platform fees, but they are as low as possible. We still support Stripe (our old platform) to give people as many options as possible. But we now prefer Every.org and default to it where we can, as it provides much better rates: Every.org Stripe Bank Fee 0% 0.8% Subscription Fee 0% 0.5% (increasing to 0.7% next year) Card Fee 2.2% + $0.30 2.2% + $0.30 These fees really add up over time, so using Every.org is a huge win for us! Every.org also supports easy one-time payments with custom amounts (an often requested feature). See our expanded donation options page for details. New Bevy Foundation Emails # As part of this announcement, we're also rolling out new bevyengine.org emails: support@bevyengine.org : The new way to ask Bevy leadership for support. This replaces bevyengine@gmail.com . foundation@bevyengine.org : The best way to reach Bevy Foundation board members. The Bevy Maintainers / Bevy Foundation Board Members now also have individual bevyengine.org emails: Carter Anderson: cart@bevyengine.org Alice Cecile: alice@bevyengine.org François Mockers: francois@bevyengine.org Robert Swain: rob@bevyengine.org James Liu: james@bevyengine.org Please Donate! # With these changes, there has never been a better time to donate to the Bevy Foundation . In addition to the default monthly donations, custom one-time donations are also now open, so if you were waiting for those ... now is the time to hop aboard! Bevy will always be free and open-source, but it isn't free to make! Because Bevy is free, we rely on the generosity of the Bevy community to fund our efforts. If you are a happy user of Bevy or you believe in our mission, please consider donating to the Bevy Foundation ... every bit helps! Donate import { enable_image_compare } from '/components.js';

  document.addEventListener("DOMContentLoaded", function () {
    enable_image_compare();
  });

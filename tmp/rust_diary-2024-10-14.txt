https://github.com/pyrohost/clavis
-->>-->>
Repository files navigation README MIT license Clavis Clavis is a Rust library that facilitates secure, encrypted communication over asynchronous streams. Leveraging X25519 for key exchange and AES-256-GCM-SIV for encryption, Clavis ensures the confidentiality and integrity of transmitted data while providing a simple and efficient API . Table of Contents Features Installation Getting Started Define Your Packets Establishing a Secure Connection Client Example Server Example API Documentation Contributing License Installation Add Clavis to your Cargo.toml : [ dependencies ] clavis = " 0.1.0 " # Replace with the latest version from crates.io Alternatively, to use the latest development version: [ dependencies ] clavis = { git = " https://github.com/pyrohost/clavis.git " } Getting Started Define Your Packets Clavis uses macros to define custom packet types for communication. This allows you to serialize and deserialize your data seamlessly. First, ensure you have the necessary dependencies in your Cargo.toml : [ dependencies ] serde = { version = " 1.0 " , features = [ " derive " ] } clavis = " 0.1.0 " Now, define your packet types: use clavis :: define_user_packets ; use serde :: { Serialize , Deserialize } ; // Define your data structure # [ derive ( Serialize , Deserialize ) ] struct MyData { id : u32 , message : String , // Add other fields as needed } // Use the macro to define your packet define_user_packets ! { MyPacket = 1 => MyData , // PacketName = PacketId => DataType } Establishing a Secure Connection Clavis supports both client and server roles. Below are examples demonstrating how to set up each. Client Example use clavis :: { EncryptedPacketStream , Role , Result } ; use tokio :: net :: TcpStream ; # [ tokio :: main ] async fn main ( ) -> Result < ( ) > { // Connect to the server let stream = TcpStream :: connect ( "127.0.0.1:7272" ) . await ? ; // Initialize the encrypted stream as a client let mut encrypted_stream = EncryptedPacketStream :: new ( stream , Role :: Client , None , None ) . await ? ; // Create a packet with your data let data = MyData { id : 1 , message : "Hello, Server!" . into ( ) , } ; let packet = MyPacket :: new ( data ) ; // Send the packet encrypted_stream . write_packet ( & packet ) . await ? ; println ! ( "Packet sent to the server." ) ; // Await a response if let Some ( response ) = encrypted_stream . read_packet :: < MyPacket > ( ) . await ? { println ! ( "Received response: {:?}" , response ) ; } Ok ( ( ) ) } Server Example use clavis :: { EncryptedPacketStream , Role , Result } ; use tokio :: net :: TcpListener ; # [ tokio :: main ] async fn main ( ) -> Result < ( ) > { // Bind the server to a local address let listener = TcpListener :: bind ( "127.0.0.1:7272" ) . await ? ; println ! ( "Server listening on 127.0.0.1:7272" ) ; loop { // Accept incoming connections let ( stream , addr ) = listener . accept ( ) . await ? ; println ! ( "New connection from {}" , addr ) ; // Spawn a new task for each connection tokio :: spawn ( async move { match handle_client ( stream ) . await { Ok ( _ ) => println ! ( "Connection with {} closed." , addr ) , Err ( e ) => eprintln ! ( "Error handling {}: {:?}" , addr, e ) , } } ) ; } } async fn handle_client ( stream : tokio :: net :: TcpStream ) -> Result < ( ) > { // Initialize the encrypted stream as a server let mut encrypted_stream = EncryptedPacketStream :: new ( stream , Role :: Server , None , None ) . await ? ; // Read a packet from the client if let Some ( packet ) = encrypted_stream . read_packet :: < MyPacket > ( ) . await ? { println ! ( "Received packet: {:?}" , packet ) ; // Respond to the client let response_data = MyData { id : packet . id , message : "Hello, Client!" . into ( ) , } ; let response_packet = MyPacket :: new ( response_data ) ; encrypted_stream . write_packet ( & response_packet ) . await ? ; println ! ( "Response sent to client." ) ; } Ok ( ( ) ) } API Documentation Comprehensive API documentation is available here . Explore the various modules, structs, and functions to leverage the full capabilities of Clavis in your projects. License This project is licensed under the MIT License .
======>
https://medium.com/towards-data-science/nine-rules-for-running-rust-on-embedded-systems-b0c247ee877e
-->>-->>
Nine Rules for Running Rust on Embedded Systems Practical Lessons from Porting range-set-blaze to no_std Carl M. Kadie · Follow Published in Towards Data Science · 16 min read · 1 day ago 69 Listen Share Rust Running on Embedded — Source: https://openai.com/dall-e-2/ . All other figures from the author. Do you want your Rust code to run everywhere — from large servers to web pages, robots, and even watches? In this final article of a three-part series [ 1 , 2 , 3 ], we’ll see how to use Rust to run on embedded devices using no_std . Porting your Rust project to a no_std environment allows you to target microcontrollers and deeply embedded systems, creating highly efficient software for constrained environments. For example, I used the upcoming version of range-set-blaze to create an LED animation sequencer and compositor that runs on a Raspberry Pi Pico: 1 minute video showing LED animation on the Pico Running Rust without the standard library presents unique challenges. Without operating system support, features like file I/O, networking, and sometimes even dynamic memory allocation are unavailable. In this article, we’ll look at practical strategies to overcome these limitations. Porting Rust to no_std requires careful steps and choices, and missing any step can lead to failure. We’ll simplify the process by following these nine rules, which we will examine in detail: Confirm that your project works with WASM WASI and WASM in the Browser. Use target thumbv7m-none-eabi and cargo tree to identify and fix dependencies incompatible with no_std . Mark main (non-test) code no_std and alloc . Replace std:: with core:: and alloc:: . Use Cargo features to let your main code use std optionally for file-related (etc.) functions. Understand why test code always uses the standard library. Create a simple embedded test project. Run it with QEMU. In Cargo.toml , add keywords and categories for WASM and no_std . [Optional] Use preallocated data types to avoid alloc . Add thumbv7m-none-eabi and QEMU to your CI (continuous integration) tests. Aside: These articles are based on a three-hour workshop that I presented at RustConf24 in Montreal. Thanks to the participants of that workshop. A special thanks, also, to the volunteers from the Seattle Rust Meetup who helped test this material. These articles replace an article I wrote last year with updated information. As with the first and second articles in this series, before we look at the rules one by one, let’s define our terms. Native: Your home OS (Linux, Windows, macOS) Standard library (std) : Provides Rust’s core functionality — Vec , String , file input/output, networking, time. WASM : WebAssembly (WASM) is a binary instruction format that runs in most browsers (and beyond). WASI : WebAssembly System Interface (WASI) allows outside-the-browser WASM to access file I/O, networking (not yet), and time handling. no_std : Instructs a Rust program not to use the full standard library, making it suitable for small, embedded devices or highly resource-constrained environments. alloc : Provides heap memory allocation capabilities ( Vec , String , etc.) in no_std environments, essential for dynamically managing memory. Based on my experience with range-set-blaze , a data structure project, here are the decisions I recommend, described one at a time. To avoid wishy-washiness, I’ll express them as rules. Rule 1: Confirm that your project works with WASM WASI and WASM in the Browser. Before porting your Rust code to an embedded environment, ensure it runs successfully in WASM WASI and WASM in the Browser . These environments expose issues related to moving away from the standard library and impose constraints like those of embedded systems. By addressing these challenges early, you’ll be closer to running your project on embedded devices. Aside: If you don’t need your project to also run on native and/or WASM, you can skip this step. You may, however, find some steps from the previous articles useful — for example, running in a 32-bit environment and understanding conditional compilation. Environments in which we wish to run our code as a Venn diagram of progressively tighter constraints. Run the following commands to confirm that your code works in both WASM WASI and WASM in the Browser: cargo test --target wasm32-wasip1 cargo test --target wasm32-unknown-unknown If the tests fail or don’t run, revisit the steps from the earlier articles in this series: WASM WASI and WASM in the Browser . The WASM WASI article also provides crucial background on understanding Rust targets (Rule 2), conditional compilation (Rule 4), and Cargo features (Rule 6). Once you’ve fulfilled these prerequisites, the next step is to see how (and if) we can get our dependencies working on embedded systems. Rule 2: Use target thumbv7m-none-eabi and cargo tree to identify and fix dependencies incompatible with no_std . To check if your dependencies are compatible with an embedded environment, compile your project for an embedded target. I recommend using the thumbv7m-none-eabi target: thumbv7m — Represents the ARM Cortex-M3 microcontroller, a popular family of embedded processors. none — Indicates that there is no operating system (OS) available. In Rust, this typically means we can’t rely on the standard library ( std ), so we use no_std . Recall that the standard library provides core functionality like Vec , String , file input/output, networking, and time. eabi — Embedded Application Binary Interface, a standard defining calling conventions, data types, and binary layout for embedded executables. Since most embedded processors share the no_std constraint, ensuring compatibility with this target helps ensure compatibility with other embedded targets. Install the target and check your project: rustup target add thumbv7m-none-eabi cargo check --target thumbv7m-none-eabi When I did this on range-set-blaze , I encountered errors complaining about dependencies, such as: This shows that my project depends on num-traits , which depends on either , ultimately depending on std . The error messages can be confusing. To better understand the situation, run this cargo tree command: cargo tree --edges no-dev --format "{p} {f}" It displays a recursive list of your project’s dependencies and their active Cargo features. For example: range-set-blaze v0.1.6 (C:\deldir\branches\rustconf24.nostd) ├── gen_ops v0.3.0 ├── itertools v0.13.0 default,use_alloc,use_std │   └── either v1.12.0 use_std ├── num-integer v0.1.46 default,std │   └── num-traits v0.2.19 default,i128,std │       [build-dependencies] │       └── autocfg v1.3.0 └── num-traits v0.2.19 default,i128,std (*) We see multiple occurrences of Cargo features named use_std and std , strongly suggesting that: These Cargo features require the standard library. We can turn these Cargo features off. Using the techniques explained in the first article , Rule 6, we disable the use_std and std Cargo features. Recall that Cargo features are additive and have defaults. To turn off the default features, we use default-features = false . We then enable the Cargo features we want to keep by specifying, for example, features = ["use_alloc"] . The Cargo.toml now reads: [dependencies] gen_ops = "0.3.0" itertools = { version = "0.13.0" , features=[ "use_alloc" ], default-features = false } num-integer = { version = "0.1.46" , default-features = false } num-traits = { version = "0.2.19" , features=[ "i128" ], default-features = false } Turning off Cargo features will not always be enough to make your dependencies no_std -compatible. For example, the popular thiserror crate introduces std into your code and offers no Cargo feature to disable it. However, the community has created no_std alternatives. You can find these alternatives by searching, for example, https://crates.io/search?q=thiserror+no_std . In the case of range-set-blaze , a problem remained related to crate gen_ops — a wonderful crate for conveniently defining operators such as + and & . The crate used std but didn’t need to. I identified the required one-line change (using the methods we'll cover in Rule 3) and submitted a pull request. The maintainer accepted it, and they released an updated version: 0.4.0 . Sometimes, our project can’t disable std because we need capabilities like file access when running on a full operating system. On embedded systems, however, we're willing—and indeed must—give up such capabilities. In Rule 4, we'll see how to make std usage optional by introducing our own Cargo features. Using these methods fixed all the dependency errors in range-set-blaze . However, resolving those errors revealed 281 errors in the main code. Progress! Rule 3: Mark main (non-test) code no_std and alloc . Replace std:: with core:: and alloc:: . At the top of your project’s lib.rs (or main.rs ) add: #![no_std] extern crate alloc; This means we won’t use the standard library, but we will still allocate memory. For range-set-blaze , this change reduced the error count from 281 to 52. Many of the remaining errors are due to using items in std that are available in core or alloc . Since much of std is just a re-export of core and alloc , we can resolve many errors by switching std references to core or alloc . This allows us to keep the essential functionality without relying on the standard library. For example, we get an error for each of these lines: use std::cmp::max; use std::cmp::Ordering; use std::collections::BTreeMap; Changing std:: to either core:: or (if memory related) alloc:: fixes the errors: use core::cmp::max; use core::cmp::Ordering; use alloc::collections::BTreeMap; Some capabilities, such as file access, are std -only—that is, they are defined outside of core and alloc . Fortunately, for range-set-blaze , switching to core and alloc resolved all 52 errors in the main code. However, this fix revealed 89 errors in its test code. Again, progress! Aside: You can also find places where std could be alloc or core via Clippy rules . We’ll address errors in the test code in Rule 5, but first, let’s figure out what to do if we need capabilities like file access when running on a full operating system. Rule 4: Use Cargo features to let your main code use std optionally for file-related (etc.) functions. If we need two versions of our code — one for running on a full operating system and one for embedded systems — we can use Cargo features (see Rule 6 in the first article ). For example, let’s define a feature called foo , which will be the default. We'll include the function demo_read_ranges_from_file only when foo is enabled. In Cargo.toml (preliminary): [features] default = [ "foo" ] foo = [] In lib.rs (preliminary): #![no_std] extern crate alloc; // ... #[cfg(feature = "foo" )] pub fn demo_read_ranges_from_file <P, T>(path: P) -> std::io:: Result <RangeSetBlaze<T>> where P: AsRef <std::path::Path>, T: FromStr + Integer, { todo!( "This function is not yet implemented." ); } This says to define function demo_read_ranges_from_file only when Cargo feature foo is enabled. We can now check various versions of our code: cargo check # enables "foo", the default Cargo features cargo check --features foo # also enables "foo" cargo check --no-default-features # enables nothing Now let’s give our Cargo feature a more meaningful name by renaming foo to std . Our Cargo.toml (intermediate) now looks like: [features] default = [ "std" ] std = [] In our lib.rs , we add these lines near the top to bring in the std library when the std Cargo feature is enabled: #[cfg(feature = "std" )] extern crate std; So, lib.rs (final) looks like this: #![no_std] extern crate alloc; #[cfg(feature = "std" )] extern crate std; // ... #[cfg(feature = "std" )] pub fn demo_read_ranges_from_file <P, T>(path: P) -> std::io:: Result <RangeSetBlaze<T>> where P: AsRef <std::path::Path>, T: FromStr + Integer, { todo!( "This function is not yet implemented." ); } We’d like to make one more change to our Cargo.toml . We want our new Cargo feature to control dependencies and their features. Here is the resulting Cargo.toml (final): [features] default = [ "std" ] std = [ "itertools/use_std" , "num-traits/std" , "num-integer/std" ] [dependencies] itertools = { version = "0.13.0" , features = [ "use_alloc" ], default-features = false } num-integer = { version = "0.1.46" , default-features = false } num-traits = { version = "0.2.19" , features = [ "i128" ], default-features = false } gen_ops = "0.4.0" Aside: If you’re confused by the Cargo.toml format for specifying dependencies and features, see my recent article: Nine Rust Cargo.toml Wats and Wat Nots: Master Cargo.toml formatting rules and avoid frustration in Towards Data Science. To check that your project compiles both with the standard library ( std ) and without, use the following commands: cargo check # std cargo check --no-default-features # no_std With cargo check working, you’d think that cargo test would be straight forward. Unfortunately, it’s not. We’ll look at that next. Rule 5: Understand why test code always uses the standard library. When we compile our project with --no-default-features , it operates in a no_std environment. However, Rust's testing framework always includes the standard library, even in a no_std project. This is because cargo test requires std ; for example, the #[test] attribute and the test harness itself are defined in the standard library. As a result, running: # DOES NOT TEST `no_std` cargo test --no-default-features does not actually test the no_std version of your code. Functions from std that are unavailable in a true no_std environment will still be accessible during testing. For instance, the following test will compile and run successfully with --no-default-features , even though it uses std::fs : #[test] fn test_read_file_metadata () { let metadata = std::fs:: metadata ( "./" ). unwrap (); assert! (metadata. is_dir ()); } Additionally, when testing in std mode, you may need to add explicit imports for features from the standard library. This is because, even though std is available during testing, your project is still compiled as #![no_std] , meaning the standard prelude is not automatically in scope. For example, you’ll often need the following imports in your test code: #![cfg(test)] use std::prelude::v1::*; use std::{format, print, println, vec}; These imports bring in the necessary utilities from the standard library so that they are available during testing. To genuinely test your code without the standard library, you’ll need to use alternative methods that do not rely on cargo test . We'll explore how to run no_std tests in the next rule. Rule 6: Create a simple embedded test project. Run it with QEMU. You can’t run your regular tests in an embedded environment. However, you can — and should — run at least one embedded test. My philosophy is that even a single test is infinitely better than none. Since “if it compiles, it works” is generally true for no_std projects, one (or a few) well-chosen test can be quite effective. Aside: There is hope for running tests in a more normal fashion [ 1 ][ 2 ]. As far as I know, nothing works easily. If this changes, please let me know and I’ll update this section. To run this test, we use QEMU (Quick Emulator, pronounced “cue-em-you”), which allows us to emulate thumbv7m-none-eabi code on our main operating system (Linux, Windows, or macOS). Install QEMU. See the QEMU download page for full information: Linux/WSL Ubuntu: sudo apt-get install qemu-system Arch: sudo pacman -S qemu-system-arm Fedora: sudo dnf install qemu-system-arm Windows Method 1: https://qemu.weilnetz.de/w64 . Run the installer (tell Windows that it is OK). Add "C:\Program Files\qemu\" to your path. Method 2: Install MSYS2 from https://www.msys2.org/ . Open MSYS2 UCRT64 terminal. pacman -S mingw-w64-x86_64-qemu . Add C:\msys64\mingw64\bin\ to your path. Mac brew install qemu or sudo port install qemu Test installation with: qemu-system-arm --version Create an embedded subproject. Create a subproject for the embedded tests: cargo new tests/embedded This command generates a new subproject, including the configuration file at tests/embedded/Cargo.toml . Aside : This command also modifies your top-level Cargo.toml to add the subproject to your workspace. In Rust, a workspace is a collection of related packages defined in the [workspace] section of the top-level Cargo.toml . All packages in the workspace share a single Cargo.lock file, ensuring consistent dependency versions across the entire workspace. Edit tests/embedded/Cargo.toml to look like this, but replace "range-set-blaze" with the name of your top-level project: [package] name = "embedded" version = "0.1.0" edition = "2021" [dependencies] alloc-cortex-m = "0.4.4" cortex-m = "0.7.7" cortex-m-rt = "0.7.3" cortex-m-semihosting = "0.5.0" panic-halt = "0.2.0" # Change to refer to your top-level project range-set-blaze = { path = "../.." , default-features = false } Update the test code. Replace the contents of tests/embedded/src/main.rs with: // Based on https://github.com/rust-embedded/cortex-m-quickstart/blob/master/examples/allocator.rs // and https://github.com/rust-lang/rust/issues/51540 #![feature(alloc_error_handler)] #![no_main] #![no_std] extern crate alloc; use alloc::string:: ToString ; use alloc_cortex_m::CortexMHeap; use core::{alloc::Layout, iter::FromIterator}; use cortex_m::asm; use cortex_m_rt::entry; use cortex_m_semihosting::{debug, hprintln}; use panic_halt as _; #[global_allocator] static ALLOCATOR: CortexMHeap = CortexMHeap:: empty (); const HEAP_SIZE: usize = 1024 ; // in bytes #[alloc_error_handler] fn alloc_error (_layout: Layout) -> ! { asm:: bkpt (); loop {} } #[entry] fn main () -> ! { unsafe { ALLOCATOR. init (cortex_m_rt:: heap_start () as usize , HEAP_SIZE) } // Test(s) goes here. Run only under emulation use range_set_blaze::RangeSetBlaze; let range_set_blaze = RangeSetBlaze:: from_iter ([ 100 , 103 , 101 , 102 , - 3 , - 4 ]); hprintln!( "{:?}" , range_set_blaze. to_string ()); if range_set_blaze. to_string () != "-4..=-3, 100..=103" { debug:: exit (debug::EXIT_FAILURE); } debug:: exit (debug::EXIT_SUCCESS); loop {} } Most of this main.rs code is embedded system boilerplate. The actual test code is: use range_set_blaze::RangeSetBlaze; let range_set_blaze = RangeSetBlaze:: from_iter ([ 100 , 103 , 101 , 102 , - 3 , - 4 ]); hprintln!( "{:?}" , range_set_blaze. to_string ()); if range_set_blaze. to_string () != "-4..=-3, 100..=103" { debug:: exit (debug::EXIT_FAILURE); } If the test fails, it returns EXIT_FAILURE ; otherwise, it returns EXIT_SUCCESS . We use the hprintln! macro to print messages to the console during emulation. Since this is an embedded system, the code ends in an infinite loop to run continuously. Add supporting files. Before you can run the test, you must add two files to the subproject: build.rs and memory.x from the Cortex-M quickstart repository : Linux/WSL/macOS cd tests/embedded wget https://raw.githubusercontent.com/rust-embedded/cortex-m-quickstart/master/build.rs wget https://raw.githubusercontent.com/rust-embedded/cortex-m-quickstart/master/memory. Windows (Powershell) cd tests/embedded Invoke-WebRequest -Uri 'https://raw.githubusercontent.com/rust-embedded/cortex-m-quickstart/master/build.rs' -OutFile 'build.rs' Invoke-WebRequest -Uri 'https://raw.githubusercontent.com/rust-embedded/cortex-m-quickstart/master/memory.x' -OutFile 'memory.x' Also, create a tests/embedded/.cargo/config.toml with the following content: [target.thumbv7m-none-eabi] runner = "qemu-system-arm -cpu cortex-m3 -machine lm3s6965evb -nographic -semihosting-config enable=on,target=native -kernel" [build] target = "thumbv7m-none-eabi" This configuration instructs Cargo to use QEMU to run the embedded code and sets thumbv7m-none-eabi as the default target for the subproject. Run the test. Run the test with cargo run (not cargo test ): # Setup # Make this subproject 'nightly' to support #![feature(alloc_error_handler)] rustup override set nightly rustup target add thumbv7m-none-eabi # If needed, cd tests/embedded cargo run You should see log messages, and the process should exit without error. In my case, I see: "-4..=-3, 100..=103" . These steps may seem like a significant amount of work just to run one (or a few) tests. However, it’s primarily a one-time effort involving mostly copy and paste. Additionally, it enables running tests in a CI environment (see Rule 9). The alternative — claiming that the code works in a no_std environment without ever actually running it in no_std —risks overlooking critical issues. The next rule is much simpler. Rule 7: In Cargo.toml , add keywords and categories for WASM and no_std . Once your package compiles and passes the additional embedded test, you may want to publish it to crates.io , Rust’s package registry. To let others know that it is compatible with WASM and no_std , add the following keywords and categories to your Cargo.toml file: [package] # ... categories = [ "no-std" , "wasm" , "embedded" ] # + others specific to your package keywords = [ "no_std" , "wasm" ] # + others specific to your package Note that for categories, we use a hyphen in no-std . For keywords, no_std (with an underscore) is more popular than no-std . Your package can have a maximum of five keywords and five categories. Here is a list of categories and keywords of possible interest, along with the number of crates using each term: Category no-std (6884) Category embedded (3455) Category wasm (2026) Category no-std::no-alloc (581) Keyword wasm (1686) Keyword no_std (1351) Keyword no-std (1157) Keyword embedded (925) Keyword webassembly (804) Good categories and keywords will help people find your package, but the system is informal. There’s no mechanism to check whether your categories and keywords are accurate, nor are you required to provide them. Next, we’ll explore one of the most restricted environments you’re likely to encounter. Rule 8: [Optional] Use preallocated data types to avoid alloc . My project, range-set-blaze , implements a dynamic data structure that requires memory allocation from the heap (via alloc ). But what if your project doesn't need dynamic memory allocation? In that case, it can run in even more restricted embedded environments—specifically those where all memory is preallocated when the program is loaded. The reasons to avoid alloc if you can: Completely deterministic memory usage Reduced risk of runtime failures (often caused by memory fragmentation) Lower power consumption There are crates available that can sometimes help you replace dynamic data structures like Vec , String , and HashMap . These alternatives generally require you to specify a maximum size. The table below shows some popular crates for this purpose: I recommend the heapless crate because it provides a collection of data structures that work well together. Here is an example of code — using heapless — related to an LED display. This code creates a mapping from a byte to a list of integers. We limit the number of items in the map and the length of the integer list to DIGIT_COUNT (in this case, 4). use heapless::{LinearMap, Vec }; // … let mut map : LinearMap< u8 , Vec < usize , DIGIT_COUNT>, DIGIT_COUNT> = LinearMap:: new (); // … let mut vec = Vec :: default (); vec. push (index). unwrap (); map. insert (*byte, vec). unwrap (); // actually copies Full details about creating a no_alloc project are beyond my experience. However, the first step is to remove this line (added in Rule 3) from your lib.rs or main.rs : extern crate alloc; // remove this Rule 9: Add thumbv7m-none-eabi and QEMU to your CI (continuous integration) tests. Your project is now compiling to no_std and passing at least one embedded-specific test. Are you done? Not quite. As I said in the previous two articles: If it’s not in CI, it doesn’t exist. Recall that continuous integration (CI) is a system that can automatically run tests every time you update your code. I use GitHub Actions as my CI platform. Here’s the configuration I added to .github/workflows/ci.yml to test my project on embedded platforms: test_thumbv7m_none_eabi: name: Setup and Check Embedded runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v4 - name: Set up Rust uses: dtolnay/rust-toolchain@master with: toolchain: stable target: thumbv7m-none-eabi - name: Install check stable and nightly run: | cargo check --target thumbv7m-none-eabi --no-default-features rustup override set nightly rustup target add thumbv7m-none-eabi cargo check --target thumbv7m-none-eabi --no-default-features sudo apt-get update && sudo apt-get install qemu qemu-system-arm - name: Test Embedded (in nightly) timeout-minutes: 1 run: | cd tests/embedded cargo run By testing embedded and no_std with CI, I can be sure that my code will continue to support embedded platforms in the future. So, there you have it — nine rules for porting your Rust code to embedded. To see a snapshot of the whole range-set-blaze project after applying all nine rules, see this branch on Github . Here is what surprised me about porting to embedded: The Bad: We cannot run our existing tests on embedded systems. Instead, we must create a new subproject and write (a few) new tests. Many popular libraries rely on std , so finding or adapting dependencies that work with no_std can be challenging. The Good: The Rust saying that “if it compiles, it works” holds true for embedded development. This gives us confidence in our code’s correctness without requiring extensive new tests. Although no_std removes our immediate access to the standard library, many items continue to be available via core and alloc . Thanks to emulation, you can develop for embedded systems without hardware. Thank you for joining me on this journey from WASI to WebAssembly in the browser and, finally, to embedded development. Rust has continued to impress me with its ability to run efficiently and safely across environments. As you explore these different domains, I hope you find Rust’s flexibility and power as compelling as I do. Whether you’re working on cloud servers, browsers, or microcontrollers, the tools we’ve discussed will help you tackle the challenges ahead with confidence. Interested in future articles? Please follow me on Medium . I write about Rust and Python, scientific programming, machine learning, and statistics. I tend to write about one article per month.
=====>
https://docs.rs/try-specialize/0.1.1/try_specialize/#alternative-crates
-->>-->>
Repository files navigation README Apache-2.0 license MIT license try-specialize The try-specialize crate provides limited, zero-cost specialization in generic context on stable Rust. use try_specialize :: TrySpecialize ; fn example_specialize_by_value < T > ( value : T ) -> Result < u32 , T > { value . try_specialize ( ) } fn example_specialize_by_ref < T : ? Sized > ( value : & T ) -> Option < & str > { value . try_specialize_ref ( ) } Introduction While specialization in Rust can be a tempting solution in many use cases,
it is usually more idiomatic to use traits instead. Traits are the idiomatic
way to achieve polymorphism in Rust, promoting better code clarity,
reusability, and maintainability. However, specialization can be suitable when you need to optimize
performance by providing specialized implementations for some types without
altering the code logic. It's also useful in specific, type-level
programming use cases like comparisons between types from different
libraries. For a simple use cases, consider the castaway crate, which offers a much
simpler API. On nightly Rust, consider using min_specialization feature
instead. The Rust standard library already uses min_specialization for
many optimizations. For a more detailed comparison, see the Alternative crates section below. About This crate offers a comprehensive API for addressing various specialization
challenges, reducing the need for unsafe code. It provides specialization
from unconstrained types, to unconstrained types, between 'static types,
and between type references and mutable references, and more. Library tests ensure that specializations are
performed at compile time and are fully optimized with no runtime cost at opt-level >= 1 . Note that the release profile uses opt-level = 3 by default. Usage Add this to your Cargo.toml : [ dependencies ] try-specialize = " 0.1.1 " Then, you can use TrySpecialize trait methods like TrySpecialize::try_specialize , TrySpecialize::try_specialize_ref and TrySpecialize::try_specialize_static . To check the possibility of
specialization in advance and use it infallibly multiple times, including
reversed or mapped specialization, use Specialization struct methods. Note that unlike casting, subtyping , and coercion , specialization does
not alter the underlying type or data. It merely qualifies the underlying
types of generics, succeeding only when the underlying types of T1 and T2 are equal. Examples Specialize type to any LifetimeFree type: use try_specialize :: TrySpecialize ; fn func < T > ( value : T ) { match value . try_specialize :: < ( u32 , String ) > ( ) { Ok ( value ) => specialized_impl ( value ) , Err ( value ) => default_impl ( value ) , } } Specialize 'static type to any 'static type: use try_specialize :: TrySpecialize ; fn func < T > ( value : T ) where T : ' static , { match value . try_specialize_static :: < ( u32 , & ' static str ) > ( ) { Ok ( value ) => specialized_impl ( value ) , Err ( value ) => default_impl ( value ) , } } Specialize Sized or Unsized type reference to any LifetimeFree type
reference: use try_specialize :: TrySpecialize ; fn func < T > ( value : & T ) where T : ? Sized , // Relax the implicit `Sized` bound. { match value . try_specialize_ref :: < str > ( ) { Some ( value ) => specialized_impl ( value ) , None => default_impl ( value ) , } } Specialize Sized or Unsized type mutable reference to any LifetimeFree type mutable reference: use try_specialize :: TrySpecialize ; fn func < T > ( value : & mut T ) where T : ? Sized , // Relax the implicit `Sized` bound. { match value . try_specialize_mut :: < [ u8 ] > ( ) { Some ( value ) => specialized_impl ( value ) , None => default_impl ( value ) , } } Specialize a third-party library container with generic types: use try_specialize :: { Specialization , TypeFn } ; fn func < K , V > ( value : hashbrown :: HashMap < K , V > ) { struct MapIntoHashMap ; impl < K , V > TypeFn < ( K , V ) > for MapIntoHashMap { type Output = hashbrown :: HashMap < K , V > ; } if let Some ( spec ) = Specialization :: < ( K , V ) , ( u32 , char ) > :: try_new ( ) { let spec = spec . map :: < MapIntoHashMap > ( ) ; let value : hashbrown :: HashMap < u32 , char > = spec . specialize ( value ) ; specialized_impl ( value ) ; } else { default_impl ( value ) ; } } For a more comprehensive example, see the examples/encode.rs , which
implements custom data encoders and decoders with per-type encoding and
decoding errors and optimized byte array encoding and decoding.
The part of this example related to the Encode implementation for a slice: // ... impl < T > Encode for [ T ] where T : Encode , { type EncodeError = T :: EncodeError ; # [ inline ] fn encode_to < W > ( & self , writer : & mut W ) -> Result < ( ) , Self :: EncodeError > where W : ? Sized + Write , { if let Some ( spec ) = Specialization :: < [ T ] , [ u8 ] > :: try_new ( ) { // Specialize self from `[T; N]` to `[u32; N]` let bytes : & [ u8 ] = spec . specialize_ref ( self ) ; // Map type specialization to its associated error specialization. let spec_err = spec . rev ( ) . map :: < MapToEncodeError > ( ) ; writer . write_all ( bytes ) // Specialize error from `io::Error` to `Self::EncodeError`. . map_err ( |err| spec_err . specialize ( err ) ) ? ; } else { for item in self { item . encode_to ( writer ) ? ; } } Ok ( ( ) ) } } // ... Find values by type in generic composite types: use try_specialize :: { LifetimeFree , TrySpecialize } ; pub trait ConsListLookup { fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree ; } impl ConsListLookup for ( ) { # [ inline ] fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree , { None } } impl < T1 , T2 > ConsListLookup for ( T1 , T2 ) where T2 : ConsListLookup , { # [ inline ] fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree , { self . 0 . try_specialize_ref ( ) . or_else ( || self . 1 . find :: < T > ( ) ) } } # [ derive ( Eq , PartialEq , Debug ) ] struct StaticStr ( & ' static str ) ; // SAFETY: It is safe to implement `LifetimeFree` for structs with no // parameters. unsafe impl LifetimeFree for StaticStr { } let input = ( 123_i32 , ( [ 1_u32 , 2 , 3 , 4 ] , ( 1_i32 , ( StaticStr ( "foo" ) , ( ( 'a' , false ) , ( ) ) ) ) , ) , ) ; assert_eq ! ( input.find::< u32 > ( ) , None ) ; assert_eq ! ( input.find::< i32 > ( ) , Some ( & 123_i32 ) ) ; assert_eq ! ( input.find::< [ u32 ; 4 ] > ( ) , Some ( & [ 1 , 2 , 3 , 4 ] ) ) ; assert_eq ! ( input.find::< [ u32 ] > ( ) , None ) ; assert_eq ! ( input.find::< StaticStr > ( ) , Some ( & StaticStr ( "foo" ) ) ) ; assert_eq ! ( input.find::< char > ( ) , None ) ; assert_eq ! ( input.find::< ( char , bool ) > ( ) , Some ( & ( 'a' , false ) ) ) ; Documentation API Documentation Feature flags alloc (implied by std , enabled by default): enables LifetimeFree implementations for alloc types, like Box , Arc , String , Vec , BTreeMap etc. std (enabled by default): enables alloc feature and LifetimeFree implementations for std types, like OsStr , Path , PathBuf , Instant , HashMap etc. unreliable : enables functions, methods and macros that rely on Rust
standard library undocumented behavior. Refer to the unreliable module
documentation for details. How it works Type comparison between 'static types compares their TypeId::of s. Type comparison between unconstrained and LifetimeFree type treats
them as 'static and compares their TypeId::of s. Specialization relies on type comparison and transmute_copy when the
equality of types is established. Unreliable trait implementation checks are performed using an expected,
but undocumented behavior of the Rust stdlib PartialEq implementation
for Arc<T> . Arc::eq uses fast path comparing references before
comparing data if T implements Eq . Alternative crates castaway : A similar crate with a much simpler macro-based API. The
macro uses Autoref-Based Specialization and automatically determines the
appropriate type of specialization, making it much easier to use. However,
if no specialization is applicable because of the same Autoref-Based
Specialization , the compiler generates completely unclear errors, which
makes it difficult to use it in complex cases. Internally uses unsafe code for type comparison and specialization. coe-rs : Smaller and simpler, but supports only static types and don't
safely combine type equality check and specialization. Internally uses unsafe code for type specialization. downcast-rs : Specialized on trait objects ( dyn ) downcasting. Can't
be used to specialize unconstrained types. syllogism and syllogism_macro : Requires to provide all possible
types to macro that generate a lot of boilerplate code and can't be used
to specialize stdlib types because of orphan rules. specialize : Requires nightly.
Adds a simple macro to inline nightly min_specialization usage into
simple if let expressions. specialized-dispatch : Requires nightly. Adds a macro to inline nightly min_specialization usage into a match -like macro. spez : Specializes expression types, using Autoref-Based
Specialization . It won't works in generic context but can be used in the
code generated by macros. impls : Determine if a type implements a trait. Can't detect erased
type bounds, so not applicable in generic context, but can be used in the
code generated by macros. Comparison of libraries supporting specialization in generic context: crate try-specialize crate castaway crate coe-rs crate downcast-rs crate syllogism min_spec... nightly feature crate specialize crate spec...ch Rust toolchain Stable Stable Stable Stable Stable Nightly Nightly Nightly API complexity Complex Simple Simple Moderate Simple Simple Simple Simple API difficulty Difficult Easy Easy Moderate Moderate Easy Easy Moderate Zero-cost (compile-time optimized) YES YES YES no YES YES YES YES Safely combines type eq check and specialization YES YES no YES YES YES YES YES Specialize value references YES YES YES N/A YES YES YES no Specialize values YES YES no N/A YES YES YES YES Specialize values without consume on failure YES YES no N/A YES YES no YES Limited non-static value specialization YES YES no N/A YES YES YES YES Full non-static value specialization no no no N/A YES no no no Specialize trait objects ( dyn ) N/A N/A N/A YES N/A N/A N/A N/A Compare types without instantiation YES no YES no YES YES YES no Support std types YES YES YES YES no YES YES YES Specialize from unconstrained type YES YES no no no YES YES YES Specialize to unconstrained type YES no no no no YES YES YES Check generic implements "erased" trait YES , but unreliable no no no no YES YES YES Specialize to generic with added bounds no no no no no YES YES YES API based on Traits Macros Traits Macros + Traits Traits Language Macros Macros Type comparison implementation based on TypeId + transmute TypeId + transmute TypeId N/A Traits Language Nightly feature Nightly feature Type casting implementation based on transmute_copy ptr::read transmute std::any::Any Traits Language Nightly feature Nightly feature Implementation free of unsafe no no no YES YES YES YES YES Primitive example of the value specialization using different libraries crate try_specialize : use try_specialize :: TrySpecialize ; fn spec < T > ( value : T ) -> Result < u32 , T > { value . try_specialize ( ) } assert_eq ! ( spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( spec ( "abc" ) , Err ( "abc" ) ) ; crate castaway : use castaway :: cast ; fn spec < T > ( value : T ) -> Result < u32 , T > { cast ! ( value, _ ) } assert_eq ! ( spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( spec ( "abc" ) , Err ( "abc" ) ) ; crate coe-rs : use coe :: { is_same , Coerce } ; // Library don't support non-reference. // specialization. Using reference. fn spec < T > ( value : & T ) -> Option < & u32 > where // Library don't support specialization of // unconstrained non-static types. T : ' static , { is_same :: < u32 , T > ( ) . then ( || value . coerce ( ) ) } fn main ( ) { assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( & "abc" ) , None ) ; } crates downcast-rs : use downcast_rs :: { impl_downcast , DowncastSync } ; trait Base : DowncastSync { } impl_downcast ! ( sync Base ) ; // Library requires all specializable // types to be defined in advance. impl Base for u32 { } impl Base for i32 { } impl Base for & ' static str { } // Library support only trait objects (`dyn`). fn spec ( value : & dyn Base ) -> Option < & u32 > { value . downcast_ref :: < u32 > ( ) } fn main ( ) { assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( & "abc" ) , None ) ; } crate specialize : // Requires nightly. #! [ feature ( min_specialization ) ] use specialize :: constrain ; // Library don't support non-consuming // value specialization. Using reference. fn spec < T : ? Sized > ( value : & T ) -> Option < & u32 > { constrain ! ( ref value as u32 ) } assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( "abc" ) , None ) ; crate specialized-dispatch : // Requires nightly. #! [ feature ( min_specialization ) ] use specialized_dispatch :: specialized_dispatch ; // The library don't support using generics. // from outer item. Using `Option`. fn spec < T > ( value : T ) -> Option < u32 > { specialized_dispatch ! { T -> Option < u32 >, fn ( value: u32 ) => Some ( value ) , default fn < T > ( _: T ) => None ,
        value, } } assert_eq ! ( spec ( 42_u32 ) , Some ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , None ) ; assert_eq ! ( spec ( "abc" ) , None ) ; crates syllogism and syllogism_macro : use syllogism :: { Distinction , Specialize } ; use syllogism_macro :: impl_specialization ; // Library specialization can not be // implemented for std types because of // orphan rules. Using custom local types. # [ derive ( Eq , PartialEq , Debug ) ] struct U32 ( u32 ) ; # [ derive ( Eq , PartialEq , Debug ) ] struct I32 ( i32 ) ; # [ derive ( Eq , PartialEq , Debug ) ] struct Str < ' a > ( & ' a str ) ; // Library requires all specializable // types to be defined in one place. impl_specialization ! ( type U32 ; type I32 ; type Str < ' a>; ) ; fn spec < T > ( value : T ) -> Result < U32 , T > where T : Specialize < U32 > , { match value . specialize ( ) { Distinction :: Special ( value ) => Ok ( value ) , Distinction :: Generic ( value ) => Err ( value ) , } } assert_eq ! ( spec ( U32 ( 42 ) ) , Ok ( U32 ( 42 ) ) ) ; assert_eq ! ( spec ( I32 ( 42_i32 ) ) , Err ( I32 ( 42 ) ) ) ; assert_eq ! ( spec ( Str ( "abc" ) ) , Err ( Str ( "abc" ) ) ) ; min_specialization nightly feature: // Requires nightly. #! [ feature ( min_specialization ) ] // The artificial example looks a bit long. // More real-world use cases are usually // on the contrary more clear and understandable. pub trait Spec : Sized { fn spec ( self ) -> Result < u32 , Self > ; } impl < T > Spec for T { default fn spec ( self ) -> Result < u32 , Self > { Err ( self ) } } impl Spec for u32 { fn spec ( self ) -> Result < u32 , Self > { Ok ( self ) } } assert_eq ! ( Spec ::spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( Spec ::spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( Spec ::spec ( "abc" ) , Err ( "abc" ) ) ; License Licensed under either of Apache License, Version 2.0 ( LICENSE-APACHE or https://www.apache.org/licenses/LICENSE-2.0 ) MIT license ( LICENSE-MIT or https://opensource.org/licenses/MIT ) at your option. Contribution Unless you explicitly state otherwise, any contribution intentionally
submitted for inclusion in the work by you, as defined in the Apache-2.0
license, shall be dual licensed as above, without any
additional terms or conditions.
======>
https://github.com/zheland/try-specialize?tab=readme-ov-file#alternative-crates
-->>-->>
Repository files navigation README Apache-2.0 license MIT license try-specialize The try-specialize crate provides limited, zero-cost specialization in generic context on stable Rust. use try_specialize :: TrySpecialize ; fn example_specialize_by_value < T > ( value : T ) -> Result < u32 , T > { value . try_specialize ( ) } fn example_specialize_by_ref < T : ? Sized > ( value : & T ) -> Option < & str > { value . try_specialize_ref ( ) } Introduction While specialization in Rust can be a tempting solution in many use cases,
it is usually more idiomatic to use traits instead. Traits are the idiomatic
way to achieve polymorphism in Rust, promoting better code clarity,
reusability, and maintainability. However, specialization can be suitable when you need to optimize
performance by providing specialized implementations for some types without
altering the code logic. It's also useful in specific, type-level
programming use cases like comparisons between types from different
libraries. For a simple use cases, consider the castaway crate, which offers a much
simpler API. On nightly Rust, consider using min_specialization feature
instead. The Rust standard library already uses min_specialization for
many optimizations. For a more detailed comparison, see the Alternative crates section below. About This crate offers a comprehensive API for addressing various specialization
challenges, reducing the need for unsafe code. It provides specialization
from unconstrained types, to unconstrained types, between 'static types,
and between type references and mutable references, and more. Library tests ensure that specializations are
performed at compile time and are fully optimized with no runtime cost at opt-level >= 1 . Note that the release profile uses opt-level = 3 by default. Usage Add this to your Cargo.toml : [ dependencies ] try-specialize = " 0.1.1 " Then, you can use TrySpecialize trait methods like TrySpecialize::try_specialize , TrySpecialize::try_specialize_ref and TrySpecialize::try_specialize_static . To check the possibility of
specialization in advance and use it infallibly multiple times, including
reversed or mapped specialization, use Specialization struct methods. Note that unlike casting, subtyping , and coercion , specialization does
not alter the underlying type or data. It merely qualifies the underlying
types of generics, succeeding only when the underlying types of T1 and T2 are equal. Examples Specialize type to any LifetimeFree type: use try_specialize :: TrySpecialize ; fn func < T > ( value : T ) { match value . try_specialize :: < ( u32 , String ) > ( ) { Ok ( value ) => specialized_impl ( value ) , Err ( value ) => default_impl ( value ) , } } Specialize 'static type to any 'static type: use try_specialize :: TrySpecialize ; fn func < T > ( value : T ) where T : ' static , { match value . try_specialize_static :: < ( u32 , & ' static str ) > ( ) { Ok ( value ) => specialized_impl ( value ) , Err ( value ) => default_impl ( value ) , } } Specialize Sized or Unsized type reference to any LifetimeFree type
reference: use try_specialize :: TrySpecialize ; fn func < T > ( value : & T ) where T : ? Sized , // Relax the implicit `Sized` bound. { match value . try_specialize_ref :: < str > ( ) { Some ( value ) => specialized_impl ( value ) , None => default_impl ( value ) , } } Specialize Sized or Unsized type mutable reference to any LifetimeFree type mutable reference: use try_specialize :: TrySpecialize ; fn func < T > ( value : & mut T ) where T : ? Sized , // Relax the implicit `Sized` bound. { match value . try_specialize_mut :: < [ u8 ] > ( ) { Some ( value ) => specialized_impl ( value ) , None => default_impl ( value ) , } } Specialize a third-party library container with generic types: use try_specialize :: { Specialization , TypeFn } ; fn func < K , V > ( value : hashbrown :: HashMap < K , V > ) { struct MapIntoHashMap ; impl < K , V > TypeFn < ( K , V ) > for MapIntoHashMap { type Output = hashbrown :: HashMap < K , V > ; } if let Some ( spec ) = Specialization :: < ( K , V ) , ( u32 , char ) > :: try_new ( ) { let spec = spec . map :: < MapIntoHashMap > ( ) ; let value : hashbrown :: HashMap < u32 , char > = spec . specialize ( value ) ; specialized_impl ( value ) ; } else { default_impl ( value ) ; } } For a more comprehensive example, see the examples/encode.rs , which
implements custom data encoders and decoders with per-type encoding and
decoding errors and optimized byte array encoding and decoding.
The part of this example related to the Encode implementation for a slice: // ... impl < T > Encode for [ T ] where T : Encode , { type EncodeError = T :: EncodeError ; # [ inline ] fn encode_to < W > ( & self , writer : & mut W ) -> Result < ( ) , Self :: EncodeError > where W : ? Sized + Write , { if let Some ( spec ) = Specialization :: < [ T ] , [ u8 ] > :: try_new ( ) { // Specialize self from `[T; N]` to `[u32; N]` let bytes : & [ u8 ] = spec . specialize_ref ( self ) ; // Map type specialization to its associated error specialization. let spec_err = spec . rev ( ) . map :: < MapToEncodeError > ( ) ; writer . write_all ( bytes ) // Specialize error from `io::Error` to `Self::EncodeError`. . map_err ( |err| spec_err . specialize ( err ) ) ? ; } else { for item in self { item . encode_to ( writer ) ? ; } } Ok ( ( ) ) } } // ... Find values by type in generic composite types: use try_specialize :: { LifetimeFree , TrySpecialize } ; pub trait ConsListLookup { fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree ; } impl ConsListLookup for ( ) { # [ inline ] fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree , { None } } impl < T1 , T2 > ConsListLookup for ( T1 , T2 ) where T2 : ConsListLookup , { # [ inline ] fn find < T > ( & self ) -> Option < & T > where T : ? Sized + LifetimeFree , { self . 0 . try_specialize_ref ( ) . or_else ( || self . 1 . find :: < T > ( ) ) } } # [ derive ( Eq , PartialEq , Debug ) ] struct StaticStr ( & ' static str ) ; // SAFETY: It is safe to implement `LifetimeFree` for structs with no // parameters. unsafe impl LifetimeFree for StaticStr { } let input = ( 123_i32 , ( [ 1_u32 , 2 , 3 , 4 ] , ( 1_i32 , ( StaticStr ( "foo" ) , ( ( 'a' , false ) , ( ) ) ) ) , ) , ) ; assert_eq ! ( input.find::< u32 > ( ) , None ) ; assert_eq ! ( input.find::< i32 > ( ) , Some ( & 123_i32 ) ) ; assert_eq ! ( input.find::< [ u32 ; 4 ] > ( ) , Some ( & [ 1 , 2 , 3 , 4 ] ) ) ; assert_eq ! ( input.find::< [ u32 ] > ( ) , None ) ; assert_eq ! ( input.find::< StaticStr > ( ) , Some ( & StaticStr ( "foo" ) ) ) ; assert_eq ! ( input.find::< char > ( ) , None ) ; assert_eq ! ( input.find::< ( char , bool ) > ( ) , Some ( & ( 'a' , false ) ) ) ; Documentation API Documentation Feature flags alloc (implied by std , enabled by default): enables LifetimeFree implementations for alloc types, like Box , Arc , String , Vec , BTreeMap etc. std (enabled by default): enables alloc feature and LifetimeFree implementations for std types, like OsStr , Path , PathBuf , Instant , HashMap etc. unreliable : enables functions, methods and macros that rely on Rust
standard library undocumented behavior. Refer to the unreliable module
documentation for details. How it works Type comparison between 'static types compares their TypeId::of s. Type comparison between unconstrained and LifetimeFree type treats
them as 'static and compares their TypeId::of s. Specialization relies on type comparison and transmute_copy when the
equality of types is established. Unreliable trait implementation checks are performed using an expected,
but undocumented behavior of the Rust stdlib PartialEq implementation
for Arc<T> . Arc::eq uses fast path comparing references before
comparing data if T implements Eq . Alternative crates castaway : A similar crate with a much simpler macro-based API. The
macro uses Autoref-Based Specialization and automatically determines the
appropriate type of specialization, making it much easier to use. However,
if no specialization is applicable because of the same Autoref-Based
Specialization , the compiler generates completely unclear errors, which
makes it difficult to use it in complex cases. Internally uses unsafe code for type comparison and specialization. coe-rs : Smaller and simpler, but supports only static types and don't
safely combine type equality check and specialization. Internally uses unsafe code for type specialization. downcast-rs : Specialized on trait objects ( dyn ) downcasting. Can't
be used to specialize unconstrained types. syllogism and syllogism_macro : Requires to provide all possible
types to macro that generate a lot of boilerplate code and can't be used
to specialize stdlib types because of orphan rules. specialize : Requires nightly.
Adds a simple macro to inline nightly min_specialization usage into
simple if let expressions. specialized-dispatch : Requires nightly. Adds a macro to inline nightly min_specialization usage into a match -like macro. spez : Specializes expression types, using Autoref-Based
Specialization . It won't works in generic context but can be used in the
code generated by macros. impls : Determine if a type implements a trait. Can't detect erased
type bounds, so not applicable in generic context, but can be used in the
code generated by macros. Comparison of libraries supporting specialization in generic context: crate try-specialize crate castaway crate coe-rs crate downcast-rs crate syllogism min_spec... nightly feature crate specialize crate spec...ch Rust toolchain Stable Stable Stable Stable Stable Nightly Nightly Nightly API complexity Complex Simple Simple Moderate Simple Simple Simple Simple API difficulty Difficult Easy Easy Moderate Moderate Easy Easy Moderate Zero-cost (compile-time optimized) YES YES YES no YES YES YES YES Safely combines type eq check and specialization YES YES no YES YES YES YES YES Specialize value references YES YES YES N/A YES YES YES no Specialize values YES YES no N/A YES YES YES YES Specialize values without consume on failure YES YES no N/A YES YES no YES Limited non-static value specialization YES YES no N/A YES YES YES YES Full non-static value specialization no no no N/A YES no no no Specialize trait objects ( dyn ) N/A N/A N/A YES N/A N/A N/A N/A Compare types without instantiation YES no YES no YES YES YES no Support std types YES YES YES YES no YES YES YES Specialize from unconstrained type YES YES no no no YES YES YES Specialize to unconstrained type YES no no no no YES YES YES Check generic implements "erased" trait YES , but unreliable no no no no YES YES YES Specialize to generic with added bounds no no no no no YES YES YES API based on Traits Macros Traits Macros + Traits Traits Language Macros Macros Type comparison implementation based on TypeId + transmute TypeId + transmute TypeId N/A Traits Language Nightly feature Nightly feature Type casting implementation based on transmute_copy ptr::read transmute std::any::Any Traits Language Nightly feature Nightly feature Implementation free of unsafe no no no YES YES YES YES YES Primitive example of the value specialization using different libraries crate try_specialize : use try_specialize :: TrySpecialize ; fn spec < T > ( value : T ) -> Result < u32 , T > { value . try_specialize ( ) } assert_eq ! ( spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( spec ( "abc" ) , Err ( "abc" ) ) ; crate castaway : use castaway :: cast ; fn spec < T > ( value : T ) -> Result < u32 , T > { cast ! ( value, _ ) } assert_eq ! ( spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( spec ( "abc" ) , Err ( "abc" ) ) ; crate coe-rs : use coe :: { is_same , Coerce } ; // Library don't support non-reference. // specialization. Using reference. fn spec < T > ( value : & T ) -> Option < & u32 > where // Library don't support specialization of // unconstrained non-static types. T : ' static , { is_same :: < u32 , T > ( ) . then ( || value . coerce ( ) ) } fn main ( ) { assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( & "abc" ) , None ) ; } crates downcast-rs : use downcast_rs :: { impl_downcast , DowncastSync } ; trait Base : DowncastSync { } impl_downcast ! ( sync Base ) ; // Library requires all specializable // types to be defined in advance. impl Base for u32 { } impl Base for i32 { } impl Base for & ' static str { } // Library support only trait objects (`dyn`). fn spec ( value : & dyn Base ) -> Option < & u32 > { value . downcast_ref :: < u32 > ( ) } fn main ( ) { assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( & "abc" ) , None ) ; } crate specialize : // Requires nightly. #! [ feature ( min_specialization ) ] use specialize :: constrain ; // Library don't support non-consuming // value specialization. Using reference. fn spec < T : ? Sized > ( value : & T ) -> Option < & u32 > { constrain ! ( ref value as u32 ) } assert_eq ! ( spec ( & 42_u32 ) , Some ( & 42 ) ) ; assert_eq ! ( spec ( & 42_i32 ) , None ) ; assert_eq ! ( spec ( "abc" ) , None ) ; crate specialized-dispatch : // Requires nightly. #! [ feature ( min_specialization ) ] use specialized_dispatch :: specialized_dispatch ; // The library don't support using generics. // from outer item. Using `Option`. fn spec < T > ( value : T ) -> Option < u32 > { specialized_dispatch ! { T -> Option < u32 >, fn ( value: u32 ) => Some ( value ) , default fn < T > ( _: T ) => None ,
        value, } } assert_eq ! ( spec ( 42_u32 ) , Some ( 42 ) ) ; assert_eq ! ( spec ( 42_i32 ) , None ) ; assert_eq ! ( spec ( "abc" ) , None ) ; crates syllogism and syllogism_macro : use syllogism :: { Distinction , Specialize } ; use syllogism_macro :: impl_specialization ; // Library specialization can not be // implemented for std types because of // orphan rules. Using custom local types. # [ derive ( Eq , PartialEq , Debug ) ] struct U32 ( u32 ) ; # [ derive ( Eq , PartialEq , Debug ) ] struct I32 ( i32 ) ; # [ derive ( Eq , PartialEq , Debug ) ] struct Str < ' a > ( & ' a str ) ; // Library requires all specializable // types to be defined in one place. impl_specialization ! ( type U32 ; type I32 ; type Str < ' a>; ) ; fn spec < T > ( value : T ) -> Result < U32 , T > where T : Specialize < U32 > , { match value . specialize ( ) { Distinction :: Special ( value ) => Ok ( value ) , Distinction :: Generic ( value ) => Err ( value ) , } } assert_eq ! ( spec ( U32 ( 42 ) ) , Ok ( U32 ( 42 ) ) ) ; assert_eq ! ( spec ( I32 ( 42_i32 ) ) , Err ( I32 ( 42 ) ) ) ; assert_eq ! ( spec ( Str ( "abc" ) ) , Err ( Str ( "abc" ) ) ) ; min_specialization nightly feature: // Requires nightly. #! [ feature ( min_specialization ) ] // The artificial example looks a bit long. // More real-world use cases are usually // on the contrary more clear and understandable. pub trait Spec : Sized { fn spec ( self ) -> Result < u32 , Self > ; } impl < T > Spec for T { default fn spec ( self ) -> Result < u32 , Self > { Err ( self ) } } impl Spec for u32 { fn spec ( self ) -> Result < u32 , Self > { Ok ( self ) } } assert_eq ! ( Spec ::spec ( 42_u32 ) , Ok ( 42 ) ) ; assert_eq ! ( Spec ::spec ( 42_i32 ) , Err ( 42 ) ) ; assert_eq ! ( Spec ::spec ( "abc" ) , Err ( "abc" ) ) ; License Licensed under either of Apache License, Version 2.0 ( LICENSE-APACHE or https://www.apache.org/licenses/LICENSE-2.0 ) MIT license ( LICENSE-MIT or https://opensource.org/licenses/MIT ) at your option. Contribution Unless you explicitly state otherwise, any contribution intentionally
submitted for inclusion in the work by you, as defined in the Apache-2.0
license, shall be dual licensed as above, without any
additional terms or conditions.

======>
https://ngi.eu
-->>-->>
18 Sep: Ammar Bukhari and the MUSAP Project: Empowering Users with Flexible Credentials Management In the bustling tech landscape of Finland, Ammar Bukhari is working at the intersection of digital identity, product management, and customer solutions. Read more 11 Sep: Dominik Beron – Building the Future of Decentralized Identity PostmarketOS is an open-source operating system project designed to extend the lifespan of smartphones founded by Oliver Smith. Read more 27 Jun: Oliver Smith on extending smartphone life-cycles with postmarketOS PostmarketOS is an open-source operating system project designed to extend the lifespan of smartphones founded by Oliver Smith. Read more 02 May: GNU Taler preserving privacy on electronic payment systems using digital coins stored in self-custody on user’s devices Taler Systems SA is a technology company specialising in creating free software for digital payment systems. Our flagship product, the GNU Taler System, is a privacy-respecting digital payment system with built-in age restrictions that allows for fast and secure transactions with anonymity for the buyer. Read more
======>
https://nlnet.nl/news/2024/20241014-announcing-CommonsFund-call.html
-->>-->>
LibrePCB 2.0 — New UI & powerful features for a future-proof LibrePCB LibrePCB is a free and open source electronics design automation (EDA) software suite to develop printed circuit boards (PCBs). It runs on all major platforms and aims to be easy to use, while still beeing able to create professional schematics and PCBs. While it is already used productively by people all around the world, the development of new features became to stuck because of limitations of the current UI concept. To pave the way for new features, a completely new UI will be developed with the goal of having a unified, tabbed window as known and proven by many other applications. In addition, a first attempt of moving from C++ to the safer language Rust will help us to benefit from modern technologies. Together with more import/export capabilities, performance improvements and other frequently requested features the outcome will be released to users by a new major version LibrePCB 2.0. ▸ For more details see : https://nlnet.nl/project/LibrePCB2.0 Test Procedures for MOSFET Open Source SPICE Model Validation — Test Procedures for MOSFET SPICE Model Validation The emergence of open PDK initiatives reduce barriers to entry for integrated circuit (IC) design and manufacturing, serves thelong term goal of promoting academic/industrial collaboration, and stimulate innovation in the field of semiconductor IC design. Open PDKs have the potential to "standardize" PDKs (process design kit), and move away from proprietary/licensed EDA vendor formats. This is needed to democratize open source IC design flow and manufacturing. Open PDKs provide open access to IC design resources. The compact/SPICE models of semiconductor devices are the core of open PDK efforts. SPICE executes implemented Verilog-A compact models. A model of a semiconductor device (passive elements and active, eg: diodes, mosfets, bjts) is primarily a "compact device model". Validation benchmarks are not yet available in the public domain. This project represents the very first attempt to implement these tests for the compact model available in open PDKs. It aims to establish such tests for the compact models in open PDKs, which are intended to be generic enough for model quality assurance testing with FOSS circuit simulators such as GnuCAP, ngspice, xyce, Qucs, among others. ▸ For more details see : https://nlnet.nl/project/MOSFET-testprocedures Verilog-A distiller — Automated porting of models from C to Verilog-A Analog circuit simulators require compact device models in order to be able to simulate circuits. The de-facto standard language for compact device model dissemination is Verilog-A. Many legacy models exist that are coded for the SPICE3 circuit simulator in the C programming language. Manual conversion from C to Verilog-A is resource-intensive, time-consuming, and error-prone. This reduces the accessibility of legacy models and limits innovation. The Verilog-A Distiller project aims to automate conversion of SPICE3 device models from C to Verilog-A. By automating this conversion, we aim to streamline model implementation, reduce development time, and enhance compatibility across different simulators. Verilog-A Distiller is a converter written in Python that utilizes the pycparser library for reading the C code of SPICE3 models. The parsed models are pruned of unnecessary SPICE3-specific parts, upon which Verilog-A code is emitted. Projects like Ngspice put a lot of effort into cleaning up and improving legacy SPICE3 models. Verilog-A Distiller makes these models available across a wide range of simulators that support Verilog-A. ▸ For more details see : https://nlnet.nl/project/Verilog-A-distiller
======>
https://github.com/GyulyVGC/sniffnet
-->>-->>
Repository files navigation README Code of conduct Apache-2.0 license MIT license Security Application to comfortably monitor your Internet traffic Cross-platform, Intuitive, Reliable Translated in: 🇨🇳 🇩🇪 🇫🇷 🇷🇺 🇵🇹 🇪🇦 🇮🇹 🇵🇱 + 12 more languages Support Sniffnet's development 💖 Sniffnet is completely free, open-source software which needs lots of effort and time to develop and maintain. If you appreciate Sniffnet, consider sponsoring :
your support will allow me to dedicate more time to this project,
constantly expanding it including new features and functionalities . A special mention goes to these awesome organizations and folks who are sponsoring Sniffnet: Download 64‑bit | 32‑bit Intel | Apple silicon amd64 | arm64 | i386 | armhf x86_64 | aarch64 Links in the table above will download the latest version of Sniffnet directly from GitHub releases . Note Remember to also install the required dependencies for your operating system. Alternative installation methods are reported in the following: from Crates.io Follow this method only if you have Rust installed on your machine. In this case, the application binary can be built and installed with: cargo install sniffnet --locked from Homebrew You can install Sniffnet Homebrew package with: brew install sniffnet from Nixpkgs You can install Sniffnet Nix package adding the following Nix code to your NixOS Configuration, usually located in /etc/nixos/configuration.nix : environment . systemPackages = [ pkgs . sniffnet ] ; Alternatively, you can install it in your home using Home Manager with: home . packages = [ pkgs . sniffnet ] ; Alternatively, you can try it in a shell with: nix-shell -p sniffnet on Arch Linux You can install Sniffnet community package via pacman : pacman -S sniffnet on FreeBSD You can install Sniffnet port with: pkg install sniffnet on NetBSD You can install Sniffnet from the official repositories via pkgin : pkgin install sniffnet on Tiny Core Linux You can install Sniffnet from the official repository with: tce-load -wi sniffnet Features 💻 choose a network adapter of your PC to inspect 🏷️ select a set of filters to apply to the observed traffic 📖 view overall statistics about your Internet traffic 📈 view real-time charts about traffic intensity 📌 keep an eye on your network even when the application is minimized 📁 export comprehensive capture reports as PCAP files 🔎 identify 6000+ upper layer services , protocols, trojans, and worms 🌐 find out domain name and ASN of the hosts you are exchanging traffic with 🏠 identify connections in your local network 🌍 get information about the country of remote hosts ( IP geolocation ) ⭐ save your favorite network hosts 🕵️‍♂️ search and inspect each of your network connections in real time 🔉 set custom notifications to inform you when defined network events occur 🎨 choose the style that fits you the most, including custom themes support ...and more! User manual Do you want to learn more ? Check out the Sniffnet Wiki , a comprehensive manual to help you
thoroughly master the application from a basic setup to the most advanced functionalities. The Wiki includes step-by-step guides, tips, examples of usage, and answers to frequent questions. Troubleshooting See details Missing dependencies Most of the errors that may arise are likely due to your system missing dependencies
required to correctly analyze a network adapter. Check the required dependencies page for instructions on how to proceed depending on your operating system. Rendering problems In some circumstances, especially if you are running on an old architecture or your graphical drivers are not updated,
the wgpu default renderer used by iced may manifest bugs (the interface glitches, color gradients are unsupported, or some icons are completely black). In these cases you can set an environment variable to switch to the tiny-skia renderer,
a CPU-only software renderer that should work properly on every environment: ICED_BACKEND=tiny-skia In any case, don't hesitate to open an issue , and I will do my best to help you! Acknowledgements A big shout-out to all the contributors of Sniffnet! The graphical user interface has been realized with iced , a cross-platform GUI library for Rust focused on simplicity and type-safety Last but not least, thanks to every single stargazer : all forms of support made it possible to keep improving Sniffnet!
======>
https://github.com/juhaku/utoipa
-->>-->>
Repository files navigation README Apache-2.0 license MIT license utoipa - Auto-generated OpenAPI documentation Pronounced /u:ˈtoʊ:i.pɑ/ or /u:ˈtoʊˌaɪ.piˈeɪ/ whatever works better for you. Want to have your API documented with OpenAPI? But don't want to be bothered
with manual YAML or JSON tweaking? Would like it to be so easy that it would almost
be utopic? Don't worry: utoipa is here to fill this gap. It aims to do, if not all, then
most of the heavy lifting for you, enabling you to focus on writing the actual API logic instead of
documentation. It aims to be minimal , simple and fast . It uses simple proc macros which
you can use to annotate your code to have items documented. The utoipa crate provides auto-generated OpenAPI documentation for Rust REST APIs. It treats
code-first approach as a first class citizen and simplifies API documentation by providing
simple macros for generating the documentation from your code. It also contains Rust types of the OpenAPI spec, allowing you to write the OpenAPI spec only using
Rust if auto generation is not your flavor or does not fit your purpose. Long term goal of the library is to be the place to go when OpenAPI documentation is needed in any Rust
codebase. Utoipa is framework-agnostic, and could be used together with any web framework, or even without one. While
being portable and standalone, one of its key aspects is simple integration with web frameworks. Choose your flavor and document your API with ice-cold IPA Flavor Support actix-web Parse path, path parameters and query parameters, recognize request body and response body. See more at docs axum Parse path and query parameters, recognize request body and response body, utoipa-axum bindings . See more at docs rocket Parse path, path parameters and query parameters, recognize request body and response body. See more at docs Others* Plain utoipa without extra flavor. This gives you all the basic benefits listed below in Features section but with little less automation. Others* = For example warp but could be anything. Refer to the existing examples to find out more. Features OpenAPI 3.1 Pluggable, easy setup and integration with frameworks. No bloat, enable what you need. Support for generic types Note! Tuples, arrays and slices cannot be used as generic arguments on types. Types implementing ToSchema manually should not have generic arguments, as
they are not composeable and will result compile error. Automatic schema collection from usages recursively. Request body from either handler function arguments (if supported by framework) or from request_body attribute. Response body from response body attribute or response content attribute. Various OpenAPI visualization tools supported out of the box. Rust type aliases via utoipa-config . What's up with the word play? The name comes from the words utopic and api where uto are the first three letters of utopic and the ipa is api reversed. Aaand... ipa is also an awesome type of beer 🍺. Crate Features macros Enable utoipa-gen macros. This is enabled by default. yaml : Enables serde_yaml serialization of OpenAPI objects. actix_extras : Enhances actix-web integration with being able to
parse path , path and query parameters from actix web path attribute macros. See docs or examples for more details. rocket_extras : Enhances rocket framework integration with being
able to parse path , path and query parameters from rocket path attribute macros. See docs or examples for more details. axum_extras : Enhances axum framework integration allowing users to use IntoParams without
defining the parameter_in attribute. See docs or examples for more details. debug : Add extra traits such as debug traits to openapi definitions and elsewhere. chrono : Add support for chrono DateTime , Date , NaiveDate , NaiveDateTime , NaiveTime and Duration types. By default these types are parsed to string types with additional format information. format: date-time for DateTime and NaiveDateTime and format: date for Date and NaiveDate according RFC3339 as ISO-8601 . To
override default string representation users have to use value_type attribute to override the type.
See docs for more details. time : Add support for time OffsetDateTime , PrimitiveDateTime , Date , and Duration types.
By default these types are parsed as string . OffsetDateTime and PrimitiveDateTime will use date-time format. Date will use date format and Duration will not have any format. To override default string representation users have to use value_type attribute
to override the type. See docs for more details. decimal : Add support for rust_decimal Decimal type. By default it is interpreted as String . If you wish to change the format you need to override the type.
See the value_type in component derive docs . decimal_float : Add support for rust_decimal Decimal type. By default it is interpreted as Number . This feature is mutually exclusive with decimal and allow to change the default type used in your
documentation for Decimal much like serde_with_float feature exposed by rust_decimal. uuid : Add support for uuid . Uuid type will be presented as String with
format uuid in OpenAPI spec. ulid : Add support for ulid . Ulid type will be presented as String with
format ulid in OpenAPI spec. url : Add support for url . Url type will be presented as String with
format uri in OpenAPI spec. smallvec : Add support for smallvec . SmallVec will be treated as Vec . openapi_extensions : Adds traits and functions that provide extra convenience functions.
See the request_body docs for an example. repr : Add support for repr_serde 's repr(u*) and repr(i*) attributes to unit type enums for
C-like enum representation. See docs for more details. preserve_order : Preserve order of properties when serializing the schema for a component.
When enabled, the properties are listed in order of fields in the corresponding struct definition.
When disabled, the properties are listed in alphabetical order. preserve_path_order : Preserve order of OpenAPI Paths according to order they have been
introduced to the #[openapi(paths(...))] macro attribute. If disabled the paths will be
ordered in alphabetical order. However the operations order under the path will be always constant according to specification indexmap : Add support for indexmap . When enabled IndexMap will be rendered as a map similar to BTreeMap and HashMap . non_strict_integers : Add support for non-standard integer formats int8 , int16 , uint8 , uint16 , uint32 , and uint64 . rc_schema : Add ToSchema support for Arc<T> and Rc<T> types. Note! serde rc feature flag must be enabled separately to allow
serialization and deserialization of Arc<T> and Rc<T> types. See more about serde feature flags . config Enables utoipa-config for the project which allows defining global configuration options for utoipa . Default Library Support Implicit partial support for serde attributes. See docs for more details. Support for http StatusCode in responses. Install Add dependency declaration to Cargo.toml . [ dependencies ] utoipa = " 5 " Examples Create type with ToSchema and use it in #[utoipa::path(...)] that is registered to the OpenApi . use utoipa :: { OpenApi , ToSchema } ; # [ derive ( ToSchema ) ] struct Pet { id : u64 , name : String , age : Option < i32 > , } mod pet_api { /// Get pet by id /// /// Get pet from database by pet id # [ utoipa :: path ( get, path = "/pets/{id}" , responses ( ( status = 200 , description = "Pet found successfully" , body = Pet ) , ( status = NOT_FOUND , description = "Pet was not found" ) ) , params ( ( "id" = u64 , Path , description = "Pet database id to get Pet for" ) , ) ) ] async fn get_pet_by_id ( pet_id : u64 ) -> Result < Pet , NotFound > { Ok ( Pet { id : pet_id , age : None , name : "lightning" . to_string ( ) , } ) } } # [ derive ( OpenApi ) ] # [ openapi ( paths ( pet_api::get_pet_by_id ) ) ] struct ApiDoc ; println ! ( "{}" , ApiDoc ::openapi ( ) .to_pretty_json ( ) .unwrap ( ) ) ; Above example will produce an OpenAPI doc like this: { "openapi" : " 3.1.0 " , "info" : { "title" : " application name from Cargo.toml " , "description" : " description from Cargo.toml " , "contact" : { "name" : " author name from Cargo.toml " , "email" : " author email from Cargo.toml " }, "license" : { "name" : " license from Cargo.toml " }, "version" : " version from Cargo.toml " }, "paths" : { "/pets/{id}" : { "get" : { "tags" : [ " pet_api " ], "summary" : " Get pet by id " , "description" : " Get pet from database by pet id " , "operationId" : " get_pet_by_id " , "parameters" : [
          { "name" : " id " , "in" : " path " , "description" : " Pet database id to get Pet for " , "required" : true , "schema" : { "type" : " integer " , "format" : " int64 " , "minimum" : 0 }
          }
        ], "responses" : { "200" : { "description" : " Pet found successfully " , "content" : { "application/json" : { "schema" : { "$ref" : " #/components/schemas/Pet " }
              }
            }
          }, "404" : { "description" : " Pet was not found " }
        }
      }
    }
  }, "components" : { "schemas" : { "Pet" : { "type" : " object " , "required" : [ " id " , " name " ], "properties" : { "age" : { "type" : [ " integer " , " null " ], "format" : " int32 " }, "id" : { "type" : " integer " , "format" : " int64 " , "minimum" : 0 }, "name" : { "type" : " string " }
        }
      }
    }
  }
} Modify OpenAPI at runtime You can modify generated OpenAPI at runtime either via generated types directly or using Modify trait. Modify generated OpenAPI via types directly. # [ derive ( OpenApi ) ] # [ openapi ( info ( description = "My Api description" ) , ) ] struct ApiDoc ; let mut doc = ApiDoc :: openapi ( ) ; doc . info . title = String :: from ( "My Api" ) ; You can even convert the generated OpenApi to OpenApiBuilder . let builder : OpenApiBuilder = ApiDoc :: openapi ( ) . into ( ) ; See Modify trait for examples on how to modify generated OpenAPI via it. Go beyond the surface See how to serve OpenAPI doc via Swagger UI check utoipa-swagger-ui crate for more details. Browse to examples for more comprehensive examples. Check IntoResponses and ToResponse for examples on deriving responses. More about OpenAPI security in security documentation . Dump generated API doc to file at build time. See issue 214 comment . FAQ Swagger UI returns 404 NotFound from built binary This is highly probably due to RustEmbed not embedding the Swagger UI to the executable. This is natural since the RustEmbed library does not by default embed files on debug builds. To get around this you can do one of the following. Build your executable in --release mode or add debug-embed feature flag to your Cargo.toml for utoipa-swagger-ui . This will enable the debug-emebed feature flag for RustEmbed as well. Read more about this here and here . Find utoipa-swagger-ui feature flags here . How to implement ToSchema for external type? There are few ways around this that are elaborated here in detail . Auto discover for OpenAPI schemas and paths? Currently there is no build in solution to automatically discover the OpenAPI types but for your luck there is a pretty neat crate that
just does this for you called utoipauto . License Licensed under either of Apache 2.0 or MIT license at your option. Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in this crate
by you, shall be dual licensed, without any additional terms or conditions.
======>
https://gosub.io
-->>-->>
A New Web Engine Written From Scratch Gosub is a web browser with its own web engine. This engine is a modular system that allows developers to
                easily plug in their own components to customize the functionality of the engine. This will result in a more
                diverse landscape of browsers in the future. With a pluggable engine, developers can create their own rendering pipeline, their own JavaScript engine without
                a single company or organisation forcing their own agenda on the web. Select your JavaScript engine Customizable Rendering pipeline Plug and play components like bricks
======>
https://github.com/gosub-io/gosub-engine
-->>-->>
Repository files navigation README MIT license Security Gosub: Gateway to Optimized Searching and Unlimited Browsing This repository holds the Gosub browser engine. It will become a standalone library that can be used by other projects
but will ultimately be used by the Gosub browser user-agent. See the About section for more information. Join us at our development Zulip chat ! For more general information you can also join our Discord server . If you are interested in contributing to Gosub, please check out the contribution guide ! About This repository is part of the Gosub browser engine project. This is the main engine that holds the following components: HTML5 tokenizer / parser CSS3 tokenizer / parser Document tree Several APIs for connecting to javascript Configuration store Networking stack Rendering engine JS bridge More will follow as the engine grows. The idea is that this engine will receive some kind of stream of bytes (most likely
from a socket or file) and parse this into a valid HTML5 document tree and CSS stylesheets.
From that point, it can be fed to a renderer engine that will render the document tree into a window, or it can be fed
to a more simplistic engine that will render it in a terminal. JS can be executed on the document tree and the document
tree can be modified by JS. Status This project is in its infancy. There is no usable browser yet. However, you can look at simple html pages and parse
them into a document tree and do some initial rendering. We can parse HTML5 and CSS3 files into a document tree or the respective css tree. This tree can be shown in the terminal
or be rendered in a very unfinished renderer. Our renderer cannot render everything yet, but it can render simple html
pages, sort of. We already implemented other parts of the engine, for a JS engine, networking stack, a configuration store and other
things however these aren't integrated yet. You can try these out by running the respective binary. We can render a part for our own site : How to run Installing dependencies This project uses cargo and rustup . First
you must install rustup at the link provided. After installing rustup , run: $ rustup toolchain install 1.73
$ rustc --version
rustc 1.73.0 (cc66ad468 2023-10-03) Once Rust is installed, run this command to pre-build the dependencies: $ cargo build --release You can run the following binaries: Command Type Description cargo run -r --bin config-store bin A simple test application of the config store for testing purposes cargo run -r --bin css3-parser bin Show the parsed css tree cargo run -r --bin display-text-tree bin A simple parser that will try and return a textual presentation of the website cargo run -r --bin gosub-parser bin The actual html5 parser/tokenizer that allows you to convert html5 into a document tree. cargo run -r --bin html5-parser-test test A test suite that tests all html5lib tests for the treebuilding cargo run -r --bin parser-test test A test suite for the parser that tests specific tests. This will be removed as soon as the parser is completely finished as this tool is for developement only. cargo run -r --bin renderer bin Render a html page (WIP) cargo run -r --bin run-js bin Run a JS file (Note: console and event loop are not yet implemented) For running the binaries, take a look at a quick introduction at /docs/binaries.md Benchmark and test suites To run the tests and benchmark suite, do: make test cargo bench
ls target/criterion/report
index.html Wasm Our engine can also be compiled to WebAssembly. You need to use WasmPack for this. To build the Wasm version, run: wasm-pack build --target web Afterwards you need to serve the small useragent around the wasm version in the wasm/ directory. You can do this by cd wasm
npm run dev # you can also use `bun run dev` To use this demo, you need to enable webgpu in chromium and disable the same origin policy. chromium --disable-web-security --enable-features=Vulkan --enable-unsafe-webgpu --user-data-dir=/tmp/chromium-temp-profile This command works on Linux only, if someone uses Windows or macOS, please open an PR! And then you have it! A browser in a browser: Contributing to the project We welcome contributions to this project but the current status makes that we are spending a lot of time researching,
building small proof-of-concepts and figuring out what needs to be done next. Much time of a contributor at this stage
of the project will be non-coding. We do like to hear from you if you are interested in contributing to the project and you can join us currently at
our Zulip chat !
======>
https://old.reddit.com/r/rust/comments/1g3okjo/introducing_clavis_an_encrypted_dead_simple/
-->>-->>
I'm currently working on a Rust library for implementing dead-simple packet communication over encrypted AES streams.    https://github.com/pyrohost/clavis   

   It's also quite fast, averaging around    100µs    for a key exchange and    50µs   ~ for transmitting packets one way assuming same-device latency, though the benchmarks are somewhat flakey as they use Duplex streams.   

   use clavis::{EncryptedPacketStream, define_user_packets, Role};
use tokio::net::TcpStream;
use serde::{Deserialize, Serialize};

#[derive(Serialize, Deserialize, Debug)]
pub struct PingPongData {
    pub message: String,
}

define_user_packets!(
    Ping = 1 => PingPongData, // PacketName = PacketId => DataType
    Pong = 2 => PingPongData
);

#[tokio::main]
async fn main() -> clavis::Result<()> {
    tracing_subscriber::fmt()
        .with_max_level(tracing::Level::DEBUG)
        .init();

    let stream = TcpStream::connect("127.0.0.1:7272").await?;
    let mut encrypted_stream = EncryptedPacketStream::new(stream, Role::Client, None, None).await?;

    let ping = UserPacket::Ping(PingPongData {
        message: "hello".to_string(),
    });
    encrypted_stream.write_packet(&ping).await?;

    if let UserPacket::Pong(pong) = encrypted_stream.read_packet().await? {
        println!("Received Pong: {:?}", pong);
    }

    Ok(())
}
   

   It's still a    very work in progress library   , but any bug fixes / security patches / improvements are welcome!   
   

======>
https://old.reddit.com/r/rust/comments/1g3i5uh/porting_rust_to_embedded_systems/
-->>-->>
I ported a Rust library to embedded (after porting to WASM). Here are "findings":   

   The Good:   

   
   "If it compiles, it works” works (unlike with WASM)   
   Many items removed by    no_std    are available via    core    and    alloc   .   
   With QEMU, you can develop for embedded systems without hardware.   
   

   The Bad:   

   
   #[test]    doesn't work. Instead, create a new subproject and write a few tests.   
   Finding    no_std   -compatible dependencies can be challenging but    cargo tree    helps.   
   

   I’ve written up my experience in a free article, which include how I setup tests via emulation and other tips:    9 Rules for Running Rust on Embedded Systems   

   Overall, I've found using Rust with embedded fun and have used it in several microcontroller projects.   

   Is there a better way test embedded Rust?   
   

======>
https://old.reddit.com/r/rust/comments/1g3dipy/tryspecialize_a_crate_for_limited_zerocost/
-->>-->>
Hello    r/rust   ,   

   I’d like to introduce my library for specialization that works on stable Rust:    try-specialize   . Basically it's a non-macro, more comprehensive, but also more complex alternative to another great crate    castaway   . You can find a comparison with other crates on    github page    or on    docs.rs page   .   

   try-specialize    library safe API provides specialization between unconstrained types (types without bounds) and    LifetimeFree    types, between    'static    types, between their references and mutable references, and more. Library tests ensure that specializations are performed at compile time and are fully optimized with no runtime cost at    opt-level >= 1    (the release profile uses    opt-level = 3    by default).   

   Quick example:
```rust
use try_specialize::TrySpecialize;   

   fn example_by_value<T>(value: T) -> Result<u32, T> {
    value.try_specialize() // Specializes from    T1    to    T2, where T2: LifetimeFree   
    // Same as:    <T as TrySpecialize>::try_specialize::<u32>(value)   
}   

   fn example_by_ref<T: ?Sized>(value: &T) -> Option<&str> {
    value.try_specialize_ref() // Specializes from    &T1    to    &T2, where T2: LifetimeFree   
    // Same as:    <T as TrySpecialize>::try_specialize_ref::<str>(value)   
}   

   assert_eq!(example_by_value(123_u32), Ok(123_u32));
assert_eq!(example_by_value(123_i32), Err(123_i32));
assert_eq!(example_by_ref("abc"), Some("abc"));
assert_eq!(example_by_ref(&[1_u8, 2, 3]), None);
assert_eq!(example_by_ref(&123_u32), None);
```   

   Please note that in most cases it is usually more idiomatic to use traits instead. Specialization is often only optimal for specific optimization purposes when with traits you cannot achieve the desired outcomes.   

   I would love to hear your feedback!   
   

======>
https://old.reddit.com/r/rust/comments/1g3hd2z/learning_rust_dev_game_engine/
-->>-->>
Hello Everyone!   

   I'm in the path of learning Rust. Reading books and doing the exercises. But I was thinking about the learning process and how could be more interesting if I build something that I like(games).    

   My choice was to build a game, the hard way obviously.   
And would be good to have a reference, like a book, video tutorial from someone doing similar activities.   
I already work with data dev. So I don´t care about the frustration of take a long time to "nothing". I just want to learn the language with something more palpable.    

   I would love to hear from you, more experienced people, the books, videos, tutorials. About developing a game engine in rust :D   

   Thank u   
   

======>
https://medium.com/@ryanrothweiler/i-spent-382-hours-learning-rust-and-all-i-got-was-this-shiny-monkey-0f3163e06921
-->>-->>
I spent 382 hours learning Rust and all I got was this shiny monkey Ryan Rothweiler · Follow 3 min read · Just now -- Listen Share Here is said monkey. I went from zero Rust knowledge, and after 382 hours of writing Rust projects, I wrote a Rust game engine to render this PBR monkey. This year I started learning Rust. This is a quick and small article of what I made and how it went. Monkey? What you see above is a quick and dirty Rust “game engine” rendering Suzanne the blender monkey with a PBR shader. Source code for engine is here . Because this is a project for learning, I did as much work myself as possible. The only crates used were png for image loading, windows-rs for windows support, web-sys, wasm-bindgen, and js-sys for web support. Some cool engine features. Nothing too fancy here. Engine runs on windows and the web. OpenGL rendering on windows. WebGL rendering on… well the web. All gl bindings / loading done myself. PBR shader (using LearnOpenGL.com as the primary source) Matrix / Vector math Architecture to separate platform from engine, separate game code from engine code, and separate rendering backend from engine. DLL hot reloading for windows development. Ramblings on the last 10 months of learning Rust. Some quick background. My current day job is a Unity Senior Software Engineer where I’ve been working for about 8 years. I’ve been working with Unity for over 13 years. I’ve written my own cpp game engine and PBR / IBL renderer, but nothing released or professional. Here is all the Rust projects I’ve worked on this year! Along with how much time spent. Gengar is the name for this game engine. So from start to finish 128 hours building the engine outline, renderer, and support for windows and web. Last Finance is an unfinished and unreleased budgeting app using Tauri + Leptos. Advent of code is 20 days of Rust solutions for advent of code 2023. Repo here. Rust Computer Enhance is an 8086 processor emulator following Casey Muratori’s Computer Enhance class. Repo here. Rust is simply reading the Rust book and following miscellaneous Rust tutorials. Blockchain is Rust blockchain research. And finally Graphite is researching the Graphite.rs editor project, which I hope to contribute to soon. Thoughts on Rust after 382 hours I primarily write C#. That is my main frame of reference. Not as difficult to use as people say. Maybe this is because I have cpp experience, and I typically avoid oop patterns. But the ownership model and borrow checker felt very natural. I never ‘fought’ with the borrow checker. The borrow checker can complain a lot. But also its usually correct in its complaints. Cargo is incredible. I write MANY more test in Rust because Cargo makes testing so smooth. I naturally write test during the development of a feature simply because its easier to test that way over manual testing. Then I get to keep those test forever going forward! Its great! Enums and exhaustive pattern matching feel like super powers compared to C#. This also goes hand in hand with error handling. Which likewise feels great compared to C#. Unity projects especially are full of implicit assumptions about what is or isn’t null. If you don’t make these assumptions then you code quickly becomes 10% business logic and 90% checking for null. The same can be said for exceptions. Better remember to be catching exceptions because maybe one will appear somewhere. In general, Rust feels easier to work with. I need to keep less in my head to grapple with a Rust project. Traits and composition over inheritance means I don’t need to mentally load a tree of overrides to understand a single method. Explicitness over implicitness means the code tells me whats happening. Implicitness requires me to mentally fetch my understanding. And that concludes my thought. Nothing groundbreaking or insightful here. Thanks for reading! Onto the next project!
======>
https://old.reddit.com/r/rust/comments/1g3kh2d/what_nonweb_gui_framework_to_use/
-->>-->>
I want to make a simple cross platform native GUI (some input fields, drop downs and buttons + an embedded terminal window for output from a command I'm wrapping). Comstraints:   

   
   I use Linux, but it needs to also work on Windows with minimum fuss. (I'm making a program to help some relatives who are not as tech savvy.)   
   I dont know web tech (and have little interest in learning it). So Tauri etc is out. No JavaScript or Typescript please.   
   I don't actually enjoy GUI coding (concurrent algorithms/data structures and embedded systems are more my thing), so something that is well documented with good tutorials is a must. (So forget iced, which says on the docs that the docs aren't good yet.)   
   I have past experience with Qt from C++ and consider it... tolerable (but not enjoyable). GTK is just awful to work with in my opinion (plus it is a mess on Windows). So relm is out. (Plus I'm a KDE user, and recent GTK decisions around font rendering have been really annoying.)   
   Something stable is good, I don't want to have to adapt to breaking changes every few months. So forget egui, iced, etc that all document themselves as being unstable or experimental. (I don't expect to need many updates to my project in the future either once implemented.)   
   Something that is easy to cross compile from Linux to Windows would be good, so native bindings to C/C++ might be a worse choice. (I don't have a Windows dev environment but will be testing the program in a VM as needed, hopefully it will mostly just work once it works on Linux.)   
   

   I looked around on    https://areweguiyet.com/   , and it seems to come down to slint or fltk. Even though fltk is binding a C library, it seems it doesn't have a lot of dependencies when targeting Windows (assuming it can be built with mingw).   

   Comparing these two options:   

   
   Neither has a ready made terminal widget, but I think I can get away without full Terminal emulation (so a read only text widget with auto scroll should be enough).   
   Fltk is well proven technology by now. Slint is far newer, higher risk of breaking changes.    
   Slint has a more restrictive license for non-FOSS, but I will be able to release the program as open source, so not a big deal.   
   It would be cool to be able to use slint on embedded for open source (reusable knowledge for future projects, yay), but it looks like that is commercial only? If I'm to learn one rust gui framework I'd prefer if it was one I could use everywhere (desktop, embedded and why not web or mobile as well).    
   It would be best to actually try cross compiling some of their example projects to Windows to see how difficult it is before deciding. So this is an unknown currently.   
   

   Anyone have any other suggestions? Or recommendations for which of these two to use.   
   

======>
https://foundation.rust-lang.org/news/announcing-the-rust-foundation-s-2024-fellows/
-->>-->>
Home About News Resources Membership Grants program Contact Careers Announcing the Rust Foundation’s 2024 Fellows The Rust Foundation is thrilled to introduce our 2024 Community Grants Fellowship cohort. by Rust Foundation Team 14 Oct 2024 The Rust Foundation's Community Grants Program supports Rust programming language maintainers, community members, and organizers via financial awards, travel stipends, and training support. Through the Community Grants Program, we aim to reward and support innovative ideas that will benefit the Rust ecosystem for an increasingly global and diverse set of users. Our Fellowship award is a grant given annually to active members of the Rust programming language who have made meaningful contributions to the Rust Project. During their time in the program, Fellows receive a monthly stipend, support for additional training, and funding for travel to relevant events. This year, three categories of Fellowship were awarded: Community Fellowships: 12-month awards for people working to build Rust communities outside of Western Europe and North America to support work such as organizing communities and events and creating content and training materials for their communities. Project Goal Fellowships 6-month awards (with the potential to extend) for people working on the Rust Project's agreed-upon goals (and sub-goals). Project Fellowships: 12-month awards for members of the Rust Project Teams and Working Groups to support contributions that serve the goals of the Rust Project Teams and Working Groups Today, the Rust Foundation is thrilled to introduce our 2024 Fellowship cohort! Please join us in welcoming… Community Fellowships # Promise Reckon ( @PromiseReckon ) # Promise Reckon is an Embedded system Engineer and a Robotics Instructor at Profix in Canada. He has been a passionate Rustacean since 2022. His focus during the Fellowship year will be promoting Rust adoption in Nigeria through building a vibrant and sustainable Rust Developers Community. This will include arranging meetups and workshops; organising and running Rust training sessions; and collaborating with local technology businesses. Kostiantyn Mysnyk ( @Wandalen ) # Kostiantyn is a Rust developer who loves to hammer out solutions with precision and efficiency. Whether tackling code, educational systems, or organizing events, he focuses on creating value, sharing knowledge, and fostering collaboration to support the growth of the community. Passionate about both technical solutions and bringing people together, he aims to make a meaningful impact through his work and leadership in the Rust ecosystem. As part of his Fellowship with the Rust Foundation, he will focus on organizing Rust boot camps and events in Ukraine to promote community engagement and education. Additionally, he will explore opportunities to integrate Rust into higher education curricula, aiming to broaden its adoption and usage among students and educators. Mordecai Etukudo ( @martcpp ) # Mordecai, also known as Mart, is a software developer from Nigeria who has contributed to several open-source projects, including AI (MetaGPT) and others. He is a student at the University of Benin, studying Marine Engineering, focusing on autopilot systems and zero-knowledge proofs (ZKP) in the maritime sector. Mordecai aims to grow Rust within Africa and help drive the adoption of Rust into university systems. Project Goal Fellowships # Alejandra González ( @blyxyas ) # I'm a programmer, cat lover, and environmentalist. I'm obsessed with performance because I don't think a user should spend hundreds just to enjoy your apps, and the planet shouldn't suffer, either. I joined the open source collective about four years ago, two and a half of those being part of the Rust project and the Clippy team. I will optimize the linting side of things, the diagnostics that the Rust compiler is famous for, and Clippy, our linter. This includes making algorithms to preview the lints that will emit and execute just those to checking that our L1 cache and type sizes are not monstrously slow. Nick Cameron ( @nrc ) # Nick is a freelance engineer and consultant. He's been involved with the Rust project since 2014 and is a former core team member. He has worked in many different areas, most recently as part of the async working group. Nick will improve the documentation of async Rust for developers of all experience levels and backgrounds. That should include revitalizing the async book, improving library and reference docs, and perhaps providing other learning material. Predrag Gruevski (obi1kenobi) # Predrag is an independent software researcher working at the intersection of dev tools, compilers, and databases. He is the author of the cargo-semver-checks linter for semantic versioning in Rust and its underlying Trustfall query engine. Accidental breaking changes in new crate releases are a lose-lose situation: they sap the time and energy of both maintainers and downstream users. The cargo-semver-checks linter can catch many kinds of such breakage and is planned to become part of "cargo publish" itself. Let's resolve the remaining merge blockers so running "cargo update" can become fearless! Manuel Drehwald ( @ZuseZ4 ) # Manuel Drehwald cares about High-Performance Computing, Scientific Computing, and Machine Learning. He works on compiler optimizations and features to support those fields. He is currently a Master's student at the University of Toronto. During the first half of his Fellowship, Manuel will focus on enabling rustc to automatically differentiate Rust code (in the calculus sense). In the second half of his Fellowship, he will work on running Rust functions on GPUs. New LLVM features enable both projects and should support almost arbitrary Rust code, including std and no-std code, basic Rust types, user-defined types, and most dependencies from crates.io . Benno Lossin ( @y86-dev ) # I learned Rust in a university project in 2021; the expressive type system instantly hooked me. At the start of 2022, I noticed the Rust for Linux effort. After viewing the code, I was shocked to see that Mutexes and other kinds of locks needed to be initialized via unsafe. I then solved The Safe Pinned Initialization Problem by creating the pinned-init crate we still use today. Afterward, I continued reviewing code and contributing to various other areas. In 2023, I joined the core team of Rust for Linux, working on making Rust a first citizen language in the Linux kernel. I am primarily working on making Rust more ergonomic in the Linux kernel. I see great potential in adding Field Projections to the Rust language; they come up very often in Rust for Linux. In addition, they allow us to turn currently unsafe APIs into safe ones. I will be working on creating and implementing the RFC. binarycat ( @lolbinarycat ) # I'm a self-taught programmer with experience in many high- and low-level languages. Most of my previous open source contributions have been to nixpkgs . I am working on improving the accessibility, discoverability, and ergonomics of rustdoc search. Project Fellowships # Onur Özkan ( @onur-ozkan ) # I am a self-taught computer scientist specializing in distributed systems, operating systems, and compiler engineering. I am one of the lead maintainers of the Rust Language project, and my responsibilities are mostly related to compiler bootstrapping. I will be working on fixing the download-rustc/ci-rustc issues so we can enable it by default, which will speed up the CI pipelines and shorten compile times for developers. Over the last 5-6 months, I have fixed tens of problems related to it (for ref, you can check all the mentioned PRs from this PR ). After that, I will focus on removing python from bootstrap entrypoint (see https://github.com/rust-lang/rust/issues/94829 ) , improving the LSP/rust-analyzer experience for the library and compiler teams (see https://github.com/rust-lang/rust/pull/120611 ) and improve the bootstrap experience by any means (e.g., fixing bugs, working with other teams, cutting down the external dependencies, etc.).” Jiayan ( @roife ) # I'm a graduate student at Nanjing University in China, with an interest in programming languages, compilers, and program analysis. I started learning Rust in 2021, and it has been my primary language for research projects since last year. I've also been contributing to Rust for a year and am a member of the rust-analyzer contributors team. The focus of my Fellowship year will be contributing to rust-analyzer, improving its stability, and refactoring some modules. I also plan to get involved in rustc development and work on driving some interesting proposals forward. Jason Newcomb ( @Jarcho ) # I've been a maintainer of Clippy since 2022 and have contributed for quite a while longer than that. Most of that time has been spent lowering the false-positive rate and internal refactorings. The fellowship will focus on reducing Clippy's false-positive rate. Notably, this includes interfacing Clippy with the borrow checker and preventing it from making suggestions that violate lifetime rules. Noah Lev Bartell-Mangel ( @camelid ) # Noah Lev Bartell-Mangel has been involved with Rust since 2020 and is a member of the Rustdoc and Compiler Contributors teams. He also enjoys the research side of programming languages and systems. He is an undergraduate student at UMass Amherst and part of the PLASMA (Programming Languages and Systems at Massachusetts) lab. During his Fellowship, Noah Lev is excited to continue his contributions to Rustdoc, const generics, and the project as a whole. With Rustdoc, he will be working to improve its user interface, performance, and reliability. His focus for const generics will be on making const parameters and arguments more powerful as part of the expanded const generics project goal. He is also developing a research idea related to Rust generics. Boxy ( @BoxyUwU ) # Boxy is a compiler engineer who has been working on the Rust compiler since early 2021. Their work has largely focused on the type system, particularly const generics. As part of Boxy's fellowship, they will primarily work on stabilizing the adt_const_params feature and reworking the generic_const_exprs feature so that it can be stabilized at some point in the future. They will also help release new versions of Rust as part of the release team and introduce better documentation for how the type system is implemented in the compiler. @eth3lbert # @eth3lbert is the newest member of the crates.io team. His work primarily focuses on improving both performance and user experience for crates.io . This includes optimizing database queries and improving rendering times, which makes the website faster for all users. The focus of @eth3lbert’s fellowship year will be implementing pagination for the crate versions page to address the long loading issue for crates with numerous versions; optimizing the search functionality and exploring possibilities to enhance the overall search experience on crates.io ; continuing to assist with other Rust-lang projects while contributing to other Rust-related projects whenever possible. Rémy Rakic ( @lqd ) # Rémy Rakic has been interested in Rust since 2013 and started contributing to the project and ecosystem in 2016. He's a member of the compiler contributors team, the compiler performance and polonius working groups, and was involved in the NLL working group. The focus of Rémy's Fellowship year will be to work on the Polonius 2024H2 project goal, continue working on compiler performance and triage as well as the rustc-perf tool itself, but also PR reviews and regular compiler maintainership activities, like issue triage, bisections and minimizations. Chris Denton ( @ChrisDenton ) # I first became interested in Rust around the time 1.0 was released, but I didn't start using it in earnest until a couple of years later. In 2019, I began contributing to Rust itself, which led to becoming a member of the Library Contributors, Crate Maintainers, and Rustup teams. During my Fellowship, I'll continue to contribute to and help maintain Rust's standard library, rust-lang crates, and Rustup, with a focus on Windows support. Deadbeef ( @fee1-dead ) # I've been contributing to the Rust compiler for about three and a half years now, working on implementing language features and pushing them toward stabilization. I've picked up work on const traits since 3 years ago and am continuing my work on it. As part of my fellowship, I will continue to push const traits toward feature completion, aiming for a future where we can use const trait methods on stable. Eric Huss ( @ehuss) # Eric Huss has been involved with the Rust project since 2017. He was drawn to Rust due to its welcoming community and the attraction of a safe systems language that exposed him to new language concepts and can be very productive to use. Eric is currently the lead of the Cargo team (representing the Devtools team on the Leadership Council) and the lead of the lang-docs team. He maintains and works on a variety of projects and infrastructure around the Rust project. The focus of Eric's Fellowship year will be leading the Cargo team and continue to assist with maintenance and new development; continuing participation in other teams, such as the Devtools team and as lead of the Language Documentation team; continuing maintenance of other Rust-lang projects, such as Rust-enhanced, mdBook, cargo-bisect-rustc, rustfix, the RFC repo, and triagebot, continuing assisting other Rust-related projects when possible. Congratulations to these well-deserving grant recipients! Hardship Grants and Event Support Grants Are Available Year-Round # Hardship Grants are financial awards ranging from $500 to $1,500 made to active Rust Project maintainers facing financial hardship. Event Support Grants are financial awards ranging from $100 to $500 made to support events (both physical and virtual). Both categories of grants are open for applications year-round. You can learn more here . Community Grants Program Support # If your organization is interested in supporting the Rust language community, donating to the Community Grants Program is a wonderful way to do so. You can inquire about supporting the program by emailing us at grants@rustfoundation.org . We accept donations from organizations for the Community Grants Program in any amount. Additionally, individuals can support the Community Grants Program through our GitHub Sponsors page. You can learn more about the Community Grants Program here and find the list of 2023 Fellows here . Congratulations again to our 2024 Fellows! Tagged: announcement foundation community grants Previous: Thanks For Joining Us at RustConf 2024 Anti-Bribery and Anti-Corruption Policy | Anti-Trust Policy | Bylaws | Cloud Compute Program | Code of Conduct | Conflict of Interest Policy | Copyright Policy | Intellectual Property Policy | Logo Policy and Media Guide | Privacy Policy | Statement on Global Regulations | Whistleblower Policy

======>
https://old.reddit.com/r/rust/comments/1g3k6i2/my_rustbased_project_will_be_supported_by_the/
-->>-->>
It's such a joy for me to announce that my open source project written in Rust (   Sniffnet    — a network monitoring tool) was    elected to receive fundings from NLnet   .   

   The NLnet Foundation supports organisations and people who contribute to an open internet for all. It funds projects that help fix the internet through open hardware, open software, open standards, open science and open data.   

   Their support is made possible by the NGI Zero Commons Fund, whose aim is to help deliver, mature and scale new internet commons across the whole technology spectrum, from libre silicon to middleware, from P2P infrastructure to convenient end user applications.   

   The NGI Zero Commons Fund exists thanks to financial support from the European Commission's    Next Generation Internet    programme, an initiative to re-imagine and re-engineer the Internet for the third millennium and beyond, shaping a value-centric, human, and inclusive society for all.   

   This is an incredible opportunity for me to further improve and expand my project, and it's a strong signal that awesome organisations exist out there to facilitate open source advancement!   
   

======>
https://smallcultfollowing.com/babysteps/blog/2024/10/14/overwrite-and-pin/
-->>-->>
The precise design of generators is of course an ongoing topic of some controversy. I am not trying to flesh out a true design here or take a position. Mostly I want to show that we can create ergonomic bridges between “must pin” types like generators and “non pin” interfaces like Iterator in an ergonomic way without explicit mentioning of pinning. ↩︎ Boats has argued that, since no existing iterator can support borrows over a yield point, generators might not need to do so either. I don’t agree. I think supporting borrows over yield points is necessary for ergonomics just as it was in futures . ↩︎ Actually for Pin<impl DerefMut<Target: Generator>> . ↩︎ I will say, I use std::mem::forget quite regularly, but mostly to make up for a shortcoming in Drop . I would like it if Drop had a separate method, fn drop_on_unwind(&mut self) , and we invoked that method when unwinding. Most of the time, it would be the same as regular drop, but in some cases it’s useful to have cleanup logic that only runs in the case of unwinding. ↩︎ In contrast, a Pin<&mut T> reference can be safely converted into an &T reference, as evidenced by Pin’s Deref impl . This is because, even if T: !Unpin , a &T reference cannot do anything that is invalid for a pinned value. You can’t swap the underlying value or read from it. ↩︎ Projection is the wonky PL term for “accessing a field”. It’s never made much sense to me, but I don’t have a better term to use, so I’m sticking with it. ↩︎ We have a syntax k#foo for explicitly referred to a keyword foo . It is meant to be used only for keywords that will be added in future Rust editions. However, I sometimes think it’d be neat to internal-ish keywords (like k#pinned ) that are used in desugaring but rarely need to be typed explicitly; you would still be able to write k#pinned if for whatever reason you wanted to. And of course we could later opt to stabilize it as pinned (no prefix required) in a future edition. ↩︎ I tried asking ChatGPT to summarize the post but, when I pasted in my post, it replied, “The message you submitted was too long, please reload the conversation and submit something shorter.” Dang ChatGPT, that’s rude! Gemini at least gave it the old college try . Score one for Google. Plus, it called my post “thought-provoking!” Aww, I’m blushing! ↩︎ The precise design of generators is of course an ongoing topic of some controversy. I am not trying to flesh out a true design here or take a position. Mostly I want to show that we can create ergonomic bridges between “must pin” types like generators and “non pin” interfaces like Iterator in an ergonomic way without explicit mentioning of pinning. ↩︎
======>
https://old.reddit.com/r/rust/comments/1g3kevi/utoipa_500_release_compile_time_openapi_for_rust/
-->>-->>
It has been quite awhile since my last post and    utoipa    has been in relatively slow paced development during the year. However I have now reactivated for another major release of utoipa which brings about a bunch of perks. This is probably the biggest release since its launch and is packed with upgrades users have been yearning for. Make it a bit more utopic than before :rocket:   

   Those already using utoipa might want to look into    Migration Guide    to get a quick review of most fundamental changes and what to expect in the new release.   

   Some highlights added in the utoipa 5.0.0   

   
   Full generic types support. This removes the old    aliases    attribute. From now on generics are supported and types must be declared with all type definitions upon usage.   
   Automatic schema collection from usages. When a schema is declared on request body or response body it will be collected along with possible schema refernces upon declaration to OpenApi with    paths(...)    or in case of axum with    routes!(...)    macro. From now on there is no need to declare schemas with    components(schemas(...))    attribute anymore.   
   Support for Rust type aliases with    utoipa-config    crate. This adds build configuration for utoipa for adding aliases among the other configuration options.   
   Axum bindings    utoipa-axum   . This crate brings axum and utoipa closer together by extending the axum Router functionality reducing duplication that is currently present.   
   Nesting support for OpenApi with    nest(...)    attribute. This allows users to separate OpenApi definitions to separate modules which then can be nested under a given path.   
   Support in    #[utoipa::path(...)]    to allow defining multiple http methods.   
   Better support for tuples and arrays regarding OpenAPI definition, thanks to OpenAPI 3.1   
   All in OpenAPI 3.1, since 5.0.0 utoipa will only be OpenAPI 3.1 compliant which adheres fully to JSON Schema specification. This allows better type definition support in OpenAPI.   
   

   To find out more visit    https://github.com/juhaku/utoipa   

   And for those interested of all changes done for the release you might want to take a look at    https://github.com/juhaku/utoipa/blob/master/CHANGELOG.md   
   

======>
https://old.reddit.com/r/rust/comments/1g3gw0l/gosub_an_open_source_browser_engine_written_in/
-->>-->>
Hi everybody.   

   A year ago we started writing a browser engine from scratch in Rust. Among other goals, we try to create a highly modular engine that allows other developers to build their browser on top. Though we are still a very small team, we managed to get a lot done in the past year, and we are able to render some simple pages.   

   Even though we are not as far as the Servo or Ladybird projects, we find it important that there is a diversity in browser engines, hence the reason for starting one from scratch. We are looking for enthusiastic developers who like to discuss, discover and develop Gosub with us.   

   Find our repository at    https://github.com/gosub-io/gosub-engine    or    https://gosub.io   
   

======>
https://old.reddit.com/r/rust/comments/1g3ni5y/rust_support_in_kde/
-->>-->>
The KDE community is working to improve Rust support in KDE as part of the KDE Goals initiative. We are implementing Rust bindings in KDE and improving support to build third party apps beyond C++.    

   This Sunday, Oct 20th at 18:00 (UTC), the KDE Goals team will be answering your questions live. Post your questions here and I'll make sure they'll answer them.    
   

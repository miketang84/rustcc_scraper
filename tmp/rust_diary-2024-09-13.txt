https://bitfieldconsulting.com/posts/rust-errors-option-result
-->>-->>
Things are always going wrong, as I’m sure you’ve noticed, and that applies to our programs too. Sometimes when a function call asks a question, there’s no answer to return—either because some error happened, or because the correct answer is, simply, “no results”. When there’s no answer So, as a matter of good API design, what should we do in these cases? For example, suppose we’re trying to write a function first , that returns the first element of a given list. If there’s at least one element in the list, there’s no problem. But what if the list is empty? In a Go program, we might have the function return two values: one of the element type, which will hold the answer if there is one, and a second bool value which will be true if there’s an answer and false to indicate that there isn’t: func first ( list [] int ) ( int , bool ) { That’s not terrible, and it’s the standard API idiom in Go for this situation. In Rust, though, there’s a way to do it with just a single value: we can return an Option . fn first(list : & [ i32 ]) -> Option < i32 > { Options As you probably guessed, an Option type indicates that there may or may not be an answer. The return value from this function can be one of two things—two variants . It can either be None , meaning “no data to return”, or it can be Some(x) , meaning “the result is x ”. It’s now up to the caller to decide what to do. We could use a match expression to check whether or not there’s an answer: match first( & list) { Some (x) => println! ( "The first element is {x}" ) , None => println! ( "No result" ) , } if let expressions Commonly, though, we just want to do something if the option is Some , but nothing otherwise. It would be annoying to write an empty match arm for the None case, and fortunately we don’t have to. Instead, we can use an if let expression to execute some code only if the option is Some , and otherwise just continue: if let Some (x) = first( & list) { println! ( "The first element is {x}" ) } // moving on... The ? operator Sometimes if the answer is None , there’s nothing else useful we can do, so it’s best to just return from the function straight away. We could do this explicitly with match or if let , but there’s a better way. We can simply propagate the None value back to our caller, by appending the question mark operator ( ? ) to it: fn first_plus_1(list : & [ i32 ]) -> Option < i32 > { Some (first(list) ? + 1 ) } Here, if the value of first(list) is Some , then we add 1 to it and return the answer as Some . On the other hand, if first(list) instead returns None , then the ? operator short-circuits this function and automatically returns None as its answer. Neat! unwrap / expect If we can pretty much guarantee, because of the program’s internal logic, that there must be Some answer, we can enforce that using the unwrap method: fn first_plus_1_or_die(list : & [ i32 ]) -> i32 { first(list) . unwrap() + 1 // panics if list is empty } Calling unwrap is a big move, though. It means the program will crash with a very rude error if first(list) is ever None : thread 'main' panicked at src/main.rs:10:17:
called `Option::unwrap()` on a `None` value Ouch! We can make that error message slightly more informative by using expect instead of unwrap : first(list) . expect( "list should not be empty" ) The name is a little confusing: expect doesn’t mean “expect the result to be this string”, it means “if the result is None , panic with this message”. But, since good programs don’t panic, and neither do good programmers, it’s very rare that using unwrap or expect is actually the right thing to do. Usually, we should either use match and handle the None case explicitly, or propagate the Option using ? . Results As handy as Option is for signalling when there’s no answer, it doesn’t give us any way to tell the caller why there isn’t an answer. With a function like first , it’s fairly obvious, so we don’t need to explain. But with a function that can fail for many different reasons, it’s useful to be able to distinguish between them. That’s where Rust’s Result type comes in. Just like Option , a Result can be one of two possible variants. It can be either Ok(x) , meaning “the answer is x ”, or it can be Err(e) , meaning “couldn’t get the answer because error e happened”. Here’s how we might define a function that returns a Result : fn sqrt(n : f64 ) -> Result < f64 , String > { if n >= 0.0 { Ok (n . sqrt()) } else { Err ( "can't take square root of negative number" . into()) } } Handling Result values Again, we can deal explicitly with the two possibilities using a match expression: match sqrt( - 5.7 ) { Ok (x) => println! ( "The answer is {x}" ) , Err (e) => println! ( "Whoops: {e}" ) , } Or we can use ? to propagate any error back to our own caller: let answer = sqrt( 9.0 ) ?; Here, if the result is Ok , then we assign the answer to answer and continue. If it’s Err , though, the ? operator causes this function to return the error, provided that its return type is also some kind of Result . Error-only results Sometimes the function’s job is just to do something, so there’s no actual answer. But maybe there can still be an error, so in that case we’d use a Result where the Ok variant doesn’t contain anything: fn print_sqrt(x : i32 ) -> Result < () , String > { let answer = sqrt(x) ?; println! ( "{answer}" ) ; Ok (()) } The Rust type () just means, in effect, “nothing goes here”. So in this example the print_sqrt function either returns Ok(()) , meaning “everything went fine”, or, implicitly, some string indicating an error (“can’t take square root of negative number”). Optionality and resultitude Some languages let you ignore possible errors altogether, automatically propagating them as exceptions , and crashing the program if they’re not handled somewhere. Other languages, like Go, make error handling explicit, at the expense of a certain amount of boilerplate code to check and handle errors everywhere they can happen. Rust’s solution, on the other hand, is rather elegant. Returning a single Option or Result from a function indicates that the answer can be “no data”, or an error. That “optionality” or “resultitude”, if you like, is part of the answer, and it can be passed around our program from place to place, or stored and retrieved, right along with the data it applies to. Sooner or later, we’ll want to extract the actual answer, if present, and that’s the point where we have to deal with the possibilities of errors or non-answers. Rust gives us the choice to deal with it right away, or defer it for later, but we have to confront the issue at some point in our program. We can’t just ignore it and hope for the best. Type checking is better than hope And, since Option and Result are distinct types , Rust can detect at compile time when we’re failing to properly address issues of optionality and resultitude. If you try to use an Option<i32> as though it were a plain old i32 value, for example, Rust will swiftly puncture your unwarranted optimism: let answer: i32 = first(&list);
            ---   ^^^^^^^^^^^^ expected `i32`, found `Option<i32>` Which is completely reasonable, and we know what to do instead: use match or ? to deal with the None case, just as we do with Result values. The fact that Rust can catch issues of forgetfulness like this for us is helpful, and the Option and Result types are a very appealing feature of the language. In practice, a lot of our code will be about handling errors and “no data” situations, so having dedicated types and the ? shorthand to deal with them is a real boost to programmer happiness. Here’s to Rust! And you can read more about it in my early access book, The Secrets of Rust: Tools! $14.95 Add To Cart document.querySelector('.product-block .product-block').classList.add('is-first-product-block'); pre { white-space: pre-wrap; position: relative; }
======>
https://github.com/RyanWelly/lisp-in-rs-macros
-->>-->>
Repository files navigation README MIT license lisp-in-rs-macros A simple, lexically scoped Lisp interpreter that operates fully in Rust's declarative macros. The lisp! macro expands to the lisp value computed by the code, and then stringifies it. This means that lisp!(CAR (CONS (QUOTE A) (QUOTE (B)))) expands to the string "A" and that all this computation happens at compile time by rustc expanding macros. Why It's a lisp interpreter written fully in Rust's macros, I think that's pretty cool. It's also less than 250 lines, which is neat. Example let output = lisp ! ( CAR ( LIST ( QUOTE A ) ( QUOTE B ) ( QUOTE C ) ) ) ; assert_eq ! ( output, "A" ) ; lisp ! ( PROGN ( DEFINE message ( LAMBDA ( ) ( QUOTE "hello there" ) ) ) ( DISPLAY ( message ) ) ( DEFINE NOT ( LAMBDA ( X ) ( COND ( X NIL ) ( TRUE TRUE ) ) ) ) ( DISPLAY ( NOT NIL ) ) ) ; // will print "hello there" and "TRUE" // "DISPLAY" forms first evaluate their arguments, then expand to a println!("{}", stringify!(evaled_argument)) As another fun example, here is a quine: lisp ! ( ( LAMBDA ( s ) ( LIST s ( LIST ( QUOTE QUOTE ) s ) ) ) ( QUOTE ( LAMBDA ( s ) ( LIST s ( LIST ( QUOTE QUOTE ) s ) ) ) ) ) ; This code expands to: stringify ! ( ( ( LAMBDA ( s ) ( LIST s ( LIST ( QUOTE QUOTE ) s ) ) ) ( QUOTE ( LAMBDA ( s ) ( LIST s ( LIST ( QUOTE QUOTE ) s ) ) ) ) ) ) ; In other words, the code evaluates to itself. Isn't that wonderful? Recursion This lisp does not currently support any explicit form of recursion. Luckily, explicit recursion is not needed, all we need is lambda. You can write a simple function that appends two lists by using self application: lisp ! ( PROGN ( DEFINE append ( LAMBDA ( self X Y ) ( COND ( ( EQ X NIL ) Y ) ( TRUE ( CONS ( CAR X ) ( self self ( CDR X ) Y ) ) ) ) ) ) ( append append ( QUOTE ( A B ) ) ( QUOTE ( C D ) ) ) ) This results in "(A B C D)". The append function does not mention append in its body, yet we can call it recursively. Wonderful! Notes for use The lisp! macro only evaluates a single expression; if you want to evaluate multiple expressions, use (PROGN expr1 expr2 expr3) . This evaluates all the expressions, and returns the value of the last expression. The DISPLAY form evaluates a single expression, then generates a println!("{}", stringify!(...)) statement which prints the stringified version of the tokens. The empty list is not self evaluating, you can use NIL or (QUOTE ()) to obtain an empty list value. The empty list is the sole "falsy" object.
Dotted lists aren't supported, cons assumes its last argument is a list. The define form can be used anywhere and evaluates to the empty list, but does not support recursion. TRUE is the only self evaluating atom (that isn't a function). Supported forms DEFINE QUOTE LAMBDA LET PROGN CAR CDR CONS LIST EQ ATOM APPLY Note: dotted lists are not supported, CONS assumes its latter argument is a list. Define does not handle recursive definitions, it's more like internal definitions in Scheme than a true lispy define. Metacircular interpreter Here is a lisp interpreter written in my lisp: lisp ! ( PROGN // Y "combinator" for two arguments ( DEFINE Y2 ( LAMBDA ( h ) ( ( LAMBDA ( x ) ( h ( LAMBDA ( a b ) ( ( x x ) a b ) ) ) ) ( LAMBDA ( x ) ( h ( LAMBDA ( a b ) ( ( x x ) a b ) ) ) ) ) ) ) ( DEFINE CADR ( LAMBDA ( X ) ( CAR ( CDR X ) ) ) ) ( DEFINE CAAR ( LAMBDA ( X ) ( CAR ( CAR X ) ) ) ) ( DEFINE CADAR ( LAMBDA ( X ) ( CAR ( CDR ( CAR X ) ) ) ) ) ( DEFINE CADDR ( LAMBDA ( X ) ( CAR ( CDR ( CDR X ) ) ) ) ) ( DEFINE CADDAR ( LAMBDA ( X ) ( CAR ( CDR ( CDR ( CAR X ) ) ) ) ) ) ( DEFINE CAADAR ( LAMBDA ( X ) ( CAR ( CAR ( CDR ( CAR X ) ) ) ) ) ) ( DEFINE ASSOC ( Y2 ( LAMBDA ( ASSOC ) ( LAMBDA ( X ENV ) ( IF ( EQ ( CAAR ENV ) X ) ( CADAR ENV ) ( ASSOC X ( CDR ENV ) ) ) ) ) ) ) ( DEFINE eval ( Y2 ( LAMBDA ( EVAL ) ( LAMBDA ( E A ) ( COND ( ( ATOM E ) ( ASSOC E A ) ) ( ( ATOM ( CAR E ) ) ( COND ( ( EQ ( CAR E ) ( QUOTE quote ) ) ( CADR E ) ) ( ( EQ ( CAR E ) ( QUOTE atom ) ) ( ATOM ( EVAL ( CADR E ) A ) ) ) ( ( EQ ( CAR E ) ( QUOTE car ) ) ( CAR ( EVAL ( CADR E ) A ) ) ) ( ( EQ ( CAR E ) ( QUOTE cdr ) ) ( CDR ( EVAL ( CADR E ) A ) ) ) ( ( EQ ( CAR E ) ( QUOTE equal ) ) ( EQ ( EVAL ( CADR E ) A ) ( EVAL ( CADDR E ) A ) ) ) ( ( EQ ( CAR E ) ( QUOTE cons ) ) ( CONS ( EVAL ( CADR E ) A ) ( EVAL ( CADDR E ) A ) ) ) ( TRUE ( EVAL ( CONS ( ASSOC ( CAR E ) A ) ( CDR E ) ) A ) ) ) ) ( ( EQ ( CAAR E ) ( QUOTE lambda ) ) ( EVAL ( CADDAR E ) ( CONS ( LIST ( CAADAR E ) ( EVAL ( CADR E ) A ) ) A ) ) ) //Evaluate the inner expression of the lambda, in the environment with the argument bound to the parameter ) ) ) ) ) ( eval ( QUOTE ( quote ( A ) ) ) NIL ) // (eval (QUOTE (atom (quote A))) NIL ) // (eval (QUOTE (cdr (cdr (quote (A B))))) NIL) // (eval (QUOTE (cons (quote a) (quote (a)))) NIL) // (eval (QUOTE ((lambda (x) (quote a)) (quote b))) NIL) ( eval ( QUOTE ( ( lambda ( X ) X ) ( quote a ) ) ) NIL ) ) ; It appears to work, but trying to evaluate ((lambda (X) X) (quote a)) in the interpreter takes more than 30 seconds and generates far more than 1 million+ tokens before cargo gets sigkilled. Using the explicit y combinator for recursion isn't particularly efficient here! To fix this, I should add an explicit recursion primitive. Technical explanation Look at EXPLANATION.md. The macro essentially simulates a SECD machine, which is a simple stack-basd abstract machine for evaulating lambda calculus terms. Awesome resources Functional Programming: Application and Implementation by Peter Henderson Ager, Mads Sig, et al. "A functional correspondence between evaluators and abstract machines." Proceedings of the 5th ACM SIGPLAN international conference on Principles and practice of declaritive programming. 2003. The Implementation of Functional Programming Languages by Simon Peyton Jones Anything Matt Might has ever written about lisp on his blog ( https://matt.might.net ) TODO Add letrec Add recursive defines

======>
https://old.reddit.com/r/rust/comments/1ffy6fb/announcing_plotlars_050_unified_bar_and_box_plots/
-->>-->>
Hello Rustaceans!   

   I’m thrilled to announce the release of    Plotlars 0.5.0   

   This version brings significant enhancements to make your data visualizations more powerful and customizable than ever before.   

   🚀 New Features   

   
   Unified Bar and Box Plots   : We’ve introduced a new BarPlot struct with an orientation field, deprecating VerticalBarPlot and HorizontalBarPlot. Similarly, the BoxPlot struct now handles both vertical and horizontal box plots. This unification simplifies the API and gives you more flexibility in representing your data.   
   Simplified Color Argument   : Added a color argument as an alternative to the colors argument for cases when you need to apply a single color to your plot. This makes it easier to set a uniform color without specifying an array of colors.   
   Customizable Marker Shapes   : Now you can customize the shape of markers in your plots, allowing for better representation of your data points.   
   Adjustable Line Width and Shapes for Line and Time Series Plots   : Added optional shape parameters and line width adjustments for line and time series plots, giving you more control over the appearance of your graphs.   
   

   But wait, there’s more! 🎉   

   We’ve also reached a milestone of    200 GitHub Stars   ! ⭐️⭐️⭐️   

   A huge thank you to all of you for your support, contributions, and feedback. Your encouragement keeps this project moving forward.   

   Explore the updated documentation and head over to the    GitHub repository    to see the new features in action. If you enjoy using Plotlars, consider leaving a star ⭐️ on GitHub to help others discover the project and support its continued development.   

   Thank you for your support, and happy plotting! 🎉   

   https://preview.redd.it/48f0urlzllod1.png?width=3386&format=png&auto=webp&s=f60e34601714b4bc516a4372bd475085a86896dd   
   

======>
https://old.reddit.com/r/rust/comments/1ffuvak/unix_functions_in_rust/
-->>-->>
This semester there's a subject called Operating Systems that interacts with the UNIX API,, the subject was designed for C and as such uses pthread.h unistd.h signal.h and so on.   

   I proposed to the teacher that he would let me make the project in Rust and the condition was that whatever I use to be aproved couldn't overshadow the system calls characteristic to the C version system calls.   

   I looked into nix and/or rustix for this objective. And would really like an oppinion on whether I should choose one over the other.   

   Also would like some sugestions for the pthread library. Since I can't use the threads native to Rust.   

   I'm currently leaning into nix but would really appreaciate if you could lend me a hand.   
   

======>
https://github.com/djc/pyrtls
-->>-->>
Repository files navigation README pyrtls: rustls-based modern TLS for Python pyrtls provides bindings to rustls , a modern Rust-based TLS implementation with an API that is
intended to be easy to use to replace the ssl module (but not entirely compatible with it). In addition to being memory-safe, the library is designed to be more secure by default. As such,
it does not implement older protocol versions, cipher suites with known security problems, and
some problematic features of the TLS protocol. For more details, review the rustls manual . Warning This project is just getting started. While rustls is mature, the Python bindings
are pretty new and not yet feature-complete. Please consider helping out (see below). Why? To bring the security and performance of rustls to the Python world. So far this is a side project. Please consider helping out: Please help fund this work on GitHub Sponsors Pull requests welcome, of course! Feedback through issues is highly appreciated If you're interested in commercial support, please contact me Features Support for TLS 1.2 and 1.3 Support for commonly used secure cipher suites Support for ALPN protocol negotiation Support for Server Name Indication (SNI) Support for session resumption Clients use the OS certificate trust store by default Exposes socket wrapper as well as sans I/O APIs In basic tests, performance is comparable to the ssl module Not implemented TLS 1.1 and older versions of the protocol Older cipher suites with security problems Using CA certificates directly to authenticate a server/client (often called self-signed
certificates). The built-in certificate verifier does not support using a trust anchor
as both a CA certificate and an end-entity certificate, in order to limit complexity and
risk in path building.
======>
https://old.reddit.com/r/rust/comments/1fg6nsp/is_deref_right_for_this/
-->>-->>
I have a Foo struct what has fields A, B, C, D, and E. I also have another struct which is a subset of these fields, call it FooPart.   

   FooPart is all I need in order to do foo(), so foo() is implemented as a method on FooPart.   

   However, sometimes I have a full Foo and want to call foo().   

   Do I:   

   A. Construct FooPart explicitly like foo.into_part().foo()   

   or   

   B. Impl Deref for Foo so it derefs to FooPart, like foo.foo()   

   ?   
   

======>
https://old.reddit.com/r/rust/comments/1fg2v43/ergonomics_for_optionintostring/
-->>-->>
I have been looking for the most ergonomic/readable way to write this without putting undo burden on lib crate users. Here's the situation:   

   I want to have a struct    Client    have a method called    new   .    new    should optionally take any type that can be converted into a string and not give any fuss. Something like this   

   rust
pub fn new<T: Into<String>>(url: Option<T>) -> Self {
    //
}
   

   Of course, the problem then arises when this function gets called with    None   , forcing the crate user to annoyingly annotate a type such as    Client::new(None::<String>)   . Definitely not what we want.   

   Is there a nice way to do this?    
   

======>
https://owdle.guessing.day/
-->>-->>
Owdle is a daily guessing game where the player guesses Overwatch heroes based on some of their characteristics. Test your knowledge and get to know more of their background! It's inspired by many other wordle -like games, such as metazooa , loldle , guessthegame , redactle , etc. Owdle is not affiliated nor endorsed by Blizzard Entertainment. Overwatch and its characters are a property of Blizzard Entertainment. Owdle is a daily guessing game where the player guesses Overwatch heroes based on some of their characteristics. Test your knowledge and get to know more of their background!
======>
https://old.reddit.com/r/rust/comments/1fg1bpg/while_trying_to_implement_lifetimes_i_couldnt/
-->>-->>
Hello, I've been reading the    Crafting Interpreters    book and writing the interpreters in Rust. I finished the AST version and started building bytecode one (here is the    repo   ). While implementing the AST version, first, I implemented just translating the code to Rust and didn't focus much on performance, idiomatic writing, etc. As expected, the first version lacked performance A LOT because I used too much cloning, and I used a lot of smart pointers, so generally, the code was horrible, but it was working. After that, I started improving the code, removing clonings, and trying to rewrite some parts more idiomatic to rust, and it worked well. The benchmark Fibonacci code took 168 seconds in the first version, but now it takes 101 seconds. However, there is still room for improvement because I still use some cloning in places where a reference would work fine.   

   While I was looking for more ways to improve my code, I found a Reddit    POST    that also tries to improve the performance of the code. Using `typed-arena` and intern string literals sound clever. However, I was stuck on implementing lifetimes to my `LoxValue` enum. Because it is basically EVERYWHERE, and when I introduce lifetimes to this enum, almost all of the code needs to be changed. I have a grasp of lifetimes, but I'm not experienced with it. After getting stuck while trying to implement lifetimes to `LoxValue` but could manage it a couple of times, I gave up. But I want to improve and become a better rust developer, so I want to be able to make it somehow. I need your help, guidance, advice, and everything that might help me is welcome. All help is appreciated, thank you.   

   Edit 1: As    u/Ok-Watercress-9624    said, I used indices instead of lifetimes for Expr. And also introduced intern strings. This dropped the calculation of the fib(40) to 64 secs from 101 secs.   
   

======>
https://blog.spiraldb.com/compressing-strings-with-fsst/
-->>-->>
vortex Compressing strings with FSST Random access string compression with FSST and Rust Andrew Duffy Sep 9, 2024 — 11 min read A new string compression algorithm, conjured up by the wizards at CWI and TUM We’ve released our Apache-licensed implementation of FSST, you can check it out on GitHub ! Vortex is our open-source Rust library for compressed Arrow. If you want to have compressed Arrow arrays on-disk, in-memory, or over-the-wire, Vortex is designed to help out. Vortex opts for a columnar representation that is the same both on-disk and in-memory, relying on a set of lightweight compression schemes that allow readers to just-in-time decompress the data you need, when you need it. We’re building out a toolkit of open codecs to efficiently and effectively compress the widest range of data. Our initial focus has been on numeric codecs, and we’ve written about our approach using the FastLanes layout to compress integers at the bleeding edge of what’s possible on today’s hardware. We’ve also implemented ALP to compress floating point arrays, as well as Roaring Bitmaps . However, not all data is numeric. String data is incredibly prevalent in the real world. Apache Parquet, the most popular analytics data format, supports compressing strings with dictionary encoding, along with a range of general purpose compressors, which as we see later can be too heavyweight for use cases that favor random indexed access to data. More recently, there’s been renewed interest within the research community around string compression in databases, and in this post we’re going to describe a relatively new codec called FSST . FSST is fast–the reference implementation achieves gigabytes-per-second decoding–and has shown to compress on average by a factor of 2-3x on many real-world datasets. We're open-sourcing a Rust crate for FSST, and we've also integrated it into our Vortex toolkit . The next few sections provide a brief history of string compression, motivating the need for a new lightweight string codec like FSST, and then provides a walk through of the steps for compression and decompression. String compression, in brief String compression algorithms are not new, and you may already be familiar with some of the more common ones such as Zip, LZ4, and Zstd. All of these operate over blocks of data. Accessing individual values require decompressing the whole block. If you're processing an entire block at a time, this may amortize out, although recent research has shown that due to increasing network and disk bandwidth, reading these formats has become increasingly CPU bound . The CPU overhead of reading block-compressed data is especially apparent in applications that do lots of small random accesses across a column. Let's take as an example an array with fixed 128KB blocks : Let's say to service a particular query, we want to access 8 values, or ~6% of the data in the array. In the best case–when the indices are clustered–we can do so by only decompressing a single block. However, if the indices are unclustered and spread out, then in the worst case we must decompress the entire array! What we want for these more dynamic applications are codecs that support random access . We want a data structure that efficiently implements the following interface: pub trait StringCodec { /// Decode and return the string at the given index. fn decode ( index : usize ) -> String ; } This is a deep oversimplification of course, but a helpful one for demonstration purposes. We’ll revisit this toy trait throughout the rest of the post to illustrate how random access is implemented for different encoding schemes. Dictionary Encoding Dictionary encoding is probably the most well-known codec that supports efficient random access. It is also one of the simplest: perform a single pass over your data to find the unique values, assign each value an integer code , and replace each occurrence of that value with the code. We can illustrate with a simple example that has 3 unique strings: Implementing the single-element decoding just requires access to the dictionary: pub struct DictionaryArray { /// The string at index `i` is replaced with the code `i`. /// Stored a single time at the top of the file. codes : Vec < String > , /// The encoded values. Each string is represented by /// an 8-byte code which is an index into the `codes` /// table. array : Vec < usize > , } impl StringCodec for DictionaryArray { fn decode ( & self , index : usize ) -> String { // Step 1: lookup the code in the `index` position let code = self . array [ index ] ; // Step 2: map the code back to a string self . codes [ code ] . clone ( ) } } Dictionary encoding is simple, and it performs excellently on many real-world datasets, particularly for categorical columns such as labels and event names. However, it’s not without its drawbacks: Dictionaries only operate at the level of entire strings. If you have repetition that only manifests at the substring level, dictionary encoding won’t help you. Dictionaries only compress well if your data is low-cardinality. As the number of unique values approaches the length of the array, dictionary compression becomes useless, and if you’re not careful it can even be larger than the raw dataset. This is especially pertinent for request logs, JSON records, UUIDs, and free text. You cannot use dictionary encoding to compress data that is not in the dictionary; you instead need to build a new dictionary from scratch. This comes up if you’re building a system that cares about fast append operations, and appending chunks to an existing column with a shared dictionary is desirable. FSST has entered the chat The folks over at CWI in the Netherlands & TUM in Germany have been studying databases for a long time , and in 2020 they dropped another banger: FSST: Fast Random Access String Compression . FSST stands for “Fast Static Symbol Table”, a new data structure to act as the backbone for fast random-access string compression. 🎓 Peter Boncz, Thomas Neumann, and Viktor Leis. 2020. FSST: fast random access string compression. Proc. VLDB Endow. 13, 12 (August 2020), 2649–2661. https://doi.org/10.14778/3407790.3407851 Putting the ST in FSST Compressing text with FSST requires you to first build a symbol table , which like a dictionary is shipped with the actual data to be used at decoding time. Unlike dictionaries, which only hold whole-strings, the symbols in a symbol table are short substrings that can occur anywhere in the array. Because most computers have a native 64-bit word size, we say that a symbol can be between 1 and 8 bytes so that it will always fit in a single register. To make it easy to pass FSST-encoded data around wherever string data would usually go, we set a max table length of 256. This ensures that all codes can be packed into a single byte, allowing our encoding to receive a slice of u8 and return a slice of u8 . Compression works string-by-string, and proceeds greedily, matching the longest prefix against the symbol table, and recording the corresponding code into our output array. Figure 1 in the paper has an example of what this looks like in practice for a small URL dataset: 256 codes is a lot, but even with an ideal symbol table, you aren't guaranteed to be able represent any possible string. For that, we need to reserve code 255 ( 0xFF ) to represent the escape code . If while compressing we encounter a substring that does not match any symbol in the table, then we emit each byte of the substring preceded by an escape byte. When the decompressor sees 0xFF in the input, it knows to immediately emit the following byte without looking it up in the symbol table. If you’re following along, you’ll realize then that the worst-case compression factor is actually a doubling of the input , which we see if we attempt to compress using an empty symbol table. For example, the string “hello” would be compressed into the byte string: [ 0xFF , b'h' , 0xFF , b'e' , 0xFF , b'l' , 0xFF , b'l' , 0xFF , b'o' ] Based on this, we can start to sketch out the codec and array implementations we’d need to implement FSST decoding with random access: struct FSSTArray { /// `symbols` stores the symbol table. Each symbol can be a string /// of 1 to 8 bytes, which we pack into a u64. symbols : Vec < u64 > , /// The length of each symbol. This vector has the same length /// as `symbols`. symbol_lengths : Vec < u8 > , /// The encoded variable-length byte array. bytes[i] is the i-th encoded /// string. /// /// Each encoded string will be a sequence of 8-bit unsigned /// integer offsets into the `symbols` table. bytes : Vec < Vec < u8 >> } impl StringCodec for FSSTArray { fn decode ( & self , index : usize ) -> String { // Get the compressed value at the index. // This is a byte array of codes. let codes = self . bytes [ index ] ; let mut pos = 0 ; let mut result = Vec :: new ( ) ; while pos < codes . len ( ) { let next_code = codes [ pos ] ; if next_code == ESCAPE { // Emit an escape byte pos += 1 ; result . push ( code [ pos ] ) ; } else { // Lookup the code in the symbol table let symbol = self . symbols [ code ] ; let len = self . symbol_lengths [ code ] as usize ; result . extend_from_slice ( & symbol . to_le_bytes ( ) [ 0 .. len ] ) ; } pos += 1 ; } // Convert the UTF-8 encoded bytes into a String for the caller String :: from_utf8 ( result ) . unwrap ( ) } } Based on these characteristics, we can quickly see that the theoretical compression factors range from 8—the maximum length of a symbol—to 0.5 in the case where every byte of a string is escaped. Thus, building a symbol table to preference long-matches and avoiding escapes is going to be key to both good compression factors and fast decompression speeds. A seat at the (symbol) table FSST compression relies on a generational algorithm for building a symbol table. In each generation, it uses the current generation’s symbol table to compress a piece of sample text, then examines the used codes to create a set of new potential candidate symbols. At the end of the generation, the best symbols are preserved for the next generation. We need to be more specific about how we decide the “top” symbols however. In the paper, the author’s define a new metric for the “ effective gain ” of a symbol. A symbol’s effective gain is equal to its length times its frequency of how it appears in the compressed text. Semantically, this is like the marginal compression factor of having this one symbol in the table, for the given piece of text we’re evaluating against. The algorithm takes 3 parameters: Maximum symbols : how big can the symbol table grow. This is directly linked to the code size. Because we’re using byte codes, this is 255 (leaving the 256th code for an escape). Maximum symbol length : How many bytes can each symbol hold? This was chosen to be 8 bytes so that it can always fit into a single 64-bit register. Number of generations : How many generations does the table selection algorithm proceed for? Because we concatenate symbols at the end of each generation, we must have at least 3 generations to have a chance to generate any 8 byte symbols. The creators of FSST did a lot of empirical testing and found 5 to provide the best balance between speed and compression quality. Now that we have all of the parameters of the algorithm, we can be a bit more rigorous in our description of the steps. First, we initialize an empty symbol table. An empty symbol table will compress any string it receives into a byte array where each character is preceded by an escape byte. For each generation: We compress our target string using our existing symbol table. When we compress, we count the symbols that were used in the encoded output. We also concatenate adjacent symbols as candidates for new symbols to add. For all existing and candidate symbols, we assign them a gain ( count * length ), and preserve the top 255 symbols by gain. We set the top 255 as the new symbol table to use in the next generation. We’ve illustrated a simplified version of the algorithm below. We use a a toy string mums tums , and we’ve altered the parameters to make things fit in one page: Maximum symbols : 4 Maximum symbol length : 3 Number of generations : 2 First comes Step 1: Compress using the existing symbol table. In this case, the symbol table is initialized to be empty, so this will escape all bytes, doubling the size of the text! Step 2: Count the symbols that appear in the compressed text. In this case, the symbols are all single-character escapes. We also count all adjacent pairs of symbols that appear, as they are good candidates to merge into a single symbol. Step 3: Rank the candidates by their effective gain. We preserve the top symbols (in this toy example, the top four) to use as the new symbol table. We now have a complete symbol table. We repeat the steps above for the second generation: In the first generation, our empty symbol table initially doubles the size of our output, but over two generations we refine this table, arriving at one that actually compresses our string by more than a factor of 2! As mentioned, this was a simplified example meant to strip away details to clarify the algorithm’s mechanics. There are quite a few moving pieces needed to actually bring this code to life: Extending support from single-strings to string arrays with many values. The naive way to do this is to call the compression/decompression kernels in a loop. A more advanced variant would compress the entire array range as a single mega-string to avoid branching. Training the symbol table on a whole array forces us to compress the full-size array 5 times, which is slow. Instead, we choose a small fixed-size sample of 16KB, drawn as 512B chunks sampled uniformly at random across all elements of the array. Both 16KB and 512 were chose empirically by the authors as good tradeoffs between locality (longer runs are more realistic representations of the source) and performance (getting a diverse enough sample to train a table that compresses the whole array well). The authors of the paper implemented an AVX-512 compression kernel, which we have not implemented in the Rust code but can lead to better performance over the scalar variant. You can learn more about their “meatgrinder” in Section 5 of the paper. Closing thoughts For all the ink spilled in this post, there's still so much we didn't get to cover. For that, we encourage you to read the paper as well as the Rust code for more details on the specific tricks needed to make the codec fast enough for use. We believe that FSST is a great building block for working with string data, and providing a good wrapper around its power brings us closer to achieving our goal of making Vortex the absolute best way to build data-intensive applications. We’re building in the open, so if you’re interested in following along on the journey, be sure to check us out on GitHub !
======>
https://old.reddit.com/r/rust/comments/1ffpgze/salvo_web_framework_0720_released/
-->>-->>
Salvo is a web framework written in Rust that focuses on ergonomics. It has many advantages over other frameworks:   

   
   No type gymnastics, and basically no deep understanding of Rust is required when using it, which is very suitable for beginners;   
   Comprehensive functions, supporting new protocols and functions such as HTTP3 and Webtransport;   
   Officially maintains many middlewares;   
   Provides a tower compatibility layer, and tower middleware can be used directly;   
   

   This update:   

   
   Allows middleware to be applied directly to Handler, which is more convenient to use;   
   Implements Writer support for tuples;   
   Provides SecureMaxSize middleware, which can control the size of secure data of requests at a finer granularity;   
   Fixes the problem of incorrect storage key in the RequestId part;   
   Made many improvements to OpenAPI support;   
   Fixes the problem of returning 405 when 404 should be returned in some cases.   
   

   Rust itself is a relatively difficult language, and the obscure design of many web frameworks makes things even worse. When you are using other frameworks, scratching your head, and are about to give up Rust, remember to come to Salvo. It will "keep you here", give you the confidence and courage to learn Rust, and let you feel and see the results of victory.   

   More detailed update information:    https://github.com/salvo-rs/salvo/releases/tag/v0.72.0   
   

======>
https://old.reddit.com/r/rust/comments/1fgajrp/cost_of_mutex/
-->>-->>
Hello guys,   

   what is the cost of mutex if i don't have data races?(So if there is no possibility that two different threads try to access that memory?)   
   

======>
https://github.com/alceal/plotlars
-->>-->>
Repository files navigation README MIT license Plotlars Plotlars is a versatile Rust library that acts as a wrapper around the Plotly
crate, bridging the gap between the powerful Polars data analysis library and
Plotly. It simplifies the process of creating visualizations from data frames,
allowing developers to focus on data insights rather than the intricacies of
plot creation. Motivation The creation of Plotlars was driven by the need to simplify the process of
creating complex plots in Rust, particularly when working with the powerful
Polars data manipulation library. Generating visualizations often requires
extensive boilerplate code and deep knowledge of both the plotting library
(Plotly) and the data structure. This complexity can be a significant hurdle,
especially for users who need to focus on analyzing and interpreting data rather
than wrestling with intricate plotting logic. To illustrate this, consider the following example where a scatter plot is
created without Plotlars : use plotly :: { common :: * , layout :: * , Plot , Scatter , } ; use polars :: prelude :: * ; fn main ( ) { let dataset = LazyCsvReader :: new ( "data/penguins.csv" ) . finish ( ) . unwrap ( ) . select ( [ col ( "species" ) . cast ( DataType :: Categorical ( None , CategoricalOrdering :: default ( ) ) ) , col ( "flipper_length_mm" ) . cast ( DataType :: Int16 ) , col ( "body_mass_g" ) . cast ( DataType :: Int16 ) , ] ) . collect ( ) . unwrap ( ) ; let group_column = "species" ; let x = "body_mass_g" ; let y = "flipper_length_mm" ; let groups = dataset . column ( group_column ) . unwrap ( ) . unique ( ) . unwrap ( ) ; let layout = Layout :: new ( ) . title ( Title :: with_text ( "Penguin Flipper Length vs Body Mass" ) ) . x_axis ( Axis :: new ( ) . title ( Title :: with_text ( "Body Mass (g)" ) ) ) . y_axis ( Axis :: new ( ) . title ( Title :: with_text ( "Flipper Length (mm)" ) ) ) . legend ( Legend :: new ( ) . title ( Title :: with_text ( "Species" ) ) ) ; let mut plot = Plot :: new ( ) ; plot . set_layout ( layout ) ; for group in groups . iter ( ) { let group = group . get_str ( ) . unwrap ( ) ; let data = dataset . clone ( ) . lazy ( ) . filter ( col ( group_column ) . eq ( lit ( group ) ) ) . collect ( ) . unwrap ( ) ; let x = data . column ( x ) . unwrap ( ) . i16 ( ) . unwrap ( ) . to_vec ( ) ; let y = data . column ( y ) . unwrap ( ) . i16 ( ) . unwrap ( ) . to_vec ( ) ; let trace = Scatter :: default ( ) . x ( x ) . y ( y ) . name ( group ) . mode ( Mode :: Markers ) . marker ( Marker :: new ( ) . size ( 10 ) . opacity ( 0.5 ) ) ; plot . add_trace ( trace ) ; } plot . show ( ) ; } In this example, creating a scatter plot involves writing substantial code to
manually handle the data and configure the plot, including grouping the data by
category and setting up the plot layout. Now, compare that to the same plot created using Plotlars : use plotlars :: { ScatterPlot , Plot , Text , } ; use polars :: prelude :: * ; fn main ( ) { let dataset = LazyCsvReader :: new ( "data/penguins.csv" ) . finish ( ) . unwrap ( ) . select ( [ col ( "species" ) . cast ( DataType :: Categorical ( None , CategoricalOrdering :: default ( ) ) ) , col ( "flipper_length_mm" ) . cast ( DataType :: Int16 ) , col ( "body_mass_g" ) . cast ( DataType :: Int16 ) , ] ) . collect ( ) . unwrap ( ) ; ScatterPlot :: builder ( ) . data ( & dataset ) . x ( "body_mass_g" ) . y ( "flipper_length_mm" ) . group ( "species" ) . size ( 10 ) . opacity ( 0.5 ) . plot_title ( "Penguin Flipper Length vs Body Mass" ) . x_title ( "Body Mass (g)" ) . y_title ( "Flipper Length (mm)" ) . legend_title ( "Species" ) . build ( ) . plot ( ) ; } This is the output: With Plotlars, the same scatter plot is created with significantly less code.
The library abstracts away the complexities of dealing with individual plot
components and allows the user to specify high-level plot characteristics. This
streamlined approach not only saves time but also reduces the potential for
errors and makes the code more readable and maintainable. Installation cargo add plotlars Features Seamless Integration with Polars: Leverage the power of Polars for efficient
data manipulation and analysis. Support for Multiple Plot Types: Easily create bar, line, scatter, and other
plot types. Customization: Modify plot appearance with an intuitive API. Plotlars in Jupyter Notebooks Plotlars seamlessly integrates with Jupyter Notebooks, allowing you to leverage
the power of interactive data visualization directly within your notebook
environment. This integration is made possible through the use of the evcxr project , which provides a Jupyter kernel
for the Rust programming language. With Polars, evcxr, and Plotlars, data science in Rust leaps to the next level
, making powerful data analysis and visualization more accessible and efficient
than ever before. License This project is licensed under the MIT License. See the LICENSE.txt file for details. Acknowledgements Polars : For providing a fast and
efficient data manipulation library. Plotly : For the inspiration and ideas
behind visualization libraries. Evcxr : For enabling the use of Rust in
Jupyter Notebooks. Rust Community: For the support and development of an amazing programming
language.
======>
https://craftinginterpreters.com/
-->>-->>
Ever wanted to make your own programming language or wondered how
they are designed and built? If so, this book is for you. Crafting Interpreters contains everything you need to implement a
full-featured, efficient scripting language. You’ll learn both high-level
concepts around parsing and semantics and gritty details like bytecode
representation and garbage collection. Your brain will light up with new ideas,
and your hands will get dirty and calloused. It’s a blast. Starting from main() , you build a language that features rich
syntax, dynamic typing, garbage collection, lexical scope, first-class
functions, closures, classes, and inheritance. All packed into a few thousand
lines of clean, fast code that you thoroughly understand because you write each
one yourself. The book is available in four delectable formats: Print 640 pages of beautiful typography and high resolution hand-drawn
    illustrations. Each page lovingly typeset by the author. The premiere reading
    experience. Amazon .com .ca .uk .au .de .fr .es .it .jp Barnes and Noble Book Depository Download Sample PDF eBook Carefully tuned CSS fits itself to your ebook reader and screen size.
    Full-color syntax highlighting and live hyperlinks. Like Alan Kay's Dynabook
    but real. Kindle Amazon .com .uk .ca .au .de .in .fr .es .it .jp .br .mx Apple Books Play Books Google Nook B&N EPUB Smashwords PDF Perfectly mirrors the hand-crafted typesetting and sharp illustrations of
    the print book, but much easier to carry around. Buy from Payhip Download Free Sample Web Meticulous responsive design looks great from your desktop down to your
    phone. Every chapter, aside, and illustration is there. Read the whole book
    for free. Really. Read Now About Robert Nystrom I got bitten by the language bug years ago while on paternity leave between
midnight feedings. I cobbled together a number of hobby languages before worming my way into an honest-to-God,
full-time programming language job. Today, I work at Google on the Dart language . Before I fell in love with languages, I developed games at Electronic Arts
for eight years. I wrote the best-selling book Game Programming
Patterns based on what I learned there. You can read that book for free
too. If you want more, you can find me on Twitter ( @munificentbob ), email me at bob at this site's domain (though I am slow to respond), read my blog , or join
my low frequency mailing list: Handcrafted by Robert Nystrom — © 2015 – 2021
======>
https://github.com/Emivvvvv/rlox
-->>-->>
Repository files navigation README MIT license rlox Rust implementations of the jlox and clox interpreters from the book Crafting Interpreters . jlox is a tree-walk interpreter written in Java and clox is a bytecode interpreter written in C. rlox-ast roadmap Rust implementation of the jlox interpreter from the second chapter of "Crafting Interpreters". Chapter Status Scanning ✅ Representing Code ✅ Parsing Expressions ✅ Evaluating Expressions ✅ Statements and State ✅ Control Flow ✅ Functions ✅ Resolving and Binding ✅ Classes ✅ Inheritance ✅ rlox-bytecode roadmap Rust implementation of the clox interpreter from the third chapter of "Crafting Interpreters". Note The foundation of rclox was inspired by jeschkies's lox-rs implementation . Chapter Status Chunks of Bytecode ✅ A Virtual Machine ✅ Scanning on Demand ⏳ Compiling Expressions ⏳ Types of Values ⏳ Strings ⏳ Hash Tables ⏳ Global Variables ⏳ Local Variables ⏳ Jumping Back and Forth ⏳ Calls and Functions ⏳ Closures ⏳ Garbage Collection ⏳ Classes and Instances ⏳ Methods and Initializers ⏳ Superclasses ⏳ Optimization ⏳
======>
https://www.reddit.com/r/ProgrammingLanguages/comments/v5d5uo/lox_interpreter_in_rust_slower_than_in_java/
-->>-->>
Go to ProgrammingLanguages r/ProgrammingLanguages r/ProgrammingLanguages This subreddit is dedicated to the theory, design and implementation of programming languages. 101K Members 35 Online • 2 yr. ago mhj ADMIN MOD Lox interpreter in Rust slower than in Java I'm learning Rust currently and have implemented an AST-walking interpreter as described in the first part of Robert Nystroms Crafting Interpreters: you can find the code here . My initial implementation in Rust was a lot slower than the Java-based implementation from the book . After optimizing some low hanging fruits (mostly by eliminating unnecessary copying by using Rc-references and by interning the lexeme-strings) I've got the difference in performance down quite a bit. But there are still benchmarks where my implementation is still over 3x as slow in comparison to the implementation in Java. You can see some numbers here . I couldn't find any obvious bottlenecks when profiling (could very well be that I'm blind :D). Maybe it's a death by thousand cuts situation, i.e. overhead of reference counting? Maybe it's JVMs JIT doing its magic (I'm not very familiar with Java's ecosystem so I haven't verified yet)? Any explanations for why my implementation is so much slower than the Java-based one are appreciated. As are tips on how to further improve performance (ideally while keeping the overall interpreter structure still close to the Java implementation, I know that a bytecode-based interpreter will blow an AST-walking one out of the water). Edit 1: Changing the hash function to FNV brought a nice performance boost . Thanks jonnyboyC13 and Uncaffeinated for bringing that up. :) The times for method_call.lox and properties.lox are now faster than the times I get with the Java implementation. Edit 2: After playing around with flamegraph I found that for equality.lox the interpreter is spending quite some time in dropping Rc<String>. I now intern string literals in the tokenizer and use those interned strings at runtime. This brought the times for equality.lox from 3.2x to 2.6x (see here ). :) Edit 3: Changing the hash function from FNV to rustc-hash saved a little time on several benchmarks (see here ). Thanks 1vader for the suggestion. :) Read more Archived post. New comments cannot be posted and votes cannot be cast. Sort by: Best Open comment sort options Best Top New Controversial Old Q&A [deleted] • 2y ago • Edited 2y ago So I pulled your repo down and ran a couple instances of cargo flamegraph running equality.lox, instantiation.lox and binary_tree.lox. So the couple of things that stick out to me there is a lot of malloc, both allocating and deallocating. Generally this is were java's VM garbage collector is likely going to be quite a bit more efficient. In equality and instantiation something like 60%+ of the time is spent doing memory related things. For the tree walker it's hard to get away from using the smart pointers but that likely a big bulk of time spent. In terms of things to try to speed things up. I did also see a decent amount of time on hashing. Because it looks like your two keys are ExprId and Symbol which are both integers you could try using this crate that has a faster hashing function for small keys https://crates.io/crates/fnv Reply reply mhj • 2y ago Thanks for looking into it. :) So the couple of things that stick out to me there is a lot of malloc, both allocating and deallocating. Generally this is were java's VM can GC are likely going to be quite a bit more efficient. That's what my gut feeling is too. Maybe I'll try to implement a version with an arena allocator with no regard for deallocation. If this will get my implementation faster than the Java version that should strongly confirm that the slowdown is due to RC vs GC here. Regarding the hashing function: I have already tried using aHash which sped thing things up but not by a lot. PS: thanks for mentioning cargo flamegraph . I haven't heard of it before, seems useful. :) Reply reply 7 more replies 7 more replies More replies More replies Uncaffeinated • 2y ago • Edited 2y ago Disclaimer: I just looked at your code, I didn't try to run or profile it myself. Here are the issues I noticed: All your Exprs and Statements are individually boxed. You should probably use either an Arena (storing refs to each node) or a flat Vec (storing the index of each node), which would let you get rid of all the Box/Rc/Vecs and allocation overhead, and probably improve data locality as well. If you go the flat Vec approach and guarantee that the left subexpression is always stored consecutively, then you don't even need to store its index explicitly, further improving compactness (that does prevent subtree sharing though). You are using Rust's default hash function for the string interner. Rust's default hash function is secure against DOS attacks, at the cost of being slower. Java uses a faster hash function that does not protect against DOS attacks. To get an apples-to-apples comparison, try using the same hash function as Java. You appear to be passing App (and hence StringInterner) around as &self for no reason. As far as I can tell, you could replace all these with &mut App and get rid of all the Cells and RefCells. You should use LALRPOP to generate your parser. This will greatly simplify your parsing and lexing code, and likely make it faster as well. For example, you have functions like statement() or primary() where you try a large number of possible tokens one by one. LR parsers replace this with a generated jump table so there is no overhead from having large numbers of possibilities. LR parsers also have the advantage of warning you if there is an ambiguity in your grammar. LALRPOP will probably speed up tokenization as well, due to using the optimized regex crate. Also, I noticed that some of your functions were using Box::from to box the subexpressions while others were using Box::new. I'm not sure why, but you presumably want to be doing the same thing in all cases. Of course, this will be a moot point once you switch to Arena/Vec (see #1), but I figured I should highlight it anyway. After that, you can probably get rid of a lot of the boxing and Rcs around Value too, but that's less important, and I didn't bother to read that far into the code to try to understand why you were doing that in the first place. Reply reply [deleted] • 2y ago LALRPOP I'm assuming this project is being written for educational purposes (it's based on a book about writing an interpreter) so not writing a parser isn't really a viable alternative Reply reply mhj • 2y ago Thanks for looking into it and the feedback. :) That is not something I've looked into as the interpreting part completely dwarfs the parsing part in terms of execution time at the moment. But maybe the improved data locality will improve the execution time? I'll give it a try. I wasn't aware that Java's hash function does not protect against DOS attacks. Hence I only tested aHash (which is DOS resistant) as an alternative hash function, which was a bit faster but not world moving. I've moved to FNV as hash function now and the improvement is quite nice (see the edit in original post). There was a dead-end I've encountered when trying to pass App as &mut. I don't remember the details and maybe I refactored the problem away in the meantime. But I think you are right, it would be nice to remove the RefCell overhead as the interner is accessed quite often. I've decided to implement the parser manually to keep the implementation as close as possible to the book in order to have a direct comparison to the Java implementation. Also, see answer to point 1) regarding execution time. Reply reply 2 more replies 2 more replies More replies 2 more replies 2 more replies More replies Wolf_Popular • 2y ago When I implemented my rust AST walk I didn't really use RC at all for any of the internal implementation. My hunch is that there might be some decisions you made there which are causing the slowdown. Reply reply 1 more reply 1 more reply More replies Uncaffeinated • 2y ago After playing around with the code for a couple hours, comparing it with the Java version etc., I've come to the conclusion that this is fundamentally a JIT vs AOT problem. Once you fixed the string hashing, the main benchmark where Java does better is equality.lox, where you are just evaluating constants and variables repeatedly in a tight loop. This is the kind of thing that JITs like Java excel at. You can certainly make the Rust interpreter much, much faster by doing more optimization in the interpreter (compiling functions to bytecode and pre-resolving variables would be a good first step). However, there's unlikely to be any way to outperform Java with a naive unoptimized interpreter executing tight loops because Java's JIT will optimize the naive code behind the scenes and Rust won't. Anyway, this was a fun challenge. If you're interested, I could try writing my own optimized interpreter for Lox to see how it compares. Reply reply 2 more replies 2 more replies More replies thatikey • 2y ago This might sound silly but - are you doing a —release build? The default cargo build is significantly slower Reply reply mhj • 2y ago I'm running the benchmarks with a release build. :) Reply reply More replies DoingABrowse • 2y ago Do you have a profile dump we can take a look at? Reply reply 1 more reply 1 more reply More replies
======>
https://old.reddit.com/u/Ok-Watercress-9624
-->>-->>
Downside is that it goes against what i said (Make illegal states unrepresantable). With embedding each operation with its operands (let call this E) makes it impossible to have states like these [Data(2),Add] Memorywise first option and  E should be comparable. (Im not so sure about E and first option having the same memory footprint. E probably has lot more overhead given each Add etc. instruction needs to refer to its operand regardless whether it is constant or not. i.e  1+2+3+4 requires stores loads  and adds etc. whereas the first option would simply encode it as 1 2 3 4  Add Add Add . And if adding multiple nums is a frequent op in this language you can always do something like 1 2 3 4 4 AddNMany ) The second option is hovewever should be the smallest in size
======>
https://github.com/salvo-rs/salvo/releases/tag/v0.72.0
-->>-->>
Skip to content {"props":{"docsUrl":"https://docs.github.com/get-started/accessibility/keyboard-shortcuts"}} {"resolvedServerColorMode":"day"} Navigation Menu Toggle navigation Sign in Product Actions Automate any workflow Packages Host and manage packages Security Find and fix vulnerabilities Codespaces Instant dev environments GitHub Copilot Write better code with AI Code review Manage code changes Issues Plan and track work Discussions Collaborate outside of code Explore All features Documentation GitHub Skills Blog Solutions By size Enterprise Teams Startups By industry Healthcare Financial services Manufacturing By use case CI/CD & Automation DevOps DevSecOps Resources Topics AI DevOps Security Software Development View all Explore Learning Pathways White papers, Ebooks, Webinars Customer Stories Partners Open Source GitHub Sponsors Fund open source developers The ReadME Project GitHub community articles Repositories Topics Trending Collections Enterprise Enterprise platform AI-powered developer platform Available add-ons Advanced Security Enterprise-grade security features GitHub Copilot Enterprise-grade AI features Premium Support Enterprise-grade 24/7 support Pricing Search or jump to... Search code, repositories, users, issues, pull requests... Search Clear Search syntax tips Provide feedback We read every piece of feedback, and take your input very seriously. Include my email address so I can be contacted Cancel Submit feedback Saved searches Use saved searches to filter your results more quickly Name Query To see all available qualifiers, see our documentation . Cancel Create saved search Sign in Sign up Reseting focus You signed in with another tab or window. Reload to refresh your session. You signed out in another tab or window. Reload to refresh your session. You switched accounts on another tab or window. Reload to refresh your session. Dismiss alert {{ message }} salvo-rs / salvo Public Notifications You must be signed in to change notification settings Fork 184 Star 3.2k Code Issues 15 Pull requests 2 Discussions Actions Security Insights Additional navigation options Code Issues Pull requests Discussions Actions Security Insights Releases v0.72.0 v0.72.0 Latest Latest Compare Choose a tag to compare Could not load tags Nothing to show {{ refName }} default Loading View all tags chrislearn released this 13 Sep 07:00 · 1 commit to main
          since this release v0.72.0 a5119ce This commit was created on GitHub.com and signed with GitHub’s verified signature . GPG key ID: B5690EEEBB952194 Learn about vigilant mode . What's Changed Impl Writer for tuple by @chrislearn in #882 Allow to wrap Handler with middlewares by @chrislearn in #883 Update force_https doc and add example by @chrislearn in #884 Add SecureMaxSize middleware to control request secure max size. by @chrislearn in #885 oapi: Fix doc typo by @chrislearn in #887 fix: Request Id should use header_name as headers key by @chrislearn in #893 oapi: Give extractors REQUIRED default value by @chrislearn in #894 oapi: Refactor features by @chrislearn in #895 oapi:Add parsing support for non strict integers by @chrislearn in #896 oapi: Add extensions support for OpenApi by @chrislearn in #897 oapi: update AnyValue parse_any by @chrislearn in #898 cors: Add CallNext to control when call next handlers by @chrislearn in #899 oapi: Add support of links by @chrislearn in #900 fix: The error status code was incorrectly set to 405 by @chrislearn in #903 Full Changelog : v0.71.1...v0.72.0 Contributors chrislearn Assets 2 Loading 4 lexara-prime-ai, tryor, TnZzZHlp, and nowethan reacted with thumbs up emoji All reactions 4 reactions 4 people reacted Footer © 2024 GitHub, Inc. Footer navigation Terms Privacy Security Status Docs Contact Manage cookies Do not share my personal information You can’t perform that action at this time.

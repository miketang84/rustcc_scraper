https://www.p99conf.io/2024/10/09/rust-tech-talks/
-->>-->>
Share This Post Rust has been a hot topic at P99 CONF since day 1 ‚Äì literally. From Brian Martin opening up the first-ever P99 CONF with, Whoops! I Wrote it in Rust! , to Glauber Costa‚Äôs enlightening session Rust Is Safe. But Is It Fast? , to Brian Cantrill‚Äôs turbocharged take on Rust, Wright‚Äôs Law, and the Future of Low-Latency Systems , Rust has earned and defended its position as a top topic. And given the conference‚Äôs focus on low-latency engineering strategies, that‚Äôs not surprising. But what is surprising is the amazing lineup of Rust speakers and topics we‚Äôll be sharing with the community at P99 CONF 2024. In case you‚Äôre new to P99 CONF, it‚Äôs a free 2-day community event for engineers obsessed with low-latency engineering strategies and performance optimization. It‚Äôs intentionally virtual, highly interactive, and purely technical. Join Us at the Conference¬† ‚Äì It‚Äôs Free + Virtual Here‚Äôs a sneak peek into some of the Rust-focused talks we‚Äôll be featuring ‚Äì as well as several other (Zig, C++, Go, Java ‚Ä¶.) talks that might be interesting to even the most resolute Rustaceans. Rust Tech Talks There‚Äôs a rather wide array of Rust talks on this year‚Äôs agenda‚Ä¶ Rust: A productive language for writing database applications Carl Lerche, Principal Engineer at AWS [and Tokio developer] When you think about Rust, you might think of performance, safety, and reliability, but what about productivity? Last year , I recommended considering Rust for developing high-level applications. Rust showed great promise, but its library ecosystem needed to mature. What has changed since then? Many higher-level applications sit on top of a database. In this talk, I will explore the current state of Rust libraries for database access, focusing on ergonomics and ease of use‚Äîtwo crucial factors in high-level database application development. Rust + io_uring + ktls: How Fast Can We Make HTTP? Amos Wenger, Writer & Video Maker aka @ fasterthanlime I‚Äôve been working on an HTTP1+2 implementation in async Rust, using io_uring and ktls, as open-source, sponsored by companies like fly.io and Shopify, named loona. All existing Rust HTTP implementations are having a hard time adopting io_uring, because their IO types and buffer management strategy is fundamentally incompatible with it: their only path forward (besides a comprehensive rewrite) is a series of compromises that would negate io_uring performance improvements. fluke is written from the ground up to take advantage of everything io_uring has to offer, and can be run within a tokio current_thread executor. To further reduce system calls (which have gotten more expensive over time, with each hardware-level security mitigations), fluke integrates with kTLS (TLS done in-kernel). In the future, I expect fluke to be a great choice for proxy software (including Kubernetes ingress), and possibly even application software if a sufficiently friendly & stable API design emerges. At the time of this submission, loona implements a lot of http/1 and http/2 correctly, and I‚Äôm eager to report the results of performance testing at P99 CONF. Writing a Kernel in Rust: Code Quality and Performance Luc Len√¥tre, Site Reliability Engineer at Clever Cloud Maestro is a kernel that started as a small school project. Initially written in C, the project then switched to Rust to improve code quality. The project is currently in a clean-up and performance improvement phase, and this talk summarizes the lessons learned from it. Latency, Throughput & Fault Tolerance: Designing the Arroyo Streaming Engine Micah Wylde, Co-founder at Arroyo Arroyo is a distributed, stateful stream processing engine written in Rust. It combines predictable millisecond-latency processing with the throughput of a high-performance batch query engine‚Äîon top of a distributed checkpointing implementation that provides fault tolerance and exactly-once processing. These design goals are often in tension: increasing throughput generally comes at the expense of latency, and consistent checkpointing can introduce periodic latency spikes while we wait for alignment and IO. In this talk, I will cover the distributed architecture and implementation of Arroyo including the core Arrow-based dataflow engine, algorithms for stateful windowing and aggregates, and the Chandy-Lamport inspired distributed checkpointing system. The Performance Engineer‚Äôs Toolkit: A Case Study on Data Analytics with Rust Will Crichton,¬† Assistant Professor at Brown University I optimized a Python data analytics pipeline in my research to make it 180,000x faster using Rust. This speedup spanned the gamut of performance techniques: compiler optimizations, data structure selection, vectorization, parallelization, and more. In this talk, I will use this case study to explain each technique, and give you a better sense of the tools in a performance engineer‚Äôs toolkit. Performance Pitfalls of Rust Async Function Pointers (And Why It Might Not Matter) Byron Wasti, Founder of Balter Load Testing An in-depth analysis of asynchronous function pointers in Rust, why they aren‚Äôt a real thing (compared to normal function pointers) and a performance analysis of each way of constructing them. From Boxed Async functions, to Enum dispatch to StackFutures. Low-Latency Mesh Services Using Actors Nikita Lapkov, Senior Rust Engineer The talk will be about how we transformed our actor system called elfo ( https://github.com/elfo-rs/elfo ) into a distributed mesh of services. Elfo started out as an async Rust actor system, where all actors lived on a single node. It was created to serve extremely I/O-heavy workloads of high-frequency trading industry, with focus on developer ergonomics and performance. As the trading business grew, single-node deployment no longer satisfied the latency requirements when connected to different exchanges. From that, the need for distributed deployment arose. The way messages are delivered is opaque to actors, since they use API provided by elfo for that. All messages are also defined as Rust structs, which we have complete control over. This means that if we ‚Äújust‚Äù implemented a network transport for delivering messages, two actors living on different nodes could talk to each other as if they were on the same node. The reality is, of course, not so simple. The talk will dive deep into how we chose multiple formats for message serialisation, implemented compression of messages while balancing between compression ratio and the latency, implemented back-pressure to avoid fast actors overwhelming the slow ones. The talk will also include how we leverage total control of the transport to make everything observable and debuggable. WebAssembly on the Edge: Sandboxing AND Performance Brian Sletten (Consultant and Author of WebAssembly: the Definitive Guide) and Ramnivas Laddad (Co-founder, Exograph and Author of AspectJ in Action) Moving applications to the Edge often complicates conventional performance techniques due to security constraints. Based on actual experiences moving Exograph-based ( https://exograph.dev ) applications into edge computing environments, we will highlight some strategies for improving performances within the limitations of sandboxed WebAssembly-based environments. This will include discussions of how Wasi-advancements and the new component model can assist balancing these two goals which are often at odds. But Maybe Rust Isn‚Äôt Always the Answer? Of course, Rust isn‚Äôt the only option for low-latency programming. P99 CONF will also feature sessions on rising Rust contender, Zig, as well as low-latency C++, Go, and even Java. For example: 1BRC ‚Äì Nerd Sniping the Java Community‚Äù (Gunnar Morling): Some of the tricks employed by the fastest solutions for the ‚ÄúOne Billion Row Challenge‚Äù #1BRC challenge that went viral earlier this year One Billion Row Challenge in Golang (Shraddha Agrawal): Solving the #1BRC Golang ‚Äì using Golang‚Äôs performance tools to reduce the execution time of processing a 16GB file from 6mins to ~12sec. Just In Time LSM Compaction (Aleksei Kladov): A deep dive into (Zig-based) TigerBeetle‚Äôs compaction algorithm ‚Äì ‚Äúgarbage collection‚Äù for LSM Speed by the Numbers: Text Encoding in C and C++ (JeanHeyd Meneide): Let‚Äôs see how we can have a generic, powerful text encoding library in languages like C and C++, WITHOUT losing the performance of a highly specialized library. Even crabby Rustaceans might want to attend these sessions for a bit of discovery, not to mention friendly debate. üòâ Join Us Online, October 23 and 24 <img width="60" height="60" src="https://www.p99conf.io/wp-content/uploads/2021/09/CynthiaDunlop-150x150.jpg" class="avatar avatar-60 photo wp-post-image" alt="Cynthia Dunlop" decoding="async" /> About Cynthia Dunlop View all posts by this author Prev Previous More To Explore <img loading="lazy" width="300" height="157" src="https://www.p99conf.io/wp-content/uploads/2023/07/rust-theme-p99-300x157.png" class="attachment-medium size-medium wp-image-3317" alt="" decoding="async" srcset="https://www.p99conf.io/wp-content/uploads/2023/07/rust-theme-p99-300x157.png 300w, https://www.p99conf.io/wp-content/uploads/2023/07/rust-theme-p99-1024x536.png 1024w, https://www.p99conf.io/wp-content/uploads/2023/07/rust-theme-p99-768x402.png 768w, https://www.p99conf.io/wp-content/uploads/2023/07/rust-theme-p99.png 1200w" sizes="(max-width: 300px) 100vw, 300px" /> What‚Äôs Next For Rust? Discover ‚Äì and Debate ‚Äì at P99 CONF 24 A preview of some of the Rust-focused talks we‚Äôll be featuring at P99 CONF 2024, as well as several other (Zig, C++, Go, Java‚Ä¶.) talks that might be interesting to even crabby Rustaceans. Cynthia Dunlop October 9, 2024 <img loading="lazy" width="300" height="157" src="https://www.p99conf.io/wp-content/uploads/2024/10/2024-books-by-speakers-p99-300x157.jpg" class="attachment-medium size-medium wp-image-5697" alt="" decoding="async" srcset="https://www.p99conf.io/wp-content/uploads/2024/10/2024-books-by-speakers-p99-300x157.jpg 300w, https://www.p99conf.io/wp-content/uploads/2024/10/2024-books-by-speakers-p99-1024x536.jpg 1024w, https://www.p99conf.io/wp-content/uploads/2024/10/2024-books-by-speakers-p99-768x402.jpg 768w, https://www.p99conf.io/wp-content/uploads/2024/10/2024-books-by-speakers-p99.jpg 1200w" sizes="(max-width: 300px) 100vw, 300px" /> 14 Books by P99 CONF Speakers: Latency, Wasm, Databases & More P99 CONF 2024 speakers have amassed a rather impressive list of publications, including quite a few books. This blog highlights 14 of them. Cynthia Dunlop October 7, 2024
======>
https://github.com/linka-cloud/prost-validate
-->>-->>
Repository files navigation README Apache-2.0 license Prost Validate This is a Rust implementation of protoc-gen-validate . It must be used with prost generated code. All validation rules are documented in the proto file or in the protoc-gen-validate documentation. It provides two implementations: A derive based implementation in the prost-validate crate. A reflection based implementation in the prost-reflect-validate crate. The test suite adapted from
the protoc-gen-validate harness tests is shared between the two implementations. Here are the benchmarks for the tests suite of the two implementations: prost-reflect-validate : harness reflect         time:   [14.849 ms 15.128 ms 15.459 ms] prost-validate : harness derive          time:   [2.5635 ms 2.5780 ms 2.5967 ms] Constraint Rule Comparison Global Constraint Rule Derive Reflect disabled ‚úÖ ‚úÖ Numerics Constraint Rule Derive Reflect const ‚úÖ ‚úÖ lt/lte/gt/gte ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ Bools Constraint Rule Derive Reflect const ‚úÖ ‚úÖ Strings Constraint Rule Derive Reflect const ‚úÖ ‚úÖ len/min_len/max_len ‚úÖ ‚úÖ min_bytes/max_bytes ‚úÖ ‚úÖ pattern ‚úÖ ‚úÖ prefix/suffix/contains ‚úÖ ‚úÖ contains/not_contains ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ email ‚úÖ ‚úÖ hostname ‚úÖ ‚úÖ address ‚úÖ ‚úÖ ip ‚úÖ ‚úÖ ipv4 ‚úÖ ‚úÖ ipv6 ‚úÖ ‚úÖ uri ‚úÖ ‚úÖ uri_ref ‚úÖ ‚úÖ uuid ‚úÖ ‚úÖ well_known_regex ‚úÖ ‚úÖ Bytes Constraint Rule Derive Reflect const ‚úÖ ‚úÖ len/min_len/max_len ‚úÖ ‚úÖ pattern ‚úÖ ‚úÖ prefix/suffix/contains ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ ip ‚úÖ ‚úÖ ipv4 ‚úÖ ‚úÖ ipv6 ‚úÖ ‚úÖ Enums Constraint Rule Derive Reflect const ‚úÖ ‚úÖ defined_only ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ Messages Constraint Rule Derive Reflect skip ‚úÖ ‚úÖ required ‚úÖ ‚úÖ Repeated Constraint Rule Derive Reflect min_items/max_items ‚úÖ ‚úÖ unique ‚úÖ ‚úÖ items ‚úÖ ‚úÖ Maps Constraint Rule Derive Reflect min_pairs/max_pairs ‚úÖ ‚úÖ no_sparse ‚ùì ‚ùì keys ‚úÖ ‚úÖ values ‚úÖ ‚úÖ OneOf Constraint Rule Derive Reflect required ‚úÖ ‚úÖ WKT Scalar Value Wrappers Constraint Rule Derive Reflect wrapper validation ‚úÖ ‚úÖ WKT Any Constraint Rule Derive Reflect required ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ WKT Duration Constraint Rule Derive Reflect required ‚úÖ ‚úÖ const ‚úÖ ‚úÖ lt/lte/gt/gte ‚úÖ ‚úÖ in/not_in ‚úÖ ‚úÖ WKT Timestamp Constraint Rule Derive Reflect required ‚úÖ ‚úÖ const ‚úÖ ‚úÖ lt/lte/gt/gte ‚úÖ ‚úÖ lt_now/gt_now ‚úÖ ‚úÖ within ‚úÖ ‚úÖ
======>
https://old.reddit.com/r/rust/comments/1fzpm8y/is_there_a_way_to_get_the_number_of_elements_for/
-->>-->>
This may seem like a basic question (and I apologize in advance). I have a file and I want to paginate its contents. To make this painless, I'm wrapping the file within a    pathbuf    to use its    .lines()    method which returns an iterator. Before paginating the contents, I want to validate the given pagination input (if the input page and size does not exceed the number of lines of the file). To do this, I need to get the number of lines of the given iterator.   

   What I did currently is: 1) collect the iterators into a vector, 2) get the length via    .len()   , 3) validate the pagination given the total items and size and then use the paginate crate to get the index to be sliced, 4) return the sliced array (see code below).    

   However, I want my method to be more efficient. I've read (and tried doing it hardcoded) that you could skip and take elements of an iterator and then collect it instead (specifically using the    .skip(input\_page\*limit).take(size).collect::<Vec<String>>().join("\\n")    method instead. This leads me to my question if there a way to get the number of elements for a given iterator without consuming it?   

   Sample Current Code:
```
//current implementation -ish
use std::fs::File;
use std::io::{self, BufRead, BufReader};
use std::path::Path;
fn read_file(path: &Path) -> io::Result<()> {
    // Open the file in read-only mode
    let file = File::open(path)?;
    // Wrap the file in a BufReader to read it line by line
    let reader = BufReader::new(file).lines();
    let lines_vec: Vec<Result<String, std::io::Error>> = reader.collect();
    let (page, limit) = (0,0);
    let n_items = lines_vec.len();
    // let p = paginate(n_items, limit); //gets the offsets
    // some function to validate pagination here
    // if input_page > p.page_count() -1 {return an error}
    // vec data here
    // do something with vec data here
    Ok(())
}
fn main() -> io::Result<()> {
    // Specify the path to the file
    let path = Path::new("example.txt");
    // Call the read_file function
    read_file(&path)
}   

   ```   

   Thank you all in advance!   
   

======>
https://old.reddit.com/r/rust/comments/1g022ti/social_login_for_the_existing_rust_backend_and/
-->>-->>
I have a web application written in Svelte that communicates with the Rust backend over WebSockets. I want to add social login support so that customers can sign up / sign in via their Google, Microsoft, or Apple accounts (which would also allow me to get users' e-mail and names providing a smoother experience).   

   After checking different strategies for approaching this task and searching on this subreddit, I realized that the only viable options seem to be:
- Supabase's social login support
- Authelia (they have beta support for OpenID Connect)
- Manually implementing it with oauth2-rs or openidconnect-rs crates.   

   I.e. either use an existing open-source authentication provider or implement it myself. If my tech stack were a typical React or SvelteKit, Supabase would be a great solution, as integrating and getting social login support is trivial. But since I have a Rust backend with Postgres database, I'm not sure if Supabase or Authelia are viable options since there are no solid Rust SDKs, meaning that I'd probably need to read the spec for the Supabase's REST API to get webhooks from Supabase and parse them to get the tokens, user info etc which on its own is not much easier than implementing OIDC manually via openidconnect crate.   

   So, I wonder how do you typically add social login support for your Rust applications, folks? Are there easier/streamlined ways to add it to the tech stack similar to mine (Supabase SDK for Auth, etc.)?   
   

======>
https://tmandry.gitlab.io/blog/posts/the-main-thing/
-->>-->>
Rust‚Äôs design goals should be about code Oct 09, 2024 Rust has been kicking around the idea of design goals , under various names, for some time. Design goals are useful because they give everyone making product decisions a common language and set of priorities to appeal to. If done well, they would accumulate the design wisdom of Rust into a concise package and assist RFC authors outside the Rust teams themselves. Last month, in the two days before RustConf, the Lang team held a meetup where I presented a draft set of design goals. These goals are framed differently from what we have tried in the past. Rather than starting from what makes a great language , they are phrased in terms of what makes great code . I think this is a much better way of framing our goals. Here is a sample revised from the draft: Correctness by construction Great code is trivial to verify. It maintains its invariants by making invalid states unrepresentable. It leans on static checks to keep bugs out of the codebase and ease the burden on reviewers. Clarity of purpose Great code brings only the important characteristics of your application to your attention. It avoids wading through needless complexity to express an idea. Great code can be written and understood iteratively, emphasizing different characteristics at different levels of abstraction. Power through composition Great code is built from smaller building blocks that are easy to understand. It leverages existing libraries where appropriate, composing them in high-level ways to express its aims. Flexibility to change Great code has the flexibility to accept changes over time, from a diverse array of contributors, without losing confidence in its correctness. The point of this framing is to put the experience of our users first. That includes programmers who are not enthusiasts or even Rust users by choice. Working with great code can be a delight for everyone involved, and a great language supports writing great code. If we do our job right, our users will not wake up every day thinking about the semantics of the language they‚Äôre using. They won‚Äôt romanticize about the brilliant minds who designed it or how clever the parsing formalism is. That can never be our goal, least of all because it‚Äôs not realistic. The best technology fades into the background. Many of our users will work in rapidly changing codebases that don‚Äôt fit in their heads. We want them to solve their problems quickly, predictably, and with confidence. We want them to ship features to their users joyfully and on time, without getting pulled into endless bug hunts. The elegance of a programming language matters to an extent, but that extent is limited. If we overemphasize it, we can end up missing out on the bigger opportunity in front of us. Building products on great code can be transformative to the engineering teams building them, and ultimately to their end users around the world. Making Async Rust Reliable

======>
https://old.reddit.com/r/rust/comments/1fzsxh1/rust_talks_at_p99_conf/
-->>-->>
There's a solid track of Rust talks at P99 CONF (free and virtual), including a keynote by Carl Lerche (creator of tokio). We'd like to encourage community members to join in the discussion. Speakers will be available to chat and answer questions.    

   https://www.p99conf.io/2024/10/09/rust-tech-talks/   
   

======>
https://github.com/tonbo-io/fusio
-->>-->>
Repository files navigation README Apache-2.0 license Fusio fusio provides Read and Write traits to operate on multiple storage backends (e.g., local disk, Amazon S3) across various asynchronous runtimes‚Äîboth poll-based ( tokio ) and completion-based ( tokio-uring , monoio )‚Äîwith: lean: binary size is at least 14√ó smaller than others. minimal-cost abstraction: compared to bare storage backends, trait definitions allow dispatching file operations without extra overhead. extensible: exposes traits to support implementing storage backends as third-party crates. fusio is now at preview version, please join our community to attend its development and semantics / behaviors discussion. Why do we need fusio ? In developing Tonbo , we needed a flexible and efficient way to handle file and file system operations across multiple storage backends‚Äîsuch as memory, local disk, and remote object storage. We also required compatibility with various asynchronous runtimes, including both completion-based runtimes and event loops in languages like Python and JavaScript. fusio addresses these needs by providing: offers traits that allow dispatch of file and file system operations to multiple storage backends. usable in diverse async runtimes, not only disk but also network I/O. ideal for embedded libs like Tonbo. can be extended via third-party crates, enabling custom asynchronous file and file system implementations. For more context, please check apache/arrow-rs#6051 . How to use it? Installation fusio = { version = " * " , features = [ " tokio " ] } Examples Runtime agnostic fusio supports switching the async runtime at compile time. Middleware libraries can build runtime-agnostic implementations, allowing the top-level application to choose the runtime. Object safety fusio provides two sets of traits: Read / Write / Seek / Fs are not object-safe. DynRead / DynWrite / DynSeek / DynFs are object-safe. You can freely transmute between them. File system traits fusio has an optional Fs trait (use default-features = false to disable it). It dispatches common file system operations (open, remove, list, etc.) to specific storage backends (local disk, Amazon S3). S3 support fusio has optional Amazon S3 support (enable it with features = ["tokio-http", "aws"] ); the behavior of S3 operations and credentials does not depend on tokio . When to choose fusio ? Overall, fusio carefully selects a subset of semantics and behaviors from multiple storage backends and async runtimes to ensure native performance in most scenarios. For example, fusio adopts a completion-based API (inspired by monoio ) so that file operations on tokio and tokio-uring have the same performance as they would without fusio . compare with object_store object_store is locked to tokio and also depends on bytes . fusio uses IoBuf / IoBufMut to allow &[u8] and Vec<u8> to avoid potential runtime costs. If you do not need to consider other async runtimes, try object_store ; as the official implementation, it integrates well with Apache Arrow and Parquet. compare with opendal fusio does not aim to be a full data access layer like opendal . fusio keeps features lean, and you are able to enable features and their dependencies one by one. The default binary size of fusio is 245KB, which is much smaller than opendal (8.9MB). If you need a full ecosystem of DAL (tracing, cache, etc.), try opendal. Also, compared with opendal::Operator , fusio exposes core traits and allows them to be implemented in third-party crates. Roadmap abstractions file operations (partial) file system operations storage backend implementations disk tokio tokio-uring monoio network HTTP client trait wi network storage runtime support tokio (over reqwest) monoio (over hyper-tls) tokio-uring (over hyper-tls) Amazon S3 Azure Blob Storage Cloudflare R2 in-memory conditional operations extensions parquet support object_store support Credits monoio : all core traits‚Äîbuffer, read, and write‚Äîare highly inspired by it. futures : its design of abstractions and organization of several crates (core, util, etc.) to avoid coupling have influenced fusio 's design. opendal : Compile-time poll-based/completion-based runtime switching inspires fusio . object_store : fusio adopts S3 credential and path behaviors from it.
======>
https://filtra.io/rust-sep-24
-->>-->>
Rust Jobs Report: September 2024 Welcome to the September 2024 Rust Jobs Report by filtra. Please enjoy our overview of the Rust job market. To add your or another company to the filtra Rust jobs index, please email filtra@filtra.io with the subject line "Rust Index" We are excited to announce that we have begun securing sponsors for the report. As previously mentioned, the addition of sponsors will make the report self-sustaining, and will provide for expansion and improvements. To sponsor the report, please reach out! filtra@filtra.io Are There Jobs in Rust? 867 We found the highest number of Rust job listings ever this month, 867. How Many Companies Use Rust? 109 The total number of hirers shrank slightly this month. So, each individual company is hiring more. What Companies Use Rust Most? Amazon 211 IBM 64 Microsoft 50 Apple 42 Cloudflare 40 DataDog 37 Canonical 27 Tesla 20 xAI 19 SpaceX 15 The leaderboard for hirers had some interesting developments in September. Amazon had a staggering number of Rust jobs posted. IBM took the number two spot from Microsoft. And, if "Elon" were a category, he'd be our number three contender. What Other Companies Use Rust? GitHub 13 Discord 11 Fastly 11 Worldcoin 11 Fortanix 9 Crypto.com 9 Figma 9 Google 8 IOHK 7 Deliveroo 7 1Password 7 HyperExponential 7 Disney 7 Activision 6 Ardan Labs 6 Mozilla 6 Parity 6 Sentry 6 Syndica 6 KeyRock 6 Matic 6 Data Direct Networks 6 Volume Finance 5 Conduit 5 informal 5 Threema 5 Deep Fence 5 HelsingAI 5 OpenAI 5 Hugging Face 4 OneSignal 4 StarkWare 4 Scaleway 4 Dropbox 4 Ellipsis Labs 4 Mercedes 4 Heliax 4 Stockly 3 Axiom 3 InfinyOn 3 AllSpice 3 HealPay 3 Open Cosmos 3 Chorus One 3 InfluxDB 3 ApolloGraphQL 3 Vivint 3 Akamai 3 Swift Navigation 3 Qumulo 3 Svix 3 10X Genomics 2 Aztec 2 Zoo 2 Matter Labs 2 Osmosis Labs 2 amo 2 Zama 2 OpenTalk 2 Fullstory 2 Qovery 2 Qdrant 2 Embark 2 Railway 2 Smarkets 2 arm 2 Star Lab 1 ZORA 1 Uniswap Labs 1 OVH 1 Ledger 1 Shadow 1 Clever Cloud 1 Rapid7 1 Zed Industries 1 Atlassian 1 Two Sigma 1 Tonari 1 Immunant 1 Prisma 1 Volka 1 Toyota Connected 1 KION 1 PingCAP 1 Liquid Analytics 1 Meilisearch 1 DSCVR 1 Ditto 1 Astropad 1 Tabnine 1 AppSignal 1 Scythe Robotics 1 FP Complete 1 Oxide 1 Grafbase 1 Zelis 1 shuttle 1 Qwiet AI 1 LINE 1 Our list of other hirers continues to get deeper. You'll notice that GitHub, Discord, Fastly, and Worldcoin all have 10+ postings. By way of newcomers, this month saw Disney joining the index. They seem to mostly use Rust for their streaming platforms. If you know of a company that should be on this list but isn't, let us know: filtra@filtra.io. What Industries Use Rust Most? cloud/infrastructure 285 crypto 85 systems/hardware 76 productivity 76 consultancy 72 data science 50 monitoring/reliability 44 iot/robotics/automotive 36 security 33 messaging/notifications 23 The list of top industries is much the same as it was last month. What Other Industries Use Rust? aerospace 18 browser 14 fintech 12 marketplace 10 gaming 9 databases 8 streaming 7 networking 3 health/biotech 3 social media 2 animation 1 Rust is pretty much everywhere! Are Rust Jobs Only For Senior Devs? junior 26 mid 480 senior 361 September was actually a pretty good month for entry-level positions. We saw twice as many posted this month compared to last month. Many of those positions were internships. get rust jobs on filtra Data for this edition of the Rust Jobs Report is filtra platform data supplemented with public job postings. Feel free to visit the filtra Rust blog to see past reports. Rust Jobs Report: September 2024 Welcome to the September 2024 Rust Jobs Report by filtra. Please enjoy our overview of the Rust job market. To add your or another company to the filtra Rust jobs index, please email filtra@filtra.io with the subject line "Rust Index"
======>
https://pranitha.rs/posts/rust-ipc-ping-pong/
-->>-->>
IPC in Rust on  2024-10-04 At work, our team recently found ourselves in need of a high-performance IPC solution in Rust. Our search led us to an insightful article by 3tilley, " IPC in Rust - a Ping Pong Comparison ," which provided a great starting point for our exploration. Inspired by this work, we decided to dive deeper and conduct our own performance measurements, with a particular interest in the promising iceoryx2 framework. Building on the work presented in the original article, we will be using UNIX Domain Sockets (both stream and datagram), Memory Mapped Files, and Shared Memory using iceoryx2 crate to explore the IPC performance between processes running on the same machine for varying payload sizes. Setup We will measure the time taken to complete a request-response cycle between two processes communicating over IPC. All the experiments are setup similar to the original work, A producer process sends a request payload of size ùë• KB A consumer process responds with a payload of same size ùë• KB We measure the round-trip time for this exchange and we will be using the same benchmarking tool Divan . Hardware and OS Env : Linux Cloud VM Arch : x86_64 CPU : 32 cores Model Name : AMD EPYC 7B13 L1d cache : 512 KiB L1i cache : 512 KiB L2 cache : 8 MiB L3 cache : 64 MiB Memory : 128GB OS : Ubuntu 20.04.1 Kernel : 5.15.0 Optimization and Measurement Techniques CPU Affinity: Tying the processes to specific cores prevents process migration between cores, which can introduce variability in measurements due to context switching overhead. Used core_affinity crate for this. CPU Warmup: To account for initial low power states and ensure consistent CPU frequency, we have implemented a 1-second warmup period before taking measurements. This allows the CPU to reach its steady-state performance, providing more accurate and consistent results across different IPC mechanisms. pub fn cpu_warmup () { let warmup = std::time::Instant::now(); loop { if warmup. elapsed () > std::time::Duration::from_millis( 1000 ) { break ; } } } Payload Generation The paylaods are randomly generated String s conataining alphanumeric characters. pub fn generate_random_data ( data_size : usize , seed : u64 ) -> Vec< u8 > { const CHARSET : & [ u8 ] = b "ABCDEFGHIJKLMNOPQRSTUVWXYZ\ abcdefghijklmnopqrstuvwxyz\ 0123456789" ; let mut rng = StdRng::seed_from_u64(seed); ( 0 .. data_size) . map (|_| { let idx = rng. gen_range ( 0 .. CHARSET . len ()); CHARSET [idx] }) . collect () } Since data transmission happens in bytes, we generate strings as byte vectors( Vec<u8> ) of given size. seed helps in generating same payloads for a given data_size ensuring data consistency for all the approaches we test. pub fn get_payload ( data_size : usize ) -> (Vec< u8 >, Vec< u8 >) { let request_data = generate_random_data (data_size, 1 ); let response_data = generate_random_data (data_size, 2 ); (request_data, response_data) } get_payload function returns request data and response data to be transmitted. Approach 1 - UNIX Domain Stream Socket UNIX Domain Sockets(UDS) are typically considered a better option if data exchange is needed between processes running on the same UNIX host, compared to IP sockets. They are said to be faster and lighter than IP sockets as they avoid some operations needed for network interface. Here's a simplified implementation of the UNIX Domain Stream Socket approach: ... // Producer const UNIX_SOCKET_PATH : &str = "/tmp/unix_stream.sock" ; impl UnixStreamRunner { pub fn new ( start_child : bool , data_size : usize ) -> Self { let unix_listener = UnixListener::bind( UNIX_SOCKET_PATH ). unwrap (); let exe = crate ::executable_path( "unix_stream_consumer" ); let child_proc = if start_child { Some(Command::new(exe). args ( & [data_size. to_string ()]). spawn (). unwrap ()) } else { None }; let wrapper = UnixStreamWrapper::from_listener(unix_listener); let (request_data, response_data) = get_payload (data_size); Self { child_proc, wrapper, data_size, request_data, response_data } } pub fn run ( &mut self , n : usize ) { let mut buf = vec![ 0 ; self .data_size]; for _ in 0 .. n { self .wrapper.stream. write ( & self .request_data). unwrap (); self .wrapper.stream. read_exact ( &mut buf). unwrap (); } } } ... // Cosumer fn main () { let args: Vec<String> = std::env::args(). collect (); let data_size = usize ::from_str( & args[ 1 ]). unwrap (); core_affinity::set_for_current(core_affinity::CoreId { id: 0 }); let mut wrapper = ipc::unix_stream::UnixStreamWrapper::unix_connect(); let (request_data, response_data) = get_payload (data_size); cpu_warmup (); let mut buf = vec![ 0 ; data_size]; while let Ok( _ ) = wrapper.stream. read_exact ( &mut buf) { wrapper.stream. write ( & response_data). unwrap (); } } For a domain stream socket, we provide path on the file system which will be created and used as the socket's address. After connecting the sockets, request_data is written to the socket and the consumer reads the message into a buffer, then responds with response_message . Approach 2 - Unix Datagram Socket While implementing UNIX domain datagram socket, we faced few issues at runtime: "Address already in use": We initially tried to use a single socket path /tmp/unix_datagram.sock for the communication, similar to unix stream socket. We tried to bind both processes to the same socket path which lead to "Address already in use" runtime error. Then we found that, for unix domain datagram sockets, we have to bind each process to a separate socket path. For communication between these two sockets, we connect() each socket to the other which sets the peer address. "Message too big": This error is due to the UDP datagram size limitation. UDP has a theoretical maximum datagram size of 65,507 bytes. To handle larger data sizes, we have to send the payload in chunks. "No buffer space available": Upon our search to resolve this error, we found on stackoverflow that this error occurs when the kernel cannot allocate a memory for the socket buffer and it is not possible to send any data over any socket till some memory is freed. We have handled this by implementing a retry mechanism. We keep retrying until we get a successful write. Data Loss: We would need more fine-grained flow control on the UDP side to prevent packet drops for higher payload sizes. As our primary intention is to measure difference between shared memory based IPC and other approaches, we are dropping that exercise for now. To prevent the program from panicking due to occasional data loss or corruption, we have removed the strict data validation checks. This allows the program to continue running even if some packets are lost or arrive out of order. Here's a simplified implementation: ... // Producer const MAX_CHUNK_SIZE : usize = 64 * KB ; const UNIX_DATAGRAM_SOCKET_1 : &str = "/tmp/unix_datagram1.sock" ; const UNIX_DATAGRAM_SOCKET_2 : &str = "/tmp/unix_datagram2.sock" ; impl UnixDatagramWrapper { pub fn new ( is_child : bool , data_size : usize ) -> Self { let (socket_path, peer_socket_path) = if is_child { ( UNIX_DATAGRAM_SOCKET_1 , UNIX_DATAGRAM_SOCKET_2 ) } else { ( UNIX_DATAGRAM_SOCKET_2 , UNIX_DATAGRAM_SOCKET_1 ) }; let socket = UnixDatagram::bind(socket_path). unwrap (); Self { socket, peer_socket_path, data_size, } } pub fn connect_to_peer ( & self ) { self .socket. connect ( & self .peer_socket_path). unwrap (); } pub fn send ( & self , data : & Vec< u8 >) { // Send data in chunks for chunk in data. chunks ( MAX_CHUNK_SIZE ) { // Retry until we have a successful write loop { match self .socket. send (chunk) { Ok( _ ) => break , Err( _ ) => continue , } } } } pub fn recv ( & self ) -> Vec< u8 > { let mut received_data = Vec::new(); // Read till we receive all chunks loop { let mut buf = vec![ 0 ; MAX_CHUNK_SIZE ]; let size = self .socket. recv ( &mut buf). unwrap (); received_data. extend_from_slice ( & buf[ .. size]); if received_data. len () == self .data_size { break ; } } received_data } } impl UnixDatagramRunner { pub fn new ( start_child : bool , data_size : usize ) -> Self { let is_child = false ; let wrapper = UnixDatagramWrapper::new(is_child, data_size); let (request_data, response_data) = get_payload (data_size); let exe = crate ::executable_path( "unix_datagram_consumer" ); let child_proc = if start_child { Some(Command::new(exe). args ( & [data_size. to_string ()]). spawn (). unwrap (),) } else { None }; // Awkward sleep to make sure the child proc is ready sleep (Duration::from_millis( 500 )); wrapper. connect_to_peer (); Self { child_proc, wrapper, request_data, response_data, } } pub fn run ( &mut self , n : usize , print : bool ) { let start = Instant::now(); for _ in 0 .. n { self .wrapper. send ( & self .request_data); let _response = self .wrapper. recv (); } } } ... // Consumer fn main () { let args: Vec<String> = std::env::args(). collect (); let data_size = usize ::from_str( & args[ 1 ]). unwrap (); core_affinity::set_for_current(core_affinity::CoreId { id: 0 }); let is_child = true ; let socket_wrapper = ipc::unix_datagram::UnixDatagramWrapper::new(is_child, data_size); socket_wrapper. connect_to_peer (); let (request_data, response_data) = get_payload (data_size); cpu_warmup (); loop { let _request = socket_wrapper. recv (); socket_wrapper. send ( & response_data); } } For each process, we bind() sockets to the socket_path and connect each socket to their peer_socket_path . Then we send request_data in chunks and retry until we have successfully sent all data. The child process reads till all the chunks are received and responds with response_data . Approach 3 - Memory Mapped Files Memory-mapped files are a method of accessing file contents by mapping them in the virtual address space of the calling process. To make the changes made by one process be visible to the other, we call mmap() with MAP_SHARED flag enabled. The synchronisation between processes has been handled similar to the original work using raw_sync crate. // Shared memory layout //|    0    |    1    |    2    |    3    |    4    |    5    |    6    |    7    | //|   producer lock   |   consumer lock   |      data buffer (ping or pong)       | pub struct MmapWrapper { pub mmap : MmapMut, pub owner : bool , pub our_event : Box<dyn EventImpl>, pub their_event : Box<dyn EventImpl>, pub data_start : usize , pub data_size : usize , } impl MmapWrapper { pub fn new ( owner : bool ) -> Self { let path: PathBuf = "/tmp/mmap_data.txt" . into (); let file = OpenOptions::new() . read ( true ) . write ( true ) . create ( true ) . open ( & path) . unwrap (); file. set_len ( 8 ). unwrap (); let mut mmap = unsafe { MmapMut::map_mut( & file). unwrap () }; let bytes = mmap. as_mut (); // The two events are locks - one for each side. Each side activates the lock while it's // writing, and then unlocks when the data can be read let ((our_event, lock_bytes_ours), (their_event, lock_bytes_theirs)) = unsafe { if owner { ( BusyEvent::new(bytes. get_mut ( 0 ). unwrap (), true ). unwrap (), BusyEvent::new(bytes. get_mut ( 2 ). unwrap (), true ). unwrap (), ) } else { ( // If we're not the owner, the events have been created already BusyEvent::from_existing(bytes. get_mut ( 2 ). unwrap ()). unwrap (), BusyEvent::from_existing(bytes. get_mut ( 0 ). unwrap ()). unwrap (), ) } }; // Confirm that we've correctly indexed two bytes for each lock assert!(lock_bytes_ours <= 2 ); assert!(lock_bytes_theirs <= 2 ); if owner { our_event. set (EventState::Clear). unwrap (); their_event. set (EventState::Clear). unwrap (); } Self { mmap, owner, our_event, their_event, data_start: 4 , data_size, } } pub fn signal_start ( &mut self ) { self .our_event. set (EventState::Clear). unwrap () } pub fn signal_finished ( &mut self ) { self .our_event. set (EventState::Signaled). unwrap () } pub fn write ( &mut self , data : & [ u8 ]) { ( &mut self .mmap[ self .data_start .. ]). write (data). unwrap (); } pub fn read ( & self ) -> & [ u8 ] { & self .mmap. as_ref ()[ self .data_start .. self .data_size] } } MmapMut::map_mut() function creates a shared and writeable memory map backed by a file. The First 4 bytes in the memory mapping is reserved for event control and the rest 4 bytes are used to store the data. pub fn run ( &mut self , n : usize , print : bool ) { for _ in 0 .. n { // Activate our lock in preparation for writing self .wrapper. signal_start (); self .wrapper. write ( & self .request_data); // Unlock after writing self .wrapper. signal_finished (); // Wait for their lock to be released so we can read if self .wrapper.their_event. wait (Timeout::Infinite). is_ok () { let str = self .wrapper. read (); } } } We lock, write, unlock and read the data when we are allowed to. Approach 4 - Shared Memory using iceoryx2 iceoryx2 library provides a zero-copy and lock-free inter-process communication. Currently the library doesn't have support for request-response mechanism, so we will use their publish-subscribe mechanism to measure the request-response cycle response time. Here is a simplified implementation: ... // Producer pub struct IceoryxWrapper { pub publisher : Publisher<ipc::Service, [ u8 ], ()>, pub subscriber : Subscriber<ipc::Service, [ u8 ], ()>, } impl IceoryxWrapper { pub fn new ( is_producer : bool , data_size : usize ) -> IceoryxWrapper { let node = NodeBuilder::new().create::<ipc::Service>(). unwrap (); let request_name = ServiceName::new( & format!( "Request" )). unwrap (); let request_service = node . service_builder ( & request_name) .publish_subscribe::<[ u8 ]>() . open_or_create () . unwrap (); let response_name = ServiceName::new( & format!( "Respose" )). unwrap (); let response_service = node . service_builder ( & response_name) .publish_subscribe::<[ u8 ]>() . open_or_create () . unwrap (); let (publisher, subscriber) = if is_producer { ( request_service . publisher_builder () . max_slice_len (data_size) . create () . unwrap (), response_service. subscriber_builder (). create (). unwrap (), ) } else { ( response_service . publisher_builder () . max_slice_len (data_size) . create () . unwrap (), request_service. subscriber_builder (). create (). unwrap (), ) }; IceoryxWrapper { publisher, subscriber, } } } impl IceoryxRunner { pub fn run ( &mut self , n : usize , print : bool ) { for _ in 0 .. n { let sample = self .wrapper .publisher . loan_slice_uninit ( self .data_size) . unwrap (); let sample = sample. write_from_slice ( self .request_data. as_slice ()); sample. send (). unwrap (); // Waiting for response loop { if let Some(recv_payload) = self .wrapper.subscriber. receive (). unwrap () { break ; } } } } } ... // Consumer fn main () { let args: Vec<String> = std::env::args(). collect (); let data_size = usize ::from_str( & args[ 1 ]). unwrap (); core_affinity::set_for_current(core_affinity::CoreId { id: 0 }); let wrapper = ipc::iceoryx::IceoryxWrapper::new( false , data_size); let (request_data, response_data) = get_payload (data_size); cpu_warmup (); loop { if let Some(recv_payload) = wrapper.subscriber. receive (). unwrap () { let sample = wrapper.publisher. loan_slice_uninit (data_size). unwrap (); let sample = sample. write_from_slice (response_data. as_slice ()); sample. send (). unwrap (); } } } We have defined two publish-subscibe services. One service is used to send the request payload and the other is used to send the response. After request_data is published, the child proccess reads the message by subscribing to the service. Then the child process publishes response_data to the other service which is subscribed and read by the parent process. Results Ping-Pong Cycle # Approach Time per Operation (¬µs) Ops per Second Time Comparison to Shared Memory 1 stdin_stdout 16.36 62k 139.1x 2 tcp_nodelay 23.45 43k 199.4x 3 tcp_yesdelay 21.71 46k 184.6x 4 udp 24.02 42k 204.2x 5 unix_datagram 17.97 56k 152.8x 6 unix_stream 15.54 64k 132.1x 7 iceoryx2 0.901 1095k 7.6x 8 shared_memory 0.1176 8496k 1.0x 9 memory_mapped_file 0.1189 8407k 1.0x Here are some observations based on the ping-pong cycle performance: Shared memory and memory-mapped file approaches are consistently the fastest, with the lowest time per operation and highest operations per second. iceoryx2 performs remarkably well and is significantly faster than traditional IPC methods like pipes, sockets, and Unix domain sockets. Though there is no significant difference, UNIX domain sockets seem to perform better than IP sockets. Varying Payloads Sizes Time per Operation (in ¬µs) Approach 1KB 2KB 4KB 8KB 16KB 32KB 64KB 128KB 256KB 512KB 1024KB iceoryx2 0.95 0.99 0.96 1.12 1.57 2.29 3.49 7.17 13.56 28.32 57.38 memory_mapped_file 0.24 0.34 0.43 0.54 0.99 2 5.39 7.82 14.6 38.18 56.92 shared_memory 0.34 0.3 0.44 0.78 1.37 2.48 4.24 8.28 16.7 35.53 65.09 tcp 23.17 24.73 24.36 28.79 26.19 29.03 41.92 95.19 153 284.5 541.9 udp 21.82 20.91 22.52 28.27 34.84 50.12 87.43 330.6 614.5 1256 2504 unix_stream 15.69 15.78 18.65 20.94 17.65 25.89 39.25 52.67 62.81 101.8 193.1 unix_datagram 18.9 20.03 23.85 25.72 37.13 54.6 79.98 153.6 250.2 467.9 917.1 stdin_stdout 16.54 16.96 19.65 19.11 26.23 33.63 49.66 159.3 335.2 660.5 1356 Ops per Sec (in k) Approach 1KB 2KB 4KB 8KB 16KB 32KB 64KB 128KB 256KB 512KB 1024KB iceoryx2 1070 1032 1061 907.4 648.9 444.5 288.4 136.7 73.91 35.36 18.11 memory_mapped_file 4303 2993 2390 1899 1028 506.8 251.1 130.9 68.43 34.86 18.04 shared_memory 4125 3428 2312 1312 744 409.7 236.5 120.6 60.09 30.55 15.44 tcp 45.38 45.05 44.01 30.05 40.63 37.13 27.07 10.77 6.84 3.43 1.8 udp 48.8 47.87 45.7 35.36 28.34 20.13 10.75 4.81 1.7 0.81 0.41 unix_stream 63.75 63.37 57.43 45.05 57.29 44.96 26.9 19.08 16.66 10.04 5.79 unix_datagram 54.6 51.61 50.32 38.87 29.98 16.81 13.3 6.23 3.83 2.14 1.09 stdin_stdout 61.1 60.06 58.43 52.77 43.82 32.82 21.35 5.83 2.88 1.58 0.74 Here are some observations: Our first observation from the plot is the clear performance gap between memory based methods and traditional IPC methods. For smaller message sizes (1KB to 8KB), memory-mapped file and shared memory consistently outperform other methods, with the lowest time per operation and highest operations per second. iceoryx2 maintains a relatively flat latency profile up to 8KB. As message size increases, the performance gap between iceoryx2 and memory-mapped files/shared memory narrows significantly and performs comparably to memory-mapped files and shared memory. Within the traditional IPC methods, UNIX stream sockets have better performance across all message sizes. For payloads above 64KB, there is a significant performance degradation in UDP methods which could be due to the datagram size limitation. Memory-mapped file and shared memory show high throughput for all payload sizes. iceoryx2 maintains competitive throughput, especially for larger payloads. Traditional IPC methods show much lower throughput, with a steep decline as payload size increases. Conclusion iceoryx2 emerges as a very attractive option for high-performance inter-process communication. While it's not as fast as raw shared memory or memory-mapped files for smaller payloads, it abstracts away the complexities of manual synchronization required in shared memory and memory-mapped file approaches. iceoryx2 strikes an excellent balance between performance and developer-friendly abstractions, making it a compelling choice for high-performance IPC in Rust projects. Special thanks to elBoberido and Abhirag for reviewing the post and suggesting improvements. Full source code is available on Github , open for critiques and improvements üòä
======>
https://dystroy.org/bacon/
-->>-->>
Overview Installation Usage Configuration Global Preferences Project Settings bacon is a background rust code checker. It's designed for minimal interaction so that you can just let it run, alongside your editor, and be notified of warnings, errors, or test failures in your Rust code. It conveys the information you need even in a small terminal so that you can keep more screen estate for your other tasks. It shows you errors before warnings, and the first errors before the last ones, so you don't have to scroll up to find what's relevant. You don't have to remember commands: the essential ones are listed on bottom and the few other ones are shown on a hit on the h key. Installation Run cargo install --locked bacon Usage Launch bacon in a terminal you'll keep visible bacon This launches the default job, usually based on cargo check :
Bacon will watch the source directories and shows you the errors and warnings found by the cargo command. You may decide to launch and watch tests by either hitting the t key, or by launching bacon with bacon test or bacon nextest if you're a nextest user. When there's a failure, hit f to restrict the job to the failing test.
Hit esc to get back to all tests. While in bacon, you can see Clippy warnings by hitting the c key. And you get back to your previous job with esc You may also open the cargo doc in your browser with the d key. You can configure and launch the jobs of your choice: tests, specific target compilations, examples, etc. and look at the results while you code. Run bacon --help to see all launch arguments, and read the cookbook . Configuration See config for details, but here's the crust: Global Preferences The prefs.toml file lets you define key bindings, or always start in summary mode or with lines wrapped. To create a default preferences file, use bacon --prefs . Shortcut: $EDITOR " $(bacon --prefs) " Project Settings You'll define in the bacon.toml file the jobs you need, perhaps an example to check, a run with special parameters, or the settings of clippy, as well as shortcuts to run those jobs. Create a bacon.toml file by running bacon -- init This file already contains some standard jobs. Add your own, for example [jobs.check-win] command = [ "cargo" , "check" , "--target" , "x86_64-pc-windows-gnu" , "--color" , "always" ]
======>
https://github.com/rick-de-water/nonany
-->>-->>
Repository files navigation README Apache-2.0 license MIT license nonany nonany provides integer types with customizable niche values in stable rust. The main benefit of integer types with niches is that it enables the compiler to do memory layout optimization, such that an Option of an integer is the same size as the integer itself: assert_eq ! ( core::mem::size_of::< Option <nonany:: NonAnyU32 < 0xDEADBEEF >>> ( ) ,
    core::mem::size_of::<nonany:: NonAnyU32 < 0xDEADBEEF >> ( ) ) ; Example use nonany :: NonAnyI8 ; assert ! ( NonAnyI8 ::< 20 >::new ( 100 ) .is_some ( ) , "Values that aren't the niche can be stored" ) ; assert ! ( NonAnyI8 ::< 20 >::new ( 20 ) .is_none ( ) , "The niche itself cannot be stored" ) ; let foo = NonAnyI8 :: < 20 > :: new ( 25 ) . unwrap ( ) ; assert_eq ! ( foo.get ( ) , 25 , "The value can be loaded" ) ; Provided types nonmax defines generic types with user defined niches for all integer types, as well as type aliases common use cases: NonAny* NonMin* NonMax* NonZero* i8 NonAnyI8 NonMinI8 NonMaxI8 NonZeroI8 i16 NonAnyI16 NonMinI16 NonMaxI16 NonZeroI16 i32 NonAnyI32 NonMinI32 NonMaxI32 NonZeroI32 i64 NonAnyI64 NonMinI64 NonMaxI64 NonZeroI64 i128 NonAnyI128 NonMinI128 NonMaxI128 NonZeroI128 isize NonAnyIsize NonMinIsize NonMaxIsize NonZeroIsize u8 NonAnyU8 NonMinU8 NonMaxU8 NonZeroU8 u16 NonAnyU16 NonMinU16 NonMaxU16 NonZeroU16 u32 NonAnyU32 NonMinU32 NonMaxU32 NonZeroU32 u64 NonAnyU64 NonMinU64 NonMaxU64 NonZeroU64 u128 NonAnyU128 NonMinU128 NonMaxU128 NonZeroU128 usize NonAnyUsize NonMinUsize NonMaxUsize NonZeroUsize How does it work? Internally all NonAny* types use the NonZero* types from the standard library. When a value is stored in NonAny* , the value is stored in the internal NonZero* as an XOR of the value and the niche. Any value XORed with the niche that isn't the niche itself can never be zero, so this works out perfectly. The upside of this technique is that it works on stable rust. The downside is that it requires an, albeit cheap, XOR operation to load and store the value. Additionally, unlike the NonZero* types, transmuting NonAny* types to their underlying integer types results in a value that was XORed with the niche, instead of the value itself. MSRV The MSRV is fixed at currently 1.56.0, and the intention is to keep it there at least until version 1.0 is released. Similar libraries nonmax - Uses the same XOR technique to create types with an <int>::MAX niche. The equivalent in nonany would be to either use a niche of <int>::MAX , or the NonMax* type aliases. nook - Uses unstable rustc_ attributes to define balanced integers. The equivalent in nonany would be to either use a niche of <int>::MIN , or the NonMin* type aliases. License Licensed under either of Apache License, Version 2.0 or MIT license at your option. Unless you explicitly state otherwise, any contribution intentionally submitted for inclusion in the work by you, as defined in the Apache-2.0 license, shall be dual licensed as above, without any additional terms or conditions.
======>
https://github.com/rust-lang/rust/pull/130350
-->>-->>
One thing I'm not entirely sure about is: if we ever plan to support actual targets (as opposed to just running under miri) that cannot implement expose_provenance , what does it do on those targets? Is a library that depends on expose_provenance conditionally sound, depending on the target? Personally I would prefer the functions to simply not exist if they can't act according to the docs, akin to target specific atomic types. That way, if a user of a library tries to run the library on a target that doesn't support expose, they get told upfront that the library can't be used. Not that I would expect that to come up much, since as far as I know every current target can implement expose, and anyone running code on CHERI or similar would need to audit their libraries anyways (since as is also a problem, and you can't really make that a compile error under CHERI). So the functions existing nearly everywhere would be fine. 1 Scripter17 reacted with thumbs up emoji All reactions 1 reaction
